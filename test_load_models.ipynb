{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test load model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLIP T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pgrimal/miniconda3/envs/cvlp2/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at data_model/t5_pretrained were not used when initializing JointEncoder: ['shared.weight', 'encoder.embed_tokens.weight', 'encoder.block.0.layer.0.SelfAttention.q.weight', 'encoder.block.0.layer.0.SelfAttention.k.weight', 'encoder.block.0.layer.0.SelfAttention.v.weight', 'encoder.block.0.layer.0.SelfAttention.o.weight', 'encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'encoder.block.0.layer.0.layer_norm.weight', 'encoder.block.0.layer.1.DenseReluDense.wi.weight', 'encoder.block.0.layer.1.DenseReluDense.wo.weight', 'encoder.block.0.layer.1.layer_norm.weight', 'encoder.block.1.layer.0.SelfAttention.q.weight', 'encoder.block.1.layer.0.SelfAttention.k.weight', 'encoder.block.1.layer.0.SelfAttention.v.weight', 'encoder.block.1.layer.0.SelfAttention.o.weight', 'encoder.block.1.layer.0.layer_norm.weight', 'encoder.block.1.layer.1.DenseReluDense.wi.weight', 'encoder.block.1.layer.1.DenseReluDense.wo.weight', 'encoder.block.1.layer.1.layer_norm.weight', 'encoder.block.2.layer.0.SelfAttention.q.weight', 'encoder.block.2.layer.0.SelfAttention.k.weight', 'encoder.block.2.layer.0.SelfAttention.v.weight', 'encoder.block.2.layer.0.SelfAttention.o.weight', 'encoder.block.2.layer.0.layer_norm.weight', 'encoder.block.2.layer.1.DenseReluDense.wi.weight', 'encoder.block.2.layer.1.DenseReluDense.wo.weight', 'encoder.block.2.layer.1.layer_norm.weight', 'encoder.block.3.layer.0.SelfAttention.q.weight', 'encoder.block.3.layer.0.SelfAttention.k.weight', 'encoder.block.3.layer.0.SelfAttention.v.weight', 'encoder.block.3.layer.0.SelfAttention.o.weight', 'encoder.block.3.layer.0.layer_norm.weight', 'encoder.block.3.layer.1.DenseReluDense.wi.weight', 'encoder.block.3.layer.1.DenseReluDense.wo.weight', 'encoder.block.3.layer.1.layer_norm.weight', 'encoder.block.4.layer.0.SelfAttention.q.weight', 'encoder.block.4.layer.0.SelfAttention.k.weight', 'encoder.block.4.layer.0.SelfAttention.v.weight', 'encoder.block.4.layer.0.SelfAttention.o.weight', 'encoder.block.4.layer.0.layer_norm.weight', 'encoder.block.4.layer.1.DenseReluDense.wi.weight', 'encoder.block.4.layer.1.DenseReluDense.wo.weight', 'encoder.block.4.layer.1.layer_norm.weight', 'encoder.block.5.layer.0.SelfAttention.q.weight', 'encoder.block.5.layer.0.SelfAttention.k.weight', 'encoder.block.5.layer.0.SelfAttention.v.weight', 'encoder.block.5.layer.0.SelfAttention.o.weight', 'encoder.block.5.layer.0.layer_norm.weight', 'encoder.block.5.layer.1.DenseReluDense.wi.weight', 'encoder.block.5.layer.1.DenseReluDense.wo.weight', 'encoder.block.5.layer.1.layer_norm.weight', 'encoder.block.6.layer.0.SelfAttention.q.weight', 'encoder.block.6.layer.0.SelfAttention.k.weight', 'encoder.block.6.layer.0.SelfAttention.v.weight', 'encoder.block.6.layer.0.SelfAttention.o.weight', 'encoder.block.6.layer.0.layer_norm.weight', 'encoder.block.6.layer.1.DenseReluDense.wi.weight', 'encoder.block.6.layer.1.DenseReluDense.wo.weight', 'encoder.block.6.layer.1.layer_norm.weight', 'encoder.block.7.layer.0.SelfAttention.q.weight', 'encoder.block.7.layer.0.SelfAttention.k.weight', 'encoder.block.7.layer.0.SelfAttention.v.weight', 'encoder.block.7.layer.0.SelfAttention.o.weight', 'encoder.block.7.layer.0.layer_norm.weight', 'encoder.block.7.layer.1.DenseReluDense.wi.weight', 'encoder.block.7.layer.1.DenseReluDense.wo.weight', 'encoder.block.7.layer.1.layer_norm.weight', 'encoder.block.8.layer.0.SelfAttention.q.weight', 'encoder.block.8.layer.0.SelfAttention.k.weight', 'encoder.block.8.layer.0.SelfAttention.v.weight', 'encoder.block.8.layer.0.SelfAttention.o.weight', 'encoder.block.8.layer.0.layer_norm.weight', 'encoder.block.8.layer.1.DenseReluDense.wi.weight', 'encoder.block.8.layer.1.DenseReluDense.wo.weight', 'encoder.block.8.layer.1.layer_norm.weight', 'encoder.block.9.layer.0.SelfAttention.q.weight', 'encoder.block.9.layer.0.SelfAttention.k.weight', 'encoder.block.9.layer.0.SelfAttention.v.weight', 'encoder.block.9.layer.0.SelfAttention.o.weight', 'encoder.block.9.layer.0.layer_norm.weight', 'encoder.block.9.layer.1.DenseReluDense.wi.weight', 'encoder.block.9.layer.1.DenseReluDense.wo.weight', 'encoder.block.9.layer.1.layer_norm.weight', 'encoder.block.10.layer.0.SelfAttention.q.weight', 'encoder.block.10.layer.0.SelfAttention.k.weight', 'encoder.block.10.layer.0.SelfAttention.v.weight', 'encoder.block.10.layer.0.SelfAttention.o.weight', 'encoder.block.10.layer.0.layer_norm.weight', 'encoder.block.10.layer.1.DenseReluDense.wi.weight', 'encoder.block.10.layer.1.DenseReluDense.wo.weight', 'encoder.block.10.layer.1.layer_norm.weight', 'encoder.block.11.layer.0.SelfAttention.q.weight', 'encoder.block.11.layer.0.SelfAttention.k.weight', 'encoder.block.11.layer.0.SelfAttention.v.weight', 'encoder.block.11.layer.0.SelfAttention.o.weight', 'encoder.block.11.layer.0.layer_norm.weight', 'encoder.block.11.layer.1.DenseReluDense.wi.weight', 'encoder.block.11.layer.1.DenseReluDense.wo.weight', 'encoder.block.11.layer.1.layer_norm.weight', 'encoder.final_layer_norm.weight', 'decoder.embed_tokens.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.final_layer_norm.weight', 'lm_head.weight']\n",
      "- This IS expected if you are initializing JointEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing JointEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of JointEncoder were not initialized from the model checkpoint at data_model/t5_pretrained and are newly initialized: ['embed_tokens.weight', 'block.0.layer.0.SelfAttention.q.weight', 'block.0.layer.0.SelfAttention.k.weight', 'block.0.layer.0.SelfAttention.v.weight', 'block.0.layer.0.SelfAttention.o.weight', 'block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'block.0.layer.0.layer_norm.weight', 'block.0.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.0.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.0.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.0.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.0.layer.1.DenseReluDense.wi.weight', 'block.0.layer.1.DenseReluDense.wo.weight', 'block.0.layer.1.layer_norm.weight', 'block.0.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.0.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.0.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.0.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.1.layer.0.SelfAttention.q.weight', 'block.1.layer.0.SelfAttention.k.weight', 'block.1.layer.0.SelfAttention.v.weight', 'block.1.layer.0.SelfAttention.o.weight', 'block.1.layer.0.layer_norm.weight', 'block.1.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.1.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.1.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.1.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.1.layer.1.DenseReluDense.wi.weight', 'block.1.layer.1.DenseReluDense.wo.weight', 'block.1.layer.1.layer_norm.weight', 'block.1.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.1.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.1.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.1.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.2.layer.0.SelfAttention.q.weight', 'block.2.layer.0.SelfAttention.k.weight', 'block.2.layer.0.SelfAttention.v.weight', 'block.2.layer.0.SelfAttention.o.weight', 'block.2.layer.0.layer_norm.weight', 'block.2.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.2.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.2.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.2.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.2.layer.1.DenseReluDense.wi.weight', 'block.2.layer.1.DenseReluDense.wo.weight', 'block.2.layer.1.layer_norm.weight', 'block.2.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.2.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.2.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.2.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.3.layer.0.SelfAttention.q.weight', 'block.3.layer.0.SelfAttention.k.weight', 'block.3.layer.0.SelfAttention.v.weight', 'block.3.layer.0.SelfAttention.o.weight', 'block.3.layer.0.layer_norm.weight', 'block.3.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.3.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.3.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.3.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.3.layer.1.DenseReluDense.wi.weight', 'block.3.layer.1.DenseReluDense.wo.weight', 'block.3.layer.1.layer_norm.weight', 'block.3.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.3.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.3.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.3.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.4.layer.0.SelfAttention.q.weight', 'block.4.layer.0.SelfAttention.k.weight', 'block.4.layer.0.SelfAttention.v.weight', 'block.4.layer.0.SelfAttention.o.weight', 'block.4.layer.0.layer_norm.weight', 'block.4.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.4.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.4.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.4.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.4.layer.1.DenseReluDense.wi.weight', 'block.4.layer.1.DenseReluDense.wo.weight', 'block.4.layer.1.layer_norm.weight', 'block.4.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.4.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.4.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.4.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.5.layer.0.SelfAttention.q.weight', 'block.5.layer.0.SelfAttention.k.weight', 'block.5.layer.0.SelfAttention.v.weight', 'block.5.layer.0.SelfAttention.o.weight', 'block.5.layer.0.layer_norm.weight', 'block.5.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.5.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.5.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.5.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.5.layer.1.DenseReluDense.wi.weight', 'block.5.layer.1.DenseReluDense.wo.weight', 'block.5.layer.1.layer_norm.weight', 'block.5.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.5.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.5.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.5.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.6.layer.0.SelfAttention.q.weight', 'block.6.layer.0.SelfAttention.k.weight', 'block.6.layer.0.SelfAttention.v.weight', 'block.6.layer.0.SelfAttention.o.weight', 'block.6.layer.0.layer_norm.weight', 'block.6.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.6.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.6.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.6.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.6.layer.1.DenseReluDense.wi.weight', 'block.6.layer.1.DenseReluDense.wo.weight', 'block.6.layer.1.layer_norm.weight', 'block.6.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.6.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.6.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.6.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.7.layer.0.SelfAttention.q.weight', 'block.7.layer.0.SelfAttention.k.weight', 'block.7.layer.0.SelfAttention.v.weight', 'block.7.layer.0.SelfAttention.o.weight', 'block.7.layer.0.layer_norm.weight', 'block.7.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.7.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.7.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.7.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.7.layer.1.DenseReluDense.wi.weight', 'block.7.layer.1.DenseReluDense.wo.weight', 'block.7.layer.1.layer_norm.weight', 'block.7.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.7.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.7.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.7.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.8.layer.0.SelfAttention.q.weight', 'block.8.layer.0.SelfAttention.k.weight', 'block.8.layer.0.SelfAttention.v.weight', 'block.8.layer.0.SelfAttention.o.weight', 'block.8.layer.0.layer_norm.weight', 'block.8.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.8.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.8.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.8.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.8.layer.1.DenseReluDense.wi.weight', 'block.8.layer.1.DenseReluDense.wo.weight', 'block.8.layer.1.layer_norm.weight', 'block.8.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.8.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.8.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.8.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.9.layer.0.SelfAttention.q.weight', 'block.9.layer.0.SelfAttention.k.weight', 'block.9.layer.0.SelfAttention.v.weight', 'block.9.layer.0.SelfAttention.o.weight', 'block.9.layer.0.layer_norm.weight', 'block.9.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.9.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.9.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.9.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.9.layer.1.DenseReluDense.wi.weight', 'block.9.layer.1.DenseReluDense.wo.weight', 'block.9.layer.1.layer_norm.weight', 'block.9.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.9.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.9.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.9.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.10.layer.0.SelfAttention.q.weight', 'block.10.layer.0.SelfAttention.k.weight', 'block.10.layer.0.SelfAttention.v.weight', 'block.10.layer.0.SelfAttention.o.weight', 'block.10.layer.0.layer_norm.weight', 'block.10.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.10.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.10.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.10.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.10.layer.1.DenseReluDense.wi.weight', 'block.10.layer.1.DenseReluDense.wo.weight', 'block.10.layer.1.layer_norm.weight', 'block.10.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.10.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.10.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.10.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.11.layer.0.SelfAttention.q.weight', 'block.11.layer.0.SelfAttention.k.weight', 'block.11.layer.0.SelfAttention.v.weight', 'block.11.layer.0.SelfAttention.o.weight', 'block.11.layer.0.layer_norm.weight', 'block.11.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.11.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.11.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.11.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.11.layer.1.DenseReluDense.wi.weight', 'block.11.layer.1.DenseReluDense.wo.weight', 'block.11.layer.1.layer_norm.weight', 'block.11.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.11.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.11.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.11.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'final_layer_norm.weight', 'visual_embedding.feat_embedding.0.weight', 'visual_embedding.feat_embedding.0.bias', 'visual_embedding.feat_embedding.1.weight', 'visual_embedding.absolute_vis_pos_embedding.0.weight', 'visual_embedding.absolute_vis_pos_embedding.0.bias', 'visual_embedding.absolute_vis_pos_embedding.1.weight', 'visual_embedding.obj_order_embedding.weight', 'visual_embedding.img_order_embedding.weight', 'projection.projection.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at data_model/t5_pretrained were not used when initializing JointEncoder: ['shared.weight', 'encoder.embed_tokens.weight', 'encoder.block.0.layer.0.SelfAttention.q.weight', 'encoder.block.0.layer.0.SelfAttention.k.weight', 'encoder.block.0.layer.0.SelfAttention.v.weight', 'encoder.block.0.layer.0.SelfAttention.o.weight', 'encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'encoder.block.0.layer.0.layer_norm.weight', 'encoder.block.0.layer.1.DenseReluDense.wi.weight', 'encoder.block.0.layer.1.DenseReluDense.wo.weight', 'encoder.block.0.layer.1.layer_norm.weight', 'encoder.block.1.layer.0.SelfAttention.q.weight', 'encoder.block.1.layer.0.SelfAttention.k.weight', 'encoder.block.1.layer.0.SelfAttention.v.weight', 'encoder.block.1.layer.0.SelfAttention.o.weight', 'encoder.block.1.layer.0.layer_norm.weight', 'encoder.block.1.layer.1.DenseReluDense.wi.weight', 'encoder.block.1.layer.1.DenseReluDense.wo.weight', 'encoder.block.1.layer.1.layer_norm.weight', 'encoder.block.2.layer.0.SelfAttention.q.weight', 'encoder.block.2.layer.0.SelfAttention.k.weight', 'encoder.block.2.layer.0.SelfAttention.v.weight', 'encoder.block.2.layer.0.SelfAttention.o.weight', 'encoder.block.2.layer.0.layer_norm.weight', 'encoder.block.2.layer.1.DenseReluDense.wi.weight', 'encoder.block.2.layer.1.DenseReluDense.wo.weight', 'encoder.block.2.layer.1.layer_norm.weight', 'encoder.block.3.layer.0.SelfAttention.q.weight', 'encoder.block.3.layer.0.SelfAttention.k.weight', 'encoder.block.3.layer.0.SelfAttention.v.weight', 'encoder.block.3.layer.0.SelfAttention.o.weight', 'encoder.block.3.layer.0.layer_norm.weight', 'encoder.block.3.layer.1.DenseReluDense.wi.weight', 'encoder.block.3.layer.1.DenseReluDense.wo.weight', 'encoder.block.3.layer.1.layer_norm.weight', 'encoder.block.4.layer.0.SelfAttention.q.weight', 'encoder.block.4.layer.0.SelfAttention.k.weight', 'encoder.block.4.layer.0.SelfAttention.v.weight', 'encoder.block.4.layer.0.SelfAttention.o.weight', 'encoder.block.4.layer.0.layer_norm.weight', 'encoder.block.4.layer.1.DenseReluDense.wi.weight', 'encoder.block.4.layer.1.DenseReluDense.wo.weight', 'encoder.block.4.layer.1.layer_norm.weight', 'encoder.block.5.layer.0.SelfAttention.q.weight', 'encoder.block.5.layer.0.SelfAttention.k.weight', 'encoder.block.5.layer.0.SelfAttention.v.weight', 'encoder.block.5.layer.0.SelfAttention.o.weight', 'encoder.block.5.layer.0.layer_norm.weight', 'encoder.block.5.layer.1.DenseReluDense.wi.weight', 'encoder.block.5.layer.1.DenseReluDense.wo.weight', 'encoder.block.5.layer.1.layer_norm.weight', 'encoder.block.6.layer.0.SelfAttention.q.weight', 'encoder.block.6.layer.0.SelfAttention.k.weight', 'encoder.block.6.layer.0.SelfAttention.v.weight', 'encoder.block.6.layer.0.SelfAttention.o.weight', 'encoder.block.6.layer.0.layer_norm.weight', 'encoder.block.6.layer.1.DenseReluDense.wi.weight', 'encoder.block.6.layer.1.DenseReluDense.wo.weight', 'encoder.block.6.layer.1.layer_norm.weight', 'encoder.block.7.layer.0.SelfAttention.q.weight', 'encoder.block.7.layer.0.SelfAttention.k.weight', 'encoder.block.7.layer.0.SelfAttention.v.weight', 'encoder.block.7.layer.0.SelfAttention.o.weight', 'encoder.block.7.layer.0.layer_norm.weight', 'encoder.block.7.layer.1.DenseReluDense.wi.weight', 'encoder.block.7.layer.1.DenseReluDense.wo.weight', 'encoder.block.7.layer.1.layer_norm.weight', 'encoder.block.8.layer.0.SelfAttention.q.weight', 'encoder.block.8.layer.0.SelfAttention.k.weight', 'encoder.block.8.layer.0.SelfAttention.v.weight', 'encoder.block.8.layer.0.SelfAttention.o.weight', 'encoder.block.8.layer.0.layer_norm.weight', 'encoder.block.8.layer.1.DenseReluDense.wi.weight', 'encoder.block.8.layer.1.DenseReluDense.wo.weight', 'encoder.block.8.layer.1.layer_norm.weight', 'encoder.block.9.layer.0.SelfAttention.q.weight', 'encoder.block.9.layer.0.SelfAttention.k.weight', 'encoder.block.9.layer.0.SelfAttention.v.weight', 'encoder.block.9.layer.0.SelfAttention.o.weight', 'encoder.block.9.layer.0.layer_norm.weight', 'encoder.block.9.layer.1.DenseReluDense.wi.weight', 'encoder.block.9.layer.1.DenseReluDense.wo.weight', 'encoder.block.9.layer.1.layer_norm.weight', 'encoder.block.10.layer.0.SelfAttention.q.weight', 'encoder.block.10.layer.0.SelfAttention.k.weight', 'encoder.block.10.layer.0.SelfAttention.v.weight', 'encoder.block.10.layer.0.SelfAttention.o.weight', 'encoder.block.10.layer.0.layer_norm.weight', 'encoder.block.10.layer.1.DenseReluDense.wi.weight', 'encoder.block.10.layer.1.DenseReluDense.wo.weight', 'encoder.block.10.layer.1.layer_norm.weight', 'encoder.block.11.layer.0.SelfAttention.q.weight', 'encoder.block.11.layer.0.SelfAttention.k.weight', 'encoder.block.11.layer.0.SelfAttention.v.weight', 'encoder.block.11.layer.0.SelfAttention.o.weight', 'encoder.block.11.layer.0.layer_norm.weight', 'encoder.block.11.layer.1.DenseReluDense.wi.weight', 'encoder.block.11.layer.1.DenseReluDense.wo.weight', 'encoder.block.11.layer.1.layer_norm.weight', 'encoder.final_layer_norm.weight', 'decoder.embed_tokens.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.final_layer_norm.weight', 'lm_head.weight']\n",
      "- This IS expected if you are initializing JointEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing JointEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of JointEncoder were not initialized from the model checkpoint at data_model/t5_pretrained and are newly initialized: ['embed_tokens.weight', 'block.0.layer.0.SelfAttention.q.weight', 'block.0.layer.0.SelfAttention.k.weight', 'block.0.layer.0.SelfAttention.v.weight', 'block.0.layer.0.SelfAttention.o.weight', 'block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'block.0.layer.0.layer_norm.weight', 'block.0.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.0.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.0.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.0.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.0.layer.1.DenseReluDense.wi.weight', 'block.0.layer.1.DenseReluDense.wo.weight', 'block.0.layer.1.layer_norm.weight', 'block.0.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.0.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.0.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.0.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.1.layer.0.SelfAttention.q.weight', 'block.1.layer.0.SelfAttention.k.weight', 'block.1.layer.0.SelfAttention.v.weight', 'block.1.layer.0.SelfAttention.o.weight', 'block.1.layer.0.layer_norm.weight', 'block.1.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.1.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.1.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.1.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.1.layer.1.DenseReluDense.wi.weight', 'block.1.layer.1.DenseReluDense.wo.weight', 'block.1.layer.1.layer_norm.weight', 'block.1.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.1.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.1.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.1.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.2.layer.0.SelfAttention.q.weight', 'block.2.layer.0.SelfAttention.k.weight', 'block.2.layer.0.SelfAttention.v.weight', 'block.2.layer.0.SelfAttention.o.weight', 'block.2.layer.0.layer_norm.weight', 'block.2.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.2.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.2.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.2.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.2.layer.1.DenseReluDense.wi.weight', 'block.2.layer.1.DenseReluDense.wo.weight', 'block.2.layer.1.layer_norm.weight', 'block.2.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.2.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.2.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.2.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.3.layer.0.SelfAttention.q.weight', 'block.3.layer.0.SelfAttention.k.weight', 'block.3.layer.0.SelfAttention.v.weight', 'block.3.layer.0.SelfAttention.o.weight', 'block.3.layer.0.layer_norm.weight', 'block.3.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.3.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.3.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.3.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.3.layer.1.DenseReluDense.wi.weight', 'block.3.layer.1.DenseReluDense.wo.weight', 'block.3.layer.1.layer_norm.weight', 'block.3.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.3.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.3.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.3.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.4.layer.0.SelfAttention.q.weight', 'block.4.layer.0.SelfAttention.k.weight', 'block.4.layer.0.SelfAttention.v.weight', 'block.4.layer.0.SelfAttention.o.weight', 'block.4.layer.0.layer_norm.weight', 'block.4.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.4.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.4.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.4.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.4.layer.1.DenseReluDense.wi.weight', 'block.4.layer.1.DenseReluDense.wo.weight', 'block.4.layer.1.layer_norm.weight', 'block.4.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.4.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.4.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.4.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.5.layer.0.SelfAttention.q.weight', 'block.5.layer.0.SelfAttention.k.weight', 'block.5.layer.0.SelfAttention.v.weight', 'block.5.layer.0.SelfAttention.o.weight', 'block.5.layer.0.layer_norm.weight', 'block.5.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.5.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.5.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.5.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.5.layer.1.DenseReluDense.wi.weight', 'block.5.layer.1.DenseReluDense.wo.weight', 'block.5.layer.1.layer_norm.weight', 'block.5.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.5.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.5.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.5.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.6.layer.0.SelfAttention.q.weight', 'block.6.layer.0.SelfAttention.k.weight', 'block.6.layer.0.SelfAttention.v.weight', 'block.6.layer.0.SelfAttention.o.weight', 'block.6.layer.0.layer_norm.weight', 'block.6.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.6.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.6.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.6.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.6.layer.1.DenseReluDense.wi.weight', 'block.6.layer.1.DenseReluDense.wo.weight', 'block.6.layer.1.layer_norm.weight', 'block.6.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.6.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.6.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.6.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.7.layer.0.SelfAttention.q.weight', 'block.7.layer.0.SelfAttention.k.weight', 'block.7.layer.0.SelfAttention.v.weight', 'block.7.layer.0.SelfAttention.o.weight', 'block.7.layer.0.layer_norm.weight', 'block.7.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.7.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.7.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.7.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.7.layer.1.DenseReluDense.wi.weight', 'block.7.layer.1.DenseReluDense.wo.weight', 'block.7.layer.1.layer_norm.weight', 'block.7.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.7.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.7.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.7.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.8.layer.0.SelfAttention.q.weight', 'block.8.layer.0.SelfAttention.k.weight', 'block.8.layer.0.SelfAttention.v.weight', 'block.8.layer.0.SelfAttention.o.weight', 'block.8.layer.0.layer_norm.weight', 'block.8.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.8.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.8.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.8.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.8.layer.1.DenseReluDense.wi.weight', 'block.8.layer.1.DenseReluDense.wo.weight', 'block.8.layer.1.layer_norm.weight', 'block.8.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.8.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.8.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.8.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.9.layer.0.SelfAttention.q.weight', 'block.9.layer.0.SelfAttention.k.weight', 'block.9.layer.0.SelfAttention.v.weight', 'block.9.layer.0.SelfAttention.o.weight', 'block.9.layer.0.layer_norm.weight', 'block.9.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.9.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.9.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.9.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.9.layer.1.DenseReluDense.wi.weight', 'block.9.layer.1.DenseReluDense.wo.weight', 'block.9.layer.1.layer_norm.weight', 'block.9.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.9.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.9.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.9.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.10.layer.0.SelfAttention.q.weight', 'block.10.layer.0.SelfAttention.k.weight', 'block.10.layer.0.SelfAttention.v.weight', 'block.10.layer.0.SelfAttention.o.weight', 'block.10.layer.0.layer_norm.weight', 'block.10.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.10.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.10.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.10.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.10.layer.1.DenseReluDense.wi.weight', 'block.10.layer.1.DenseReluDense.wo.weight', 'block.10.layer.1.layer_norm.weight', 'block.10.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.10.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.10.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.10.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.11.layer.0.SelfAttention.q.weight', 'block.11.layer.0.SelfAttention.k.weight', 'block.11.layer.0.SelfAttention.v.weight', 'block.11.layer.0.SelfAttention.o.weight', 'block.11.layer.0.layer_norm.weight', 'block.11.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.11.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.11.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.11.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.11.layer.1.DenseReluDense.wi.weight', 'block.11.layer.1.DenseReluDense.wo.weight', 'block.11.layer.1.layer_norm.weight', 'block.11.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.11.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.11.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.11.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'final_layer_norm.weight', 'visual_embedding.feat_embedding.0.weight', 'visual_embedding.feat_embedding.0.bias', 'visual_embedding.feat_embedding.1.weight', 'visual_embedding.absolute_vis_pos_embedding.0.weight', 'visual_embedding.absolute_vis_pos_embedding.0.bias', 'visual_embedding.absolute_vis_pos_embedding.1.weight', 'visual_embedding.obj_order_embedding.weight', 'visual_embedding.img_order_embedding.weight', 'projection.projection.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint\n",
      "Model loaded from  data_model/jointEncoder/state_dict/VLT5epoch30.pth\n",
      "_IncompatibleKeys(missing_keys=['block.0.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.0.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.0.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.0.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.0.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.0.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.0.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.0.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.1.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.1.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.1.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.1.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.1.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.1.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.1.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.1.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.2.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.2.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.2.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.2.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.2.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.2.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.2.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.2.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.3.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.3.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.3.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.3.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.3.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.3.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.3.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.3.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.4.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.4.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.4.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.4.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.4.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.4.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.4.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.4.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.5.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.5.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.5.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.5.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.5.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.5.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.5.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.5.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.6.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.6.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.6.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.6.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.6.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.6.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.6.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.6.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.7.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.7.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.7.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.7.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.7.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.7.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.7.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.7.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.8.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.8.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.8.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.8.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.8.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.8.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.8.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.8.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.9.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.9.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.9.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.9.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.9.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.9.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.9.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.9.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.10.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.10.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.10.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.10.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.10.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.10.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.10.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.10.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.11.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.11.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.11.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.11.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.11.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.11.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.11.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.11.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'projection.projection.weight'], unexpected_keys=[])\n",
      "Model loaded from  data_model/jointEncoder/state_dict/VLT5epoch30.pth\n",
      "_IncompatibleKeys(missing_keys=['block.0.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.0.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.0.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.0.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.0.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.0.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.0.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.0.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.1.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.1.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.1.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.1.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.1.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.1.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.1.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.1.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.2.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.2.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.2.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.2.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.2.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.2.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.2.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.2.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.3.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.3.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.3.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.3.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.3.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.3.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.3.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.3.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.4.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.4.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.4.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.4.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.4.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.4.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.4.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.4.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.5.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.5.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.5.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.5.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.5.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.5.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.5.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.5.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.6.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.6.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.6.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.6.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.6.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.6.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.6.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.6.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.7.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.7.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.7.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.7.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.7.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.7.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.7.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.7.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.8.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.8.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.8.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.8.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.8.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.8.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.8.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.8.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.9.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.9.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.9.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.9.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.9.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.9.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.9.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.9.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.10.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.10.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.10.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.10.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.10.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.10.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.10.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.10.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.11.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.11.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.11.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.11.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.11.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.11.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.11.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.11.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'projection.projection.weight'], unexpected_keys=[])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from cvlep.VLT5.param import Config\n",
    "\n",
    "if \"RANK\" in os.environ and \"WORLD_SIZE\" in os.environ:  # torchrun launch\n",
    "    rank = int(os.environ[\"RANK\"])\n",
    "    local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "    world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "elif int(os.environ.get('SLURM_NPROCS', 1)) > 1:  # slurm launch\n",
    "    rank = int(os.environ[\"SLURM_PROCID\"])\n",
    "    local_rank = int(os.environ[\"SLURM_LOCALID\"])\n",
    "    world_size = int(os.environ[\"SLURM_NPROCS\"])\n",
    "else:  # single gpu & process launch\n",
    "    rank = 0\n",
    "    local_rank = 0\n",
    "    world_size = 0\n",
    "\n",
    "config_encoder_question = \"experiments/config_vladapter/local/encoder_simple_adapter.json\"\n",
    "config_encoder_passage = \"experiments/config_vladapter/local/encoder_simple_adapter.json\"\n",
    "config_model = \"experiments/config_vladapter/local/config_model.json\"\n",
    "config_training = \"experiments/config_vladapter/local/training_simple_adapter.json\"\n",
    "\n",
    "# Training config\n",
    "config_training = Config.load_json(config_training)\n",
    "config_training.world_size = world_size\n",
    "config_training.rank = rank\n",
    "config_training.local_rank = local_rank\n",
    "if world_size > 1:\n",
    "    config_training.distributed = True\n",
    "    config_training.multiGPU = True\n",
    "else:\n",
    "    config_training.distributed = False\n",
    "    config_training.multiGPU = False\n",
    "\n",
    "config_encoder_question = Config.load_json(config_encoder_question)\n",
    "config_encoder_passage = Config.load_json(config_encoder_passage)\n",
    "config_model = Config.load_json(config_model)\n",
    "\n",
    "from cvlep.trainer_base_vladapter import Trainer\n",
    "\n",
    "trainer = Trainer(config_encoder_question,config_encoder_passage, config_model, config_training, train=False, local=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "a = nn.Linear(768, 10, bias= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_26852/4112195018.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_parameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "add_code_sample_docstrings() got an unexpected keyword argument 'tokenizer_class'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5679/1841660083.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcvlep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer_base_vladapter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_encoder_question\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconfig_encoder_passage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_training\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Projects/visual_language_representation/cvlep/trainer_base_vladapter.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDistributedDataParallel\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mDDP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistributed\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcvlep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCLIPT5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodeling_t5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVLT5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcvlep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCLIPT5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodeling_t5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mJointEncoder\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mencoderVLT5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcvlep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVLT5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Projects/visual_language_representation/cvlep/CLIPT5/modeling_t5.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBeamScorer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBeamSearchScorer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcvlep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCLIPT5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodeling_bart\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDownsample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSparseSample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOneDDownsample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m from .adapters import (\n",
      "\u001b[0;32m~/Documents/Projects/visual_language_representation/cvlep/CLIPT5/modeling_bart.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmy_transformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodeling_bart\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBartModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBartForConditionalGeneration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBartDecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBartEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Projects/visual_language_representation/cvlep/CLIPT5/my_transformers/modeling_bart.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0mBART_START_DOCSTRING\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1292\u001b[0m )\n\u001b[0;32m-> 1293\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mBartModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBartPretrainedModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mBartConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Projects/visual_language_representation/cvlep/CLIPT5/my_transformers/modeling_bart.py\u001b[0m in \u001b[0;36mBartModel\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1327\u001b[0m         \u001b[0mcheckpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"facebook/bart-large\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m         \u001b[0moutput_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSeq2SeqModelOutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m         \u001b[0mconfig_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_CONFIG_FOR_DOC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1330\u001b[0m     )\n\u001b[1;32m   1331\u001b[0m     def forward(\n",
      "\u001b[0;31mTypeError\u001b[0m: add_code_sample_docstrings() got an unexpected keyword argument 'tokenizer_class'"
     ]
    }
   ],
   "source": [
    "from cvlep.trainer_base_vladapter import Trainer\n",
    "\n",
    "trainer = Trainer(config_encoder_question,config_encoder_passage, config_model, config_training, train=False, local=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = trainer.model.image_passage_encoder.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VLT5(\n",
      "  (shared): Embedding(32200, 768)\n",
      "  (encoder): JointEncoder(\n",
      "    (embed_tokens): Embedding(32200, 768)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 12)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (4): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (5): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (6): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (7): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (8): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (9): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (10): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (11): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (visual_embedding): VisualEmbedding(\n",
      "      (feat_embedding): Sequential(\n",
      "        (0): Linear(in_features=2048, out_features=768, bias=True)\n",
      "        (1): T5LayerNorm()\n",
      "      )\n",
      "      (absolute_vis_pos_embedding): Sequential(\n",
      "        (0): Linear(in_features=5, out_features=768, bias=True)\n",
      "        (1): T5LayerNorm()\n",
      "      )\n",
      "      (obj_order_embedding): Embedding(32200, 768)\n",
      "      (img_order_embedding): Embedding(2, 768)\n",
      "    )\n",
      "  )\n",
      "  (decoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32200, 768)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 12)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (4): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (5): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (6): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (7): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (8): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (9): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (10): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (11): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=32200, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected 1 arguments, got 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_25082/4206866470.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_passage_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: expected 1 arguments, got 0"
     ]
    }
   ],
   "source": [
    "trainer.model.image_passage_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VLT5 without adapter or prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pgrimal/miniconda3/envs/cvlp/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of VLT5 were not initialized from the model checkpoint at data_model/t5_pretrained and are newly initialized: ['encoder.visual_embedding.feat_embedding.0.weight', 'encoder.visual_embedding.feat_embedding.0.bias', 'encoder.visual_embedding.feat_embedding.1.weight', 'encoder.visual_embedding.absolute_vis_pos_embedding.0.weight', 'encoder.visual_embedding.absolute_vis_pos_embedding.0.bias', 'encoder.visual_embedding.absolute_vis_pos_embedding.1.weight', 'encoder.visual_embedding.obj_order_embedding.weight', 'encoder.visual_embedding.img_order_embedding.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of VLT5 were not initialized from the model checkpoint at data_model/t5_pretrained and are newly initialized: ['encoder.visual_embedding.feat_embedding.0.weight', 'encoder.visual_embedding.feat_embedding.0.bias', 'encoder.visual_embedding.feat_embedding.1.weight', 'encoder.visual_embedding.absolute_vis_pos_embedding.0.weight', 'encoder.visual_embedding.absolute_vis_pos_embedding.0.bias', 'encoder.visual_embedding.absolute_vis_pos_embedding.1.weight', 'encoder.visual_embedding.obj_order_embedding.weight', 'encoder.visual_embedding.img_order_embedding.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from  data_model/whole_model/vlt5/VLT5Epoch30.pth\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['encoder.visual_embedding.layer_norm.weight'])\n",
      "Model loaded from  data_model/whole_model/vlt5/VLT5Epoch30.pth\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['encoder.visual_embedding.layer_norm.weight'])\n",
      "Configurations\n",
      "{'share_embedding': True, 'share_vis_embedding': True}\n"
     ]
    }
   ],
   "source": [
    "from cvlep.trainer_base_vladapter import Trainer\n",
    "\n",
    "\n",
    "\n",
    "trainer_model = Trainer(config_encoder_question, config_encoder_passage,config_model, config_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, p in trainer_model.model.image_passage_encoder.named_parameters():\n",
    "    if p.requires_grad==True:\n",
    "        print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VLT5 were not initialized from the model checkpoint at data_model/t5_pretrained and are newly initialized: ['encoder.visual_embedding.feat_embedding.0.weight', 'encoder.visual_embedding.feat_embedding.0.bias', 'encoder.visual_embedding.feat_embedding.1.weight', 'encoder.visual_embedding.absolute_vis_pos_embedding.0.weight', 'encoder.visual_embedding.absolute_vis_pos_embedding.0.bias', 'encoder.visual_embedding.absolute_vis_pos_embedding.1.weight', 'encoder.visual_embedding.obj_order_embedding.weight', 'encoder.visual_embedding.img_order_embedding.weight', 'encoder.prompt_modules.prompts.IR.prefix_embedding.0.weight', 'encoder.prompt_modules.prompts.IR.prefix_embedding.1.weight', 'encoder.prompt_modules.prompts.IR.prefix_embedding.1.bias', 'encoder.prompt_modules.prompts.IR.prefix_embedding.3.weight', 'encoder.prompt_modules.prompts.IR.prefix_embedding.3.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of VLT5 were not initialized from the model checkpoint at data_model/t5_pretrained and are newly initialized: ['encoder.visual_embedding.feat_embedding.0.weight', 'encoder.visual_embedding.feat_embedding.0.bias', 'encoder.visual_embedding.feat_embedding.1.weight', 'encoder.visual_embedding.absolute_vis_pos_embedding.0.weight', 'encoder.visual_embedding.absolute_vis_pos_embedding.0.bias', 'encoder.visual_embedding.absolute_vis_pos_embedding.1.weight', 'encoder.visual_embedding.obj_order_embedding.weight', 'encoder.visual_embedding.img_order_embedding.weight', 'encoder.prompt_modules.prompts.IR.prefix_embedding.0.weight', 'encoder.prompt_modules.prompts.IR.prefix_embedding.1.weight', 'encoder.prompt_modules.prompts.IR.prefix_embedding.1.bias', 'encoder.prompt_modules.prompts.IR.prefix_embedding.3.weight', 'encoder.prompt_modules.prompts.IR.prefix_embedding.3.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from  data_model/whole_model/vlt5/VLT5Epoch30.pth\n",
      "_IncompatibleKeys(missing_keys=['encoder.prompt_modules.prompts.IR.prefix_embedding.0.weight', 'encoder.prompt_modules.prompts.IR.prefix_embedding.1.weight', 'encoder.prompt_modules.prompts.IR.prefix_embedding.1.bias', 'encoder.prompt_modules.prompts.IR.prefix_embedding.3.weight', 'encoder.prompt_modules.prompts.IR.prefix_embedding.3.bias'], unexpected_keys=['encoder.visual_embedding.layer_norm.weight'])\n",
      "Model loaded from  data_model/whole_model/vlt5/VLT5Epoch30.pth\n",
      "_IncompatibleKeys(missing_keys=['encoder.prompt_modules.prompts.IR.prefix_embedding.0.weight', 'encoder.prompt_modules.prompts.IR.prefix_embedding.1.weight', 'encoder.prompt_modules.prompts.IR.prefix_embedding.1.bias', 'encoder.prompt_modules.prompts.IR.prefix_embedding.3.weight', 'encoder.prompt_modules.prompts.IR.prefix_embedding.3.bias'], unexpected_keys=['encoder.visual_embedding.layer_norm.weight'])\n",
      "None\n",
      "encoder.prompt_modules is trainable...\n",
      "encoder.prompt_modules is trainable...\n"
     ]
    }
   ],
   "source": [
    "from cvlep.trainer_base_vladapter import Trainer\n",
    "\n",
    "config_encoder_question = \"experiments/config_vladapter/prompt/encoder_prompting.json\"\n",
    "config_encoder_passage = \"experiments/config_vladapter/prompt/encoder_prompting.json\"\n",
    "config_training = \"experiments/config_vladapter/prompt/training_prompt.json\"\n",
    "\n",
    "trainer_clipt5 = Trainer(config_encoder_question, config_encoder_passage, config_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.prompt_modules.prompts.IR.prefix_embedding.0.weight\n",
      "encoder.prompt_modules.prompts.IR.prefix_embedding.1.weight\n",
      "encoder.prompt_modules.prompts.IR.prefix_embedding.1.bias\n",
      "encoder.prompt_modules.prompts.IR.prefix_embedding.3.weight\n",
      "encoder.prompt_modules.prompts.IR.prefix_embedding.3.bias\n"
     ]
    }
   ],
   "source": [
    "for n, p in trainer_clipt5.model.image_passage_encoder.named_parameters():\n",
    "    if p.requires_grad==True:\n",
    "        print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adapter config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VLT5 were not initialized from the model checkpoint at data_model/t5_pretrained and are newly initialized: ['encoder.block.0.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'encoder.block.0.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'encoder.block.0.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'encoder.block.0.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'encoder.block.0.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'encoder.block.0.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'encoder.block.0.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'encoder.block.0.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'encoder.block.1.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'encoder.block.1.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'encoder.block.1.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'encoder.block.1.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'encoder.block.1.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'encoder.block.1.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'encoder.block.1.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'encoder.block.1.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'encoder.block.2.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'encoder.block.2.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'encoder.block.2.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'encoder.block.2.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'encoder.block.2.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'encoder.block.2.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'encoder.block.2.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'encoder.block.2.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'encoder.block.3.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'encoder.block.3.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'encoder.block.3.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'encoder.block.3.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'encoder.block.3.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'encoder.block.3.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'encoder.block.3.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'encoder.block.3.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'encoder.block.4.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'encoder.block.4.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'encoder.block.4.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'encoder.block.4.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'encoder.block.4.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'encoder.block.4.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'encoder.block.4.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'encoder.block.4.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'encoder.block.5.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'encoder.block.5.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'encoder.block.5.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'encoder.block.5.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'encoder.block.5.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'encoder.block.5.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'encoder.block.5.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'encoder.block.5.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'encoder.block.6.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'encoder.block.6.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'encoder.block.6.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'encoder.block.6.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'encoder.block.6.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'encoder.block.6.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'encoder.block.6.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'encoder.block.6.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'encoder.block.7.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'encoder.block.7.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'encoder.block.7.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'encoder.block.7.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'encoder.block.7.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'encoder.block.7.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'encoder.block.7.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'encoder.block.7.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'encoder.block.8.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'encoder.block.8.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'encoder.block.8.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'encoder.block.8.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'encoder.block.8.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'encoder.block.8.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'encoder.block.8.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'encoder.block.8.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'encoder.block.9.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'encoder.block.9.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'encoder.block.9.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'encoder.block.9.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'encoder.block.9.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'encoder.block.9.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'encoder.block.9.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'encoder.block.9.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'encoder.block.10.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'encoder.block.10.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'encoder.block.10.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'encoder.block.10.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'encoder.block.10.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'encoder.block.10.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'encoder.block.10.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'encoder.block.10.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'encoder.block.11.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'encoder.block.11.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'encoder.block.11.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'encoder.block.11.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'encoder.block.11.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'encoder.block.11.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'encoder.block.11.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'encoder.block.11.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'encoder.visual_embedding.feat_embedding.0.weight', 'encoder.visual_embedding.feat_embedding.0.bias', 'encoder.visual_embedding.feat_embedding.1.weight', 'encoder.visual_embedding.absolute_vis_pos_embedding.0.weight', 'encoder.visual_embedding.absolute_vis_pos_embedding.0.bias', 'encoder.visual_embedding.absolute_vis_pos_embedding.1.weight', 'encoder.visual_embedding.obj_order_embedding.weight', 'encoder.visual_embedding.img_order_embedding.weight', 'decoder.block.0.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'decoder.block.0.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'decoder.block.0.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'decoder.block.0.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'decoder.block.0.layer.2.ff_adapter.adapters.IR.down_sampler.weight', 'decoder.block.0.layer.2.ff_adapter.adapters.IR.down_sampler.bias', 'decoder.block.0.layer.2.ff_adapter.adapters.IR.up_sampler.weight', 'decoder.block.0.layer.2.ff_adapter.adapters.IR.up_sampler.bias', 'decoder.block.1.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'decoder.block.1.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'decoder.block.1.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'decoder.block.1.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'decoder.block.1.layer.2.ff_adapter.adapters.IR.down_sampler.weight', 'decoder.block.1.layer.2.ff_adapter.adapters.IR.down_sampler.bias', 'decoder.block.1.layer.2.ff_adapter.adapters.IR.up_sampler.weight', 'decoder.block.1.layer.2.ff_adapter.adapters.IR.up_sampler.bias', 'decoder.block.2.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'decoder.block.2.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'decoder.block.2.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'decoder.block.2.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'decoder.block.2.layer.2.ff_adapter.adapters.IR.down_sampler.weight', 'decoder.block.2.layer.2.ff_adapter.adapters.IR.down_sampler.bias', 'decoder.block.2.layer.2.ff_adapter.adapters.IR.up_sampler.weight', 'decoder.block.2.layer.2.ff_adapter.adapters.IR.up_sampler.bias', 'decoder.block.3.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'decoder.block.3.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'decoder.block.3.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'decoder.block.3.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'decoder.block.3.layer.2.ff_adapter.adapters.IR.down_sampler.weight', 'decoder.block.3.layer.2.ff_adapter.adapters.IR.down_sampler.bias', 'decoder.block.3.layer.2.ff_adapter.adapters.IR.up_sampler.weight', 'decoder.block.3.layer.2.ff_adapter.adapters.IR.up_sampler.bias', 'decoder.block.4.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'decoder.block.4.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'decoder.block.4.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'decoder.block.4.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'decoder.block.4.layer.2.ff_adapter.adapters.IR.down_sampler.weight', 'decoder.block.4.layer.2.ff_adapter.adapters.IR.down_sampler.bias', 'decoder.block.4.layer.2.ff_adapter.adapters.IR.up_sampler.weight', 'decoder.block.4.layer.2.ff_adapter.adapters.IR.up_sampler.bias', 'decoder.block.5.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'decoder.block.5.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'decoder.block.5.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'decoder.block.5.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'decoder.block.5.layer.2.ff_adapter.adapters.IR.down_sampler.weight', 'decoder.block.5.layer.2.ff_adapter.adapters.IR.down_sampler.bias', 'decoder.block.5.layer.2.ff_adapter.adapters.IR.up_sampler.weight', 'decoder.block.5.layer.2.ff_adapter.adapters.IR.up_sampler.bias', 'decoder.block.6.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'decoder.block.6.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'decoder.block.6.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'decoder.block.6.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'decoder.block.6.layer.2.ff_adapter.adapters.IR.down_sampler.weight', 'decoder.block.6.layer.2.ff_adapter.adapters.IR.down_sampler.bias', 'decoder.block.6.layer.2.ff_adapter.adapters.IR.up_sampler.weight', 'decoder.block.6.layer.2.ff_adapter.adapters.IR.up_sampler.bias', 'decoder.block.7.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'decoder.block.7.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'decoder.block.7.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'decoder.block.7.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'decoder.block.7.layer.2.ff_adapter.adapters.IR.down_sampler.weight', 'decoder.block.7.layer.2.ff_adapter.adapters.IR.down_sampler.bias', 'decoder.block.7.layer.2.ff_adapter.adapters.IR.up_sampler.weight', 'decoder.block.7.layer.2.ff_adapter.adapters.IR.up_sampler.bias', 'decoder.block.8.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'decoder.block.8.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'decoder.block.8.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'decoder.block.8.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'decoder.block.8.layer.2.ff_adapter.adapters.IR.down_sampler.weight', 'decoder.block.8.layer.2.ff_adapter.adapters.IR.down_sampler.bias', 'decoder.block.8.layer.2.ff_adapter.adapters.IR.up_sampler.weight', 'decoder.block.8.layer.2.ff_adapter.adapters.IR.up_sampler.bias', 'decoder.block.9.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'decoder.block.9.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'decoder.block.9.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'decoder.block.9.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'decoder.block.9.layer.2.ff_adapter.adapters.IR.down_sampler.weight', 'decoder.block.9.layer.2.ff_adapter.adapters.IR.down_sampler.bias', 'decoder.block.9.layer.2.ff_adapter.adapters.IR.up_sampler.weight', 'decoder.block.9.layer.2.ff_adapter.adapters.IR.up_sampler.bias', 'decoder.block.10.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'decoder.block.10.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'decoder.block.10.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'decoder.block.10.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'decoder.block.10.layer.2.ff_adapter.adapters.IR.down_sampler.weight', 'decoder.block.10.layer.2.ff_adapter.adapters.IR.down_sampler.bias', 'decoder.block.10.layer.2.ff_adapter.adapters.IR.up_sampler.weight', 'decoder.block.10.layer.2.ff_adapter.adapters.IR.up_sampler.bias', 'decoder.block.11.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'decoder.block.11.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'decoder.block.11.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'decoder.block.11.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'decoder.block.11.layer.2.ff_adapter.adapters.IR.down_sampler.weight', 'decoder.block.11.layer.2.ff_adapter.adapters.IR.down_sampler.bias', 'decoder.block.11.layer.2.ff_adapter.adapters.IR.up_sampler.weight', 'decoder.block.11.layer.2.ff_adapter.adapters.IR.up_sampler.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of VLT5 were not initialized from the model checkpoint at data_model/t5_pretrained and are newly initialized: ['encoder.block.0.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'encoder.block.0.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'encoder.block.0.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'encoder.block.0.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'encoder.block.0.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'encoder.block.0.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'encoder.block.0.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'encoder.block.0.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'encoder.block.1.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'encoder.block.1.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'encoder.block.1.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'encoder.block.1.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'encoder.block.1.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'encoder.block.1.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'encoder.block.1.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'encoder.block.1.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'encoder.block.2.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'encoder.block.2.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'encoder.block.2.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'encoder.block.2.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'encoder.block.2.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'encoder.block.2.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'encoder.block.2.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'encoder.block.2.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'encoder.block.3.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'encoder.block.3.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'encoder.block.3.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'encoder.block.3.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'encoder.block.3.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'encoder.block.3.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'encoder.block.3.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'encoder.block.3.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'encoder.block.4.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'encoder.block.4.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'encoder.block.4.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'encoder.block.4.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'encoder.block.4.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'encoder.block.4.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'encoder.block.4.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'encoder.block.4.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'encoder.block.5.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'encoder.block.5.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'encoder.block.5.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'encoder.block.5.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'encoder.block.5.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'encoder.block.5.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'encoder.block.5.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'encoder.block.5.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'encoder.block.6.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'encoder.block.6.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'encoder.block.6.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'encoder.block.6.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'encoder.block.6.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'encoder.block.6.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'encoder.block.6.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'encoder.block.6.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'encoder.block.7.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'encoder.block.7.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'encoder.block.7.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'encoder.block.7.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'encoder.block.7.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'encoder.block.7.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'encoder.block.7.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'encoder.block.7.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'encoder.block.8.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'encoder.block.8.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'encoder.block.8.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'encoder.block.8.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'encoder.block.8.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'encoder.block.8.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'encoder.block.8.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'encoder.block.8.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'encoder.block.9.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'encoder.block.9.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'encoder.block.9.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'encoder.block.9.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'encoder.block.9.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'encoder.block.9.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'encoder.block.9.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'encoder.block.9.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'encoder.block.10.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'encoder.block.10.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'encoder.block.10.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'encoder.block.10.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'encoder.block.10.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'encoder.block.10.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'encoder.block.10.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'encoder.block.10.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'encoder.block.11.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'encoder.block.11.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'encoder.block.11.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'encoder.block.11.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'encoder.block.11.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'encoder.block.11.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'encoder.block.11.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'encoder.block.11.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'encoder.visual_embedding.feat_embedding.0.weight', 'encoder.visual_embedding.feat_embedding.0.bias', 'encoder.visual_embedding.feat_embedding.1.weight', 'encoder.visual_embedding.absolute_vis_pos_embedding.0.weight', 'encoder.visual_embedding.absolute_vis_pos_embedding.0.bias', 'encoder.visual_embedding.absolute_vis_pos_embedding.1.weight', 'encoder.visual_embedding.obj_order_embedding.weight', 'encoder.visual_embedding.img_order_embedding.weight', 'decoder.block.0.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'decoder.block.0.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'decoder.block.0.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'decoder.block.0.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'decoder.block.0.layer.2.ff_adapter.adapters.IR.down_sampler.weight', 'decoder.block.0.layer.2.ff_adapter.adapters.IR.down_sampler.bias', 'decoder.block.0.layer.2.ff_adapter.adapters.IR.up_sampler.weight', 'decoder.block.0.layer.2.ff_adapter.adapters.IR.up_sampler.bias', 'decoder.block.1.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'decoder.block.1.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'decoder.block.1.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'decoder.block.1.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'decoder.block.1.layer.2.ff_adapter.adapters.IR.down_sampler.weight', 'decoder.block.1.layer.2.ff_adapter.adapters.IR.down_sampler.bias', 'decoder.block.1.layer.2.ff_adapter.adapters.IR.up_sampler.weight', 'decoder.block.1.layer.2.ff_adapter.adapters.IR.up_sampler.bias', 'decoder.block.2.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'decoder.block.2.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'decoder.block.2.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'decoder.block.2.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'decoder.block.2.layer.2.ff_adapter.adapters.IR.down_sampler.weight', 'decoder.block.2.layer.2.ff_adapter.adapters.IR.down_sampler.bias', 'decoder.block.2.layer.2.ff_adapter.adapters.IR.up_sampler.weight', 'decoder.block.2.layer.2.ff_adapter.adapters.IR.up_sampler.bias', 'decoder.block.3.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'decoder.block.3.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'decoder.block.3.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'decoder.block.3.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'decoder.block.3.layer.2.ff_adapter.adapters.IR.down_sampler.weight', 'decoder.block.3.layer.2.ff_adapter.adapters.IR.down_sampler.bias', 'decoder.block.3.layer.2.ff_adapter.adapters.IR.up_sampler.weight', 'decoder.block.3.layer.2.ff_adapter.adapters.IR.up_sampler.bias', 'decoder.block.4.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'decoder.block.4.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'decoder.block.4.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'decoder.block.4.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'decoder.block.4.layer.2.ff_adapter.adapters.IR.down_sampler.weight', 'decoder.block.4.layer.2.ff_adapter.adapters.IR.down_sampler.bias', 'decoder.block.4.layer.2.ff_adapter.adapters.IR.up_sampler.weight', 'decoder.block.4.layer.2.ff_adapter.adapters.IR.up_sampler.bias', 'decoder.block.5.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'decoder.block.5.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'decoder.block.5.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'decoder.block.5.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'decoder.block.5.layer.2.ff_adapter.adapters.IR.down_sampler.weight', 'decoder.block.5.layer.2.ff_adapter.adapters.IR.down_sampler.bias', 'decoder.block.5.layer.2.ff_adapter.adapters.IR.up_sampler.weight', 'decoder.block.5.layer.2.ff_adapter.adapters.IR.up_sampler.bias', 'decoder.block.6.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'decoder.block.6.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'decoder.block.6.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'decoder.block.6.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'decoder.block.6.layer.2.ff_adapter.adapters.IR.down_sampler.weight', 'decoder.block.6.layer.2.ff_adapter.adapters.IR.down_sampler.bias', 'decoder.block.6.layer.2.ff_adapter.adapters.IR.up_sampler.weight', 'decoder.block.6.layer.2.ff_adapter.adapters.IR.up_sampler.bias', 'decoder.block.7.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'decoder.block.7.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'decoder.block.7.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'decoder.block.7.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'decoder.block.7.layer.2.ff_adapter.adapters.IR.down_sampler.weight', 'decoder.block.7.layer.2.ff_adapter.adapters.IR.down_sampler.bias', 'decoder.block.7.layer.2.ff_adapter.adapters.IR.up_sampler.weight', 'decoder.block.7.layer.2.ff_adapter.adapters.IR.up_sampler.bias', 'decoder.block.8.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'decoder.block.8.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'decoder.block.8.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'decoder.block.8.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'decoder.block.8.layer.2.ff_adapter.adapters.IR.down_sampler.weight', 'decoder.block.8.layer.2.ff_adapter.adapters.IR.down_sampler.bias', 'decoder.block.8.layer.2.ff_adapter.adapters.IR.up_sampler.weight', 'decoder.block.8.layer.2.ff_adapter.adapters.IR.up_sampler.bias', 'decoder.block.9.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'decoder.block.9.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'decoder.block.9.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'decoder.block.9.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'decoder.block.9.layer.2.ff_adapter.adapters.IR.down_sampler.weight', 'decoder.block.9.layer.2.ff_adapter.adapters.IR.down_sampler.bias', 'decoder.block.9.layer.2.ff_adapter.adapters.IR.up_sampler.weight', 'decoder.block.9.layer.2.ff_adapter.adapters.IR.up_sampler.bias', 'decoder.block.10.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'decoder.block.10.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'decoder.block.10.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'decoder.block.10.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'decoder.block.10.layer.2.ff_adapter.adapters.IR.down_sampler.weight', 'decoder.block.10.layer.2.ff_adapter.adapters.IR.down_sampler.bias', 'decoder.block.10.layer.2.ff_adapter.adapters.IR.up_sampler.weight', 'decoder.block.10.layer.2.ff_adapter.adapters.IR.up_sampler.bias', 'decoder.block.11.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'decoder.block.11.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'decoder.block.11.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'decoder.block.11.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'decoder.block.11.layer.2.ff_adapter.adapters.IR.down_sampler.weight', 'decoder.block.11.layer.2.ff_adapter.adapters.IR.down_sampler.bias', 'decoder.block.11.layer.2.ff_adapter.adapters.IR.up_sampler.weight', 'decoder.block.11.layer.2.ff_adapter.adapters.IR.up_sampler.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from  data_model/whole_model/vlt5/VLT5Epoch30.pth\n",
      "_IncompatibleKeys(missing_keys=['encoder.block.0.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'encoder.block.0.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'encoder.block.0.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'encoder.block.0.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'encoder.block.0.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'encoder.block.0.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'encoder.block.0.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'encoder.block.0.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'encoder.block.1.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'encoder.block.1.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'encoder.block.1.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'encoder.block.1.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'encoder.block.1.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'encoder.block.1.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'encoder.block.1.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'encoder.block.1.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'encoder.block.2.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'encoder.block.2.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'encoder.block.2.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'encoder.block.2.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'encoder.block.2.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'encoder.block.2.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'encoder.block.2.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'encoder.block.2.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'encoder.block.3.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'encoder.block.3.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'encoder.block.3.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'encoder.block.3.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'encoder.block.3.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'encoder.block.3.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'encoder.block.3.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'encoder.block.3.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'encoder.block.4.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'encoder.block.4.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'encoder.block.4.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'encoder.block.4.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'encoder.block.4.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'encoder.block.4.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'encoder.block.4.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'encoder.block.4.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'encoder.block.5.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'encoder.block.5.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'encoder.block.5.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'encoder.block.5.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'encoder.block.5.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'encoder.block.5.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'encoder.block.5.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'encoder.block.5.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'encoder.block.6.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'encoder.block.6.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'encoder.block.6.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'encoder.block.6.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'encoder.block.6.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'encoder.block.6.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'encoder.block.6.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'encoder.block.6.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'encoder.block.7.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'encoder.block.7.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'encoder.block.7.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'encoder.block.7.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'encoder.block.7.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'encoder.block.7.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'encoder.block.7.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'encoder.block.7.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'encoder.block.8.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'encoder.block.8.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'encoder.block.8.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'encoder.block.8.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'encoder.block.8.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'encoder.block.8.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'encoder.block.8.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'encoder.block.8.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'encoder.block.9.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'encoder.block.9.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'encoder.block.9.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'encoder.block.9.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'encoder.block.9.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'encoder.block.9.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'encoder.block.9.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'encoder.block.9.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'encoder.block.10.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'encoder.block.10.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'encoder.block.10.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'encoder.block.10.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'encoder.block.10.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'encoder.block.10.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'encoder.block.10.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'encoder.block.10.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'encoder.block.11.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'encoder.block.11.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'encoder.block.11.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'encoder.block.11.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'encoder.block.11.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'encoder.block.11.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'encoder.block.11.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'encoder.block.11.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'decoder.block.0.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'decoder.block.0.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'decoder.block.0.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'decoder.block.0.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'decoder.block.0.layer.2.ff_adapter.adapters.IR.down_sampler.weight', 'decoder.block.0.layer.2.ff_adapter.adapters.IR.down_sampler.bias', 'decoder.block.0.layer.2.ff_adapter.adapters.IR.up_sampler.weight', 'decoder.block.0.layer.2.ff_adapter.adapters.IR.up_sampler.bias', 'decoder.block.1.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'decoder.block.1.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'decoder.block.1.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'decoder.block.1.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'decoder.block.1.layer.2.ff_adapter.adapters.IR.down_sampler.weight', 'decoder.block.1.layer.2.ff_adapter.adapters.IR.down_sampler.bias', 'decoder.block.1.layer.2.ff_adapter.adapters.IR.up_sampler.weight', 'decoder.block.1.layer.2.ff_adapter.adapters.IR.up_sampler.bias', 'decoder.block.2.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'decoder.block.2.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'decoder.block.2.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'decoder.block.2.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'decoder.block.2.layer.2.ff_adapter.adapters.IR.down_sampler.weight', 'decoder.block.2.layer.2.ff_adapter.adapters.IR.down_sampler.bias', 'decoder.block.2.layer.2.ff_adapter.adapters.IR.up_sampler.weight', 'decoder.block.2.layer.2.ff_adapter.adapters.IR.up_sampler.bias', 'decoder.block.3.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'decoder.block.3.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'decoder.block.3.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'decoder.block.3.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'decoder.block.3.layer.2.ff_adapter.adapters.IR.down_sampler.weight', 'decoder.block.3.layer.2.ff_adapter.adapters.IR.down_sampler.bias', 'decoder.block.3.layer.2.ff_adapter.adapters.IR.up_sampler.weight', 'decoder.block.3.layer.2.ff_adapter.adapters.IR.up_sampler.bias', 'decoder.block.4.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'decoder.block.4.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'decoder.block.4.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'decoder.block.4.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'decoder.block.4.layer.2.ff_adapter.adapters.IR.down_sampler.weight', 'decoder.block.4.layer.2.ff_adapter.adapters.IR.down_sampler.bias', 'decoder.block.4.layer.2.ff_adapter.adapters.IR.up_sampler.weight', 'decoder.block.4.layer.2.ff_adapter.adapters.IR.up_sampler.bias', 'decoder.block.5.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'decoder.block.5.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'decoder.block.5.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'decoder.block.5.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'decoder.block.5.layer.2.ff_adapter.adapters.IR.down_sampler.weight', 'decoder.block.5.layer.2.ff_adapter.adapters.IR.down_sampler.bias', 'decoder.block.5.layer.2.ff_adapter.adapters.IR.up_sampler.weight', 'decoder.block.5.layer.2.ff_adapter.adapters.IR.up_sampler.bias', 'decoder.block.6.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'decoder.block.6.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'decoder.block.6.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'decoder.block.6.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'decoder.block.6.layer.2.ff_adapter.adapters.IR.down_sampler.weight', 'decoder.block.6.layer.2.ff_adapter.adapters.IR.down_sampler.bias', 'decoder.block.6.layer.2.ff_adapter.adapters.IR.up_sampler.weight', 'decoder.block.6.layer.2.ff_adapter.adapters.IR.up_sampler.bias', 'decoder.block.7.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'decoder.block.7.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'decoder.block.7.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'decoder.block.7.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'decoder.block.7.layer.2.ff_adapter.adapters.IR.down_sampler.weight', 'decoder.block.7.layer.2.ff_adapter.adapters.IR.down_sampler.bias', 'decoder.block.7.layer.2.ff_adapter.adapters.IR.up_sampler.weight', 'decoder.block.7.layer.2.ff_adapter.adapters.IR.up_sampler.bias', 'decoder.block.8.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'decoder.block.8.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'decoder.block.8.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'decoder.block.8.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'decoder.block.8.layer.2.ff_adapter.adapters.IR.down_sampler.weight', 'decoder.block.8.layer.2.ff_adapter.adapters.IR.down_sampler.bias', 'decoder.block.8.layer.2.ff_adapter.adapters.IR.up_sampler.weight', 'decoder.block.8.layer.2.ff_adapter.adapters.IR.up_sampler.bias', 'decoder.block.9.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'decoder.block.9.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'decoder.block.9.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'decoder.block.9.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'decoder.block.9.layer.2.ff_adapter.adapters.IR.down_sampler.weight', 'decoder.block.9.layer.2.ff_adapter.adapters.IR.down_sampler.bias', 'decoder.block.9.layer.2.ff_adapter.adapters.IR.up_sampler.weight', 'decoder.block.9.layer.2.ff_adapter.adapters.IR.up_sampler.bias', 'decoder.block.10.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'decoder.block.10.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'decoder.block.10.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'decoder.block.10.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'decoder.block.10.layer.2.ff_adapter.adapters.IR.down_sampler.weight', 'decoder.block.10.layer.2.ff_adapter.adapters.IR.down_sampler.bias', 'decoder.block.10.layer.2.ff_adapter.adapters.IR.up_sampler.weight', 'decoder.block.10.layer.2.ff_adapter.adapters.IR.up_sampler.bias', 'decoder.block.11.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'decoder.block.11.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'decoder.block.11.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'decoder.block.11.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'decoder.block.11.layer.2.ff_adapter.adapters.IR.down_sampler.weight', 'decoder.block.11.layer.2.ff_adapter.adapters.IR.down_sampler.bias', 'decoder.block.11.layer.2.ff_adapter.adapters.IR.up_sampler.weight', 'decoder.block.11.layer.2.ff_adapter.adapters.IR.up_sampler.bias'], unexpected_keys=['encoder.visual_embedding.layer_norm.weight'])\n",
      "Model loaded from  data_model/whole_model/vlt5/VLT5Epoch30.pth\n",
      "_IncompatibleKeys(missing_keys=['encoder.block.0.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'encoder.block.0.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'encoder.block.0.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'encoder.block.0.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'encoder.block.0.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'encoder.block.0.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'encoder.block.0.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'encoder.block.0.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'encoder.block.1.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'encoder.block.1.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'encoder.block.1.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'encoder.block.1.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'encoder.block.1.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'encoder.block.1.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'encoder.block.1.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'encoder.block.1.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'encoder.block.2.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'encoder.block.2.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'encoder.block.2.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'encoder.block.2.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'encoder.block.2.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'encoder.block.2.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'encoder.block.2.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'encoder.block.2.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'encoder.block.3.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'encoder.block.3.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'encoder.block.3.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'encoder.block.3.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'encoder.block.3.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'encoder.block.3.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'encoder.block.3.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'encoder.block.3.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'encoder.block.4.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'encoder.block.4.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'encoder.block.4.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'encoder.block.4.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'encoder.block.4.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'encoder.block.4.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'encoder.block.4.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'encoder.block.4.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'encoder.block.5.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'encoder.block.5.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'encoder.block.5.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'encoder.block.5.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'encoder.block.5.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'encoder.block.5.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'encoder.block.5.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'encoder.block.5.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'encoder.block.6.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'encoder.block.6.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'encoder.block.6.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'encoder.block.6.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'encoder.block.6.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'encoder.block.6.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'encoder.block.6.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'encoder.block.6.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'encoder.block.7.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'encoder.block.7.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'encoder.block.7.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'encoder.block.7.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'encoder.block.7.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'encoder.block.7.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'encoder.block.7.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'encoder.block.7.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'encoder.block.8.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'encoder.block.8.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'encoder.block.8.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'encoder.block.8.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'encoder.block.8.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'encoder.block.8.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'encoder.block.8.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'encoder.block.8.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'encoder.block.9.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'encoder.block.9.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'encoder.block.9.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'encoder.block.9.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'encoder.block.9.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'encoder.block.9.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'encoder.block.9.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'encoder.block.9.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'encoder.block.10.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'encoder.block.10.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'encoder.block.10.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'encoder.block.10.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'encoder.block.10.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'encoder.block.10.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'encoder.block.10.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'encoder.block.10.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'encoder.block.11.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'encoder.block.11.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'encoder.block.11.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'encoder.block.11.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'encoder.block.11.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'encoder.block.11.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'encoder.block.11.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'encoder.block.11.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'decoder.block.0.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'decoder.block.0.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'decoder.block.0.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'decoder.block.0.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'decoder.block.0.layer.2.ff_adapter.adapters.IR.down_sampler.weight', 'decoder.block.0.layer.2.ff_adapter.adapters.IR.down_sampler.bias', 'decoder.block.0.layer.2.ff_adapter.adapters.IR.up_sampler.weight', 'decoder.block.0.layer.2.ff_adapter.adapters.IR.up_sampler.bias', 'decoder.block.1.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'decoder.block.1.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'decoder.block.1.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'decoder.block.1.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'decoder.block.1.layer.2.ff_adapter.adapters.IR.down_sampler.weight', 'decoder.block.1.layer.2.ff_adapter.adapters.IR.down_sampler.bias', 'decoder.block.1.layer.2.ff_adapter.adapters.IR.up_sampler.weight', 'decoder.block.1.layer.2.ff_adapter.adapters.IR.up_sampler.bias', 'decoder.block.2.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'decoder.block.2.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'decoder.block.2.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'decoder.block.2.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'decoder.block.2.layer.2.ff_adapter.adapters.IR.down_sampler.weight', 'decoder.block.2.layer.2.ff_adapter.adapters.IR.down_sampler.bias', 'decoder.block.2.layer.2.ff_adapter.adapters.IR.up_sampler.weight', 'decoder.block.2.layer.2.ff_adapter.adapters.IR.up_sampler.bias', 'decoder.block.3.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'decoder.block.3.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'decoder.block.3.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'decoder.block.3.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'decoder.block.3.layer.2.ff_adapter.adapters.IR.down_sampler.weight', 'decoder.block.3.layer.2.ff_adapter.adapters.IR.down_sampler.bias', 'decoder.block.3.layer.2.ff_adapter.adapters.IR.up_sampler.weight', 'decoder.block.3.layer.2.ff_adapter.adapters.IR.up_sampler.bias', 'decoder.block.4.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'decoder.block.4.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'decoder.block.4.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'decoder.block.4.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'decoder.block.4.layer.2.ff_adapter.adapters.IR.down_sampler.weight', 'decoder.block.4.layer.2.ff_adapter.adapters.IR.down_sampler.bias', 'decoder.block.4.layer.2.ff_adapter.adapters.IR.up_sampler.weight', 'decoder.block.4.layer.2.ff_adapter.adapters.IR.up_sampler.bias', 'decoder.block.5.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'decoder.block.5.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'decoder.block.5.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'decoder.block.5.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'decoder.block.5.layer.2.ff_adapter.adapters.IR.down_sampler.weight', 'decoder.block.5.layer.2.ff_adapter.adapters.IR.down_sampler.bias', 'decoder.block.5.layer.2.ff_adapter.adapters.IR.up_sampler.weight', 'decoder.block.5.layer.2.ff_adapter.adapters.IR.up_sampler.bias', 'decoder.block.6.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'decoder.block.6.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'decoder.block.6.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'decoder.block.6.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'decoder.block.6.layer.2.ff_adapter.adapters.IR.down_sampler.weight', 'decoder.block.6.layer.2.ff_adapter.adapters.IR.down_sampler.bias', 'decoder.block.6.layer.2.ff_adapter.adapters.IR.up_sampler.weight', 'decoder.block.6.layer.2.ff_adapter.adapters.IR.up_sampler.bias', 'decoder.block.7.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'decoder.block.7.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'decoder.block.7.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'decoder.block.7.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'decoder.block.7.layer.2.ff_adapter.adapters.IR.down_sampler.weight', 'decoder.block.7.layer.2.ff_adapter.adapters.IR.down_sampler.bias', 'decoder.block.7.layer.2.ff_adapter.adapters.IR.up_sampler.weight', 'decoder.block.7.layer.2.ff_adapter.adapters.IR.up_sampler.bias', 'decoder.block.8.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'decoder.block.8.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'decoder.block.8.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'decoder.block.8.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'decoder.block.8.layer.2.ff_adapter.adapters.IR.down_sampler.weight', 'decoder.block.8.layer.2.ff_adapter.adapters.IR.down_sampler.bias', 'decoder.block.8.layer.2.ff_adapter.adapters.IR.up_sampler.weight', 'decoder.block.8.layer.2.ff_adapter.adapters.IR.up_sampler.bias', 'decoder.block.9.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'decoder.block.9.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'decoder.block.9.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'decoder.block.9.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'decoder.block.9.layer.2.ff_adapter.adapters.IR.down_sampler.weight', 'decoder.block.9.layer.2.ff_adapter.adapters.IR.down_sampler.bias', 'decoder.block.9.layer.2.ff_adapter.adapters.IR.up_sampler.weight', 'decoder.block.9.layer.2.ff_adapter.adapters.IR.up_sampler.bias', 'decoder.block.10.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'decoder.block.10.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'decoder.block.10.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'decoder.block.10.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'decoder.block.10.layer.2.ff_adapter.adapters.IR.down_sampler.weight', 'decoder.block.10.layer.2.ff_adapter.adapters.IR.down_sampler.bias', 'decoder.block.10.layer.2.ff_adapter.adapters.IR.up_sampler.weight', 'decoder.block.10.layer.2.ff_adapter.adapters.IR.up_sampler.bias', 'decoder.block.11.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'decoder.block.11.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'decoder.block.11.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'decoder.block.11.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'decoder.block.11.layer.2.ff_adapter.adapters.IR.down_sampler.weight', 'decoder.block.11.layer.2.ff_adapter.adapters.IR.down_sampler.bias', 'decoder.block.11.layer.2.ff_adapter.adapters.IR.up_sampler.weight', 'decoder.block.11.layer.2.ff_adapter.adapters.IR.up_sampler.bias'], unexpected_keys=['encoder.visual_embedding.layer_norm.weight'])\n",
      "None\n",
      "encoder.block.0.layer.0.attn_adapter is trainable...\n",
      "encoder.block.0.layer.1.ff_adapter is trainable...\n",
      "encoder.block.1.layer.0.attn_adapter is trainable...\n",
      "encoder.block.1.layer.1.ff_adapter is trainable...\n",
      "encoder.block.2.layer.0.attn_adapter is trainable...\n",
      "encoder.block.2.layer.1.ff_adapter is trainable...\n",
      "encoder.block.3.layer.0.attn_adapter is trainable...\n",
      "encoder.block.3.layer.1.ff_adapter is trainable...\n",
      "encoder.block.4.layer.0.attn_adapter is trainable...\n",
      "encoder.block.4.layer.1.ff_adapter is trainable...\n",
      "encoder.block.5.layer.0.attn_adapter is trainable...\n",
      "encoder.block.5.layer.1.ff_adapter is trainable...\n",
      "encoder.block.6.layer.0.attn_adapter is trainable...\n",
      "encoder.block.6.layer.1.ff_adapter is trainable...\n",
      "encoder.block.7.layer.0.attn_adapter is trainable...\n",
      "encoder.block.7.layer.1.ff_adapter is trainable...\n",
      "encoder.block.8.layer.0.attn_adapter is trainable...\n",
      "encoder.block.8.layer.1.ff_adapter is trainable...\n",
      "encoder.block.9.layer.0.attn_adapter is trainable...\n",
      "encoder.block.9.layer.1.ff_adapter is trainable...\n",
      "encoder.block.10.layer.0.attn_adapter is trainable...\n",
      "encoder.block.10.layer.1.ff_adapter is trainable...\n",
      "encoder.block.11.layer.0.attn_adapter is trainable...\n",
      "encoder.block.11.layer.1.ff_adapter is trainable...\n",
      "decoder.block.0.layer.0.attn_adapter is trainable...\n",
      "decoder.block.0.layer.2.ff_adapter is trainable...\n",
      "decoder.block.1.layer.0.attn_adapter is trainable...\n",
      "decoder.block.1.layer.2.ff_adapter is trainable...\n",
      "decoder.block.2.layer.0.attn_adapter is trainable...\n",
      "decoder.block.2.layer.2.ff_adapter is trainable...\n",
      "decoder.block.3.layer.0.attn_adapter is trainable...\n",
      "decoder.block.3.layer.2.ff_adapter is trainable...\n",
      "decoder.block.4.layer.0.attn_adapter is trainable...\n",
      "decoder.block.4.layer.2.ff_adapter is trainable...\n",
      "decoder.block.5.layer.0.attn_adapter is trainable...\n",
      "decoder.block.5.layer.2.ff_adapter is trainable...\n",
      "decoder.block.6.layer.0.attn_adapter is trainable...\n",
      "decoder.block.6.layer.2.ff_adapter is trainable...\n",
      "decoder.block.7.layer.0.attn_adapter is trainable...\n",
      "decoder.block.7.layer.2.ff_adapter is trainable...\n",
      "decoder.block.8.layer.0.attn_adapter is trainable...\n",
      "decoder.block.8.layer.2.ff_adapter is trainable...\n",
      "decoder.block.9.layer.0.attn_adapter is trainable...\n",
      "decoder.block.9.layer.2.ff_adapter is trainable...\n",
      "decoder.block.10.layer.0.attn_adapter is trainable...\n",
      "decoder.block.10.layer.2.ff_adapter is trainable...\n",
      "decoder.block.11.layer.0.attn_adapter is trainable...\n",
      "decoder.block.11.layer.2.ff_adapter is trainable...\n",
      "encoder.block.0.layer.0.attn_adapter is trainable...\n",
      "encoder.block.0.layer.1.ff_adapter is trainable...\n",
      "encoder.block.1.layer.0.attn_adapter is trainable...\n",
      "encoder.block.1.layer.1.ff_adapter is trainable...\n",
      "encoder.block.2.layer.0.attn_adapter is trainable...\n",
      "encoder.block.2.layer.1.ff_adapter is trainable...\n",
      "encoder.block.3.layer.0.attn_adapter is trainable...\n",
      "encoder.block.3.layer.1.ff_adapter is trainable...\n",
      "encoder.block.4.layer.0.attn_adapter is trainable...\n",
      "encoder.block.4.layer.1.ff_adapter is trainable...\n",
      "encoder.block.5.layer.0.attn_adapter is trainable...\n",
      "encoder.block.5.layer.1.ff_adapter is trainable...\n",
      "encoder.block.6.layer.0.attn_adapter is trainable...\n",
      "encoder.block.6.layer.1.ff_adapter is trainable...\n",
      "encoder.block.7.layer.0.attn_adapter is trainable...\n",
      "encoder.block.7.layer.1.ff_adapter is trainable...\n",
      "encoder.block.8.layer.0.attn_adapter is trainable...\n",
      "encoder.block.8.layer.1.ff_adapter is trainable...\n",
      "encoder.block.9.layer.0.attn_adapter is trainable...\n",
      "encoder.block.9.layer.1.ff_adapter is trainable...\n",
      "encoder.block.10.layer.0.attn_adapter is trainable...\n",
      "encoder.block.10.layer.1.ff_adapter is trainable...\n",
      "encoder.block.11.layer.0.attn_adapter is trainable...\n",
      "encoder.block.11.layer.1.ff_adapter is trainable...\n",
      "decoder.block.0.layer.0.attn_adapter is trainable...\n",
      "decoder.block.0.layer.2.ff_adapter is trainable...\n",
      "decoder.block.1.layer.0.attn_adapter is trainable...\n",
      "decoder.block.1.layer.2.ff_adapter is trainable...\n",
      "decoder.block.2.layer.0.attn_adapter is trainable...\n",
      "decoder.block.2.layer.2.ff_adapter is trainable...\n",
      "decoder.block.3.layer.0.attn_adapter is trainable...\n",
      "decoder.block.3.layer.2.ff_adapter is trainable...\n",
      "decoder.block.4.layer.0.attn_adapter is trainable...\n",
      "decoder.block.4.layer.2.ff_adapter is trainable...\n",
      "decoder.block.5.layer.0.attn_adapter is trainable...\n",
      "decoder.block.5.layer.2.ff_adapter is trainable...\n",
      "decoder.block.6.layer.0.attn_adapter is trainable...\n",
      "decoder.block.6.layer.2.ff_adapter is trainable...\n",
      "decoder.block.7.layer.0.attn_adapter is trainable...\n",
      "decoder.block.7.layer.2.ff_adapter is trainable...\n",
      "decoder.block.8.layer.0.attn_adapter is trainable...\n",
      "decoder.block.8.layer.2.ff_adapter is trainable...\n",
      "decoder.block.9.layer.0.attn_adapter is trainable...\n",
      "decoder.block.9.layer.2.ff_adapter is trainable...\n",
      "decoder.block.10.layer.0.attn_adapter is trainable...\n",
      "decoder.block.10.layer.2.ff_adapter is trainable...\n",
      "decoder.block.11.layer.0.attn_adapter is trainable...\n",
      "decoder.block.11.layer.2.ff_adapter is trainable...\n"
     ]
    }
   ],
   "source": [
    "from cvlep.trainer_base_vladapter import Trainer\n",
    "\n",
    "config_encoder_question = \"experiments/config_vladapter/adapter_for_contrastive/encoder_simple_adapter.json\"\n",
    "config_encoder_passage = \"experiments/config_vladapter/adapter_for_contrastive/encoder_simple_adapter.json\"\n",
    "config_training = \"experiments/config_vladapter/adapter_for_contrastive/training_simple_adapter.json\"\n",
    "\n",
    "trainer_clipt5 = Trainer(config_encoder_question, config_encoder_passage, config_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.block.0.layer.0.attn_adapter.adapters.IR.down_sampler.weight\n",
      "encoder.block.0.layer.0.attn_adapter.adapters.IR.down_sampler.bias\n",
      "encoder.block.0.layer.0.attn_adapter.adapters.IR.up_sampler.weight\n",
      "encoder.block.0.layer.0.attn_adapter.adapters.IR.up_sampler.bias\n",
      "encoder.block.0.layer.1.ff_adapter.adapters.IR.down_sampler.weight\n",
      "encoder.block.0.layer.1.ff_adapter.adapters.IR.down_sampler.bias\n",
      "encoder.block.0.layer.1.ff_adapter.adapters.IR.up_sampler.weight\n",
      "encoder.block.0.layer.1.ff_adapter.adapters.IR.up_sampler.bias\n",
      "encoder.block.1.layer.0.attn_adapter.adapters.IR.down_sampler.weight\n",
      "encoder.block.1.layer.0.attn_adapter.adapters.IR.down_sampler.bias\n",
      "encoder.block.1.layer.0.attn_adapter.adapters.IR.up_sampler.weight\n",
      "encoder.block.1.layer.0.attn_adapter.adapters.IR.up_sampler.bias\n",
      "encoder.block.1.layer.1.ff_adapter.adapters.IR.down_sampler.weight\n",
      "encoder.block.1.layer.1.ff_adapter.adapters.IR.down_sampler.bias\n",
      "encoder.block.1.layer.1.ff_adapter.adapters.IR.up_sampler.weight\n",
      "encoder.block.1.layer.1.ff_adapter.adapters.IR.up_sampler.bias\n",
      "encoder.block.2.layer.0.attn_adapter.adapters.IR.down_sampler.weight\n",
      "encoder.block.2.layer.0.attn_adapter.adapters.IR.down_sampler.bias\n",
      "encoder.block.2.layer.0.attn_adapter.adapters.IR.up_sampler.weight\n",
      "encoder.block.2.layer.0.attn_adapter.adapters.IR.up_sampler.bias\n",
      "encoder.block.2.layer.1.ff_adapter.adapters.IR.down_sampler.weight\n",
      "encoder.block.2.layer.1.ff_adapter.adapters.IR.down_sampler.bias\n",
      "encoder.block.2.layer.1.ff_adapter.adapters.IR.up_sampler.weight\n",
      "encoder.block.2.layer.1.ff_adapter.adapters.IR.up_sampler.bias\n",
      "encoder.block.3.layer.0.attn_adapter.adapters.IR.down_sampler.weight\n",
      "encoder.block.3.layer.0.attn_adapter.adapters.IR.down_sampler.bias\n",
      "encoder.block.3.layer.0.attn_adapter.adapters.IR.up_sampler.weight\n",
      "encoder.block.3.layer.0.attn_adapter.adapters.IR.up_sampler.bias\n",
      "encoder.block.3.layer.1.ff_adapter.adapters.IR.down_sampler.weight\n",
      "encoder.block.3.layer.1.ff_adapter.adapters.IR.down_sampler.bias\n",
      "encoder.block.3.layer.1.ff_adapter.adapters.IR.up_sampler.weight\n",
      "encoder.block.3.layer.1.ff_adapter.adapters.IR.up_sampler.bias\n",
      "encoder.block.4.layer.0.attn_adapter.adapters.IR.down_sampler.weight\n",
      "encoder.block.4.layer.0.attn_adapter.adapters.IR.down_sampler.bias\n",
      "encoder.block.4.layer.0.attn_adapter.adapters.IR.up_sampler.weight\n",
      "encoder.block.4.layer.0.attn_adapter.adapters.IR.up_sampler.bias\n",
      "encoder.block.4.layer.1.ff_adapter.adapters.IR.down_sampler.weight\n",
      "encoder.block.4.layer.1.ff_adapter.adapters.IR.down_sampler.bias\n",
      "encoder.block.4.layer.1.ff_adapter.adapters.IR.up_sampler.weight\n",
      "encoder.block.4.layer.1.ff_adapter.adapters.IR.up_sampler.bias\n",
      "encoder.block.5.layer.0.attn_adapter.adapters.IR.down_sampler.weight\n",
      "encoder.block.5.layer.0.attn_adapter.adapters.IR.down_sampler.bias\n",
      "encoder.block.5.layer.0.attn_adapter.adapters.IR.up_sampler.weight\n",
      "encoder.block.5.layer.0.attn_adapter.adapters.IR.up_sampler.bias\n",
      "encoder.block.5.layer.1.ff_adapter.adapters.IR.down_sampler.weight\n",
      "encoder.block.5.layer.1.ff_adapter.adapters.IR.down_sampler.bias\n",
      "encoder.block.5.layer.1.ff_adapter.adapters.IR.up_sampler.weight\n",
      "encoder.block.5.layer.1.ff_adapter.adapters.IR.up_sampler.bias\n",
      "encoder.block.6.layer.0.attn_adapter.adapters.IR.down_sampler.weight\n",
      "encoder.block.6.layer.0.attn_adapter.adapters.IR.down_sampler.bias\n",
      "encoder.block.6.layer.0.attn_adapter.adapters.IR.up_sampler.weight\n",
      "encoder.block.6.layer.0.attn_adapter.adapters.IR.up_sampler.bias\n",
      "encoder.block.6.layer.1.ff_adapter.adapters.IR.down_sampler.weight\n",
      "encoder.block.6.layer.1.ff_adapter.adapters.IR.down_sampler.bias\n",
      "encoder.block.6.layer.1.ff_adapter.adapters.IR.up_sampler.weight\n",
      "encoder.block.6.layer.1.ff_adapter.adapters.IR.up_sampler.bias\n",
      "encoder.block.7.layer.0.attn_adapter.adapters.IR.down_sampler.weight\n",
      "encoder.block.7.layer.0.attn_adapter.adapters.IR.down_sampler.bias\n",
      "encoder.block.7.layer.0.attn_adapter.adapters.IR.up_sampler.weight\n",
      "encoder.block.7.layer.0.attn_adapter.adapters.IR.up_sampler.bias\n",
      "encoder.block.7.layer.1.ff_adapter.adapters.IR.down_sampler.weight\n",
      "encoder.block.7.layer.1.ff_adapter.adapters.IR.down_sampler.bias\n",
      "encoder.block.7.layer.1.ff_adapter.adapters.IR.up_sampler.weight\n",
      "encoder.block.7.layer.1.ff_adapter.adapters.IR.up_sampler.bias\n",
      "encoder.block.8.layer.0.attn_adapter.adapters.IR.down_sampler.weight\n",
      "encoder.block.8.layer.0.attn_adapter.adapters.IR.down_sampler.bias\n",
      "encoder.block.8.layer.0.attn_adapter.adapters.IR.up_sampler.weight\n",
      "encoder.block.8.layer.0.attn_adapter.adapters.IR.up_sampler.bias\n",
      "encoder.block.8.layer.1.ff_adapter.adapters.IR.down_sampler.weight\n",
      "encoder.block.8.layer.1.ff_adapter.adapters.IR.down_sampler.bias\n",
      "encoder.block.8.layer.1.ff_adapter.adapters.IR.up_sampler.weight\n",
      "encoder.block.8.layer.1.ff_adapter.adapters.IR.up_sampler.bias\n",
      "encoder.block.9.layer.0.attn_adapter.adapters.IR.down_sampler.weight\n",
      "encoder.block.9.layer.0.attn_adapter.adapters.IR.down_sampler.bias\n",
      "encoder.block.9.layer.0.attn_adapter.adapters.IR.up_sampler.weight\n",
      "encoder.block.9.layer.0.attn_adapter.adapters.IR.up_sampler.bias\n",
      "encoder.block.9.layer.1.ff_adapter.adapters.IR.down_sampler.weight\n",
      "encoder.block.9.layer.1.ff_adapter.adapters.IR.down_sampler.bias\n",
      "encoder.block.9.layer.1.ff_adapter.adapters.IR.up_sampler.weight\n",
      "encoder.block.9.layer.1.ff_adapter.adapters.IR.up_sampler.bias\n",
      "encoder.block.10.layer.0.attn_adapter.adapters.IR.down_sampler.weight\n",
      "encoder.block.10.layer.0.attn_adapter.adapters.IR.down_sampler.bias\n",
      "encoder.block.10.layer.0.attn_adapter.adapters.IR.up_sampler.weight\n",
      "encoder.block.10.layer.0.attn_adapter.adapters.IR.up_sampler.bias\n",
      "encoder.block.10.layer.1.ff_adapter.adapters.IR.down_sampler.weight\n",
      "encoder.block.10.layer.1.ff_adapter.adapters.IR.down_sampler.bias\n",
      "encoder.block.10.layer.1.ff_adapter.adapters.IR.up_sampler.weight\n",
      "encoder.block.10.layer.1.ff_adapter.adapters.IR.up_sampler.bias\n",
      "encoder.block.11.layer.0.attn_adapter.adapters.IR.down_sampler.weight\n",
      "encoder.block.11.layer.0.attn_adapter.adapters.IR.down_sampler.bias\n",
      "encoder.block.11.layer.0.attn_adapter.adapters.IR.up_sampler.weight\n",
      "encoder.block.11.layer.0.attn_adapter.adapters.IR.up_sampler.bias\n",
      "encoder.block.11.layer.1.ff_adapter.adapters.IR.down_sampler.weight\n",
      "encoder.block.11.layer.1.ff_adapter.adapters.IR.down_sampler.bias\n",
      "encoder.block.11.layer.1.ff_adapter.adapters.IR.up_sampler.weight\n",
      "encoder.block.11.layer.1.ff_adapter.adapters.IR.up_sampler.bias\n",
      "decoder.block.0.layer.0.attn_adapter.adapters.IR.down_sampler.weight\n",
      "decoder.block.0.layer.0.attn_adapter.adapters.IR.down_sampler.bias\n",
      "decoder.block.0.layer.0.attn_adapter.adapters.IR.up_sampler.weight\n",
      "decoder.block.0.layer.0.attn_adapter.adapters.IR.up_sampler.bias\n",
      "decoder.block.0.layer.2.ff_adapter.adapters.IR.down_sampler.weight\n",
      "decoder.block.0.layer.2.ff_adapter.adapters.IR.down_sampler.bias\n",
      "decoder.block.0.layer.2.ff_adapter.adapters.IR.up_sampler.weight\n",
      "decoder.block.0.layer.2.ff_adapter.adapters.IR.up_sampler.bias\n",
      "decoder.block.1.layer.0.attn_adapter.adapters.IR.down_sampler.weight\n",
      "decoder.block.1.layer.0.attn_adapter.adapters.IR.down_sampler.bias\n",
      "decoder.block.1.layer.0.attn_adapter.adapters.IR.up_sampler.weight\n",
      "decoder.block.1.layer.0.attn_adapter.adapters.IR.up_sampler.bias\n",
      "decoder.block.1.layer.2.ff_adapter.adapters.IR.down_sampler.weight\n",
      "decoder.block.1.layer.2.ff_adapter.adapters.IR.down_sampler.bias\n",
      "decoder.block.1.layer.2.ff_adapter.adapters.IR.up_sampler.weight\n",
      "decoder.block.1.layer.2.ff_adapter.adapters.IR.up_sampler.bias\n",
      "decoder.block.2.layer.0.attn_adapter.adapters.IR.down_sampler.weight\n",
      "decoder.block.2.layer.0.attn_adapter.adapters.IR.down_sampler.bias\n",
      "decoder.block.2.layer.0.attn_adapter.adapters.IR.up_sampler.weight\n",
      "decoder.block.2.layer.0.attn_adapter.adapters.IR.up_sampler.bias\n",
      "decoder.block.2.layer.2.ff_adapter.adapters.IR.down_sampler.weight\n",
      "decoder.block.2.layer.2.ff_adapter.adapters.IR.down_sampler.bias\n",
      "decoder.block.2.layer.2.ff_adapter.adapters.IR.up_sampler.weight\n",
      "decoder.block.2.layer.2.ff_adapter.adapters.IR.up_sampler.bias\n",
      "decoder.block.3.layer.0.attn_adapter.adapters.IR.down_sampler.weight\n",
      "decoder.block.3.layer.0.attn_adapter.adapters.IR.down_sampler.bias\n",
      "decoder.block.3.layer.0.attn_adapter.adapters.IR.up_sampler.weight\n",
      "decoder.block.3.layer.0.attn_adapter.adapters.IR.up_sampler.bias\n",
      "decoder.block.3.layer.2.ff_adapter.adapters.IR.down_sampler.weight\n",
      "decoder.block.3.layer.2.ff_adapter.adapters.IR.down_sampler.bias\n",
      "decoder.block.3.layer.2.ff_adapter.adapters.IR.up_sampler.weight\n",
      "decoder.block.3.layer.2.ff_adapter.adapters.IR.up_sampler.bias\n",
      "decoder.block.4.layer.0.attn_adapter.adapters.IR.down_sampler.weight\n",
      "decoder.block.4.layer.0.attn_adapter.adapters.IR.down_sampler.bias\n",
      "decoder.block.4.layer.0.attn_adapter.adapters.IR.up_sampler.weight\n",
      "decoder.block.4.layer.0.attn_adapter.adapters.IR.up_sampler.bias\n",
      "decoder.block.4.layer.2.ff_adapter.adapters.IR.down_sampler.weight\n",
      "decoder.block.4.layer.2.ff_adapter.adapters.IR.down_sampler.bias\n",
      "decoder.block.4.layer.2.ff_adapter.adapters.IR.up_sampler.weight\n",
      "decoder.block.4.layer.2.ff_adapter.adapters.IR.up_sampler.bias\n",
      "decoder.block.5.layer.0.attn_adapter.adapters.IR.down_sampler.weight\n",
      "decoder.block.5.layer.0.attn_adapter.adapters.IR.down_sampler.bias\n",
      "decoder.block.5.layer.0.attn_adapter.adapters.IR.up_sampler.weight\n",
      "decoder.block.5.layer.0.attn_adapter.adapters.IR.up_sampler.bias\n",
      "decoder.block.5.layer.2.ff_adapter.adapters.IR.down_sampler.weight\n",
      "decoder.block.5.layer.2.ff_adapter.adapters.IR.down_sampler.bias\n",
      "decoder.block.5.layer.2.ff_adapter.adapters.IR.up_sampler.weight\n",
      "decoder.block.5.layer.2.ff_adapter.adapters.IR.up_sampler.bias\n",
      "decoder.block.6.layer.0.attn_adapter.adapters.IR.down_sampler.weight\n",
      "decoder.block.6.layer.0.attn_adapter.adapters.IR.down_sampler.bias\n",
      "decoder.block.6.layer.0.attn_adapter.adapters.IR.up_sampler.weight\n",
      "decoder.block.6.layer.0.attn_adapter.adapters.IR.up_sampler.bias\n",
      "decoder.block.6.layer.2.ff_adapter.adapters.IR.down_sampler.weight\n",
      "decoder.block.6.layer.2.ff_adapter.adapters.IR.down_sampler.bias\n",
      "decoder.block.6.layer.2.ff_adapter.adapters.IR.up_sampler.weight\n",
      "decoder.block.6.layer.2.ff_adapter.adapters.IR.up_sampler.bias\n",
      "decoder.block.7.layer.0.attn_adapter.adapters.IR.down_sampler.weight\n",
      "decoder.block.7.layer.0.attn_adapter.adapters.IR.down_sampler.bias\n",
      "decoder.block.7.layer.0.attn_adapter.adapters.IR.up_sampler.weight\n",
      "decoder.block.7.layer.0.attn_adapter.adapters.IR.up_sampler.bias\n",
      "decoder.block.7.layer.2.ff_adapter.adapters.IR.down_sampler.weight\n",
      "decoder.block.7.layer.2.ff_adapter.adapters.IR.down_sampler.bias\n",
      "decoder.block.7.layer.2.ff_adapter.adapters.IR.up_sampler.weight\n",
      "decoder.block.7.layer.2.ff_adapter.adapters.IR.up_sampler.bias\n",
      "decoder.block.8.layer.0.attn_adapter.adapters.IR.down_sampler.weight\n",
      "decoder.block.8.layer.0.attn_adapter.adapters.IR.down_sampler.bias\n",
      "decoder.block.8.layer.0.attn_adapter.adapters.IR.up_sampler.weight\n",
      "decoder.block.8.layer.0.attn_adapter.adapters.IR.up_sampler.bias\n",
      "decoder.block.8.layer.2.ff_adapter.adapters.IR.down_sampler.weight\n",
      "decoder.block.8.layer.2.ff_adapter.adapters.IR.down_sampler.bias\n",
      "decoder.block.8.layer.2.ff_adapter.adapters.IR.up_sampler.weight\n",
      "decoder.block.8.layer.2.ff_adapter.adapters.IR.up_sampler.bias\n",
      "decoder.block.9.layer.0.attn_adapter.adapters.IR.down_sampler.weight\n",
      "decoder.block.9.layer.0.attn_adapter.adapters.IR.down_sampler.bias\n",
      "decoder.block.9.layer.0.attn_adapter.adapters.IR.up_sampler.weight\n",
      "decoder.block.9.layer.0.attn_adapter.adapters.IR.up_sampler.bias\n",
      "decoder.block.9.layer.2.ff_adapter.adapters.IR.down_sampler.weight\n",
      "decoder.block.9.layer.2.ff_adapter.adapters.IR.down_sampler.bias\n",
      "decoder.block.9.layer.2.ff_adapter.adapters.IR.up_sampler.weight\n",
      "decoder.block.9.layer.2.ff_adapter.adapters.IR.up_sampler.bias\n",
      "decoder.block.10.layer.0.attn_adapter.adapters.IR.down_sampler.weight\n",
      "decoder.block.10.layer.0.attn_adapter.adapters.IR.down_sampler.bias\n",
      "decoder.block.10.layer.0.attn_adapter.adapters.IR.up_sampler.weight\n",
      "decoder.block.10.layer.0.attn_adapter.adapters.IR.up_sampler.bias\n",
      "decoder.block.10.layer.2.ff_adapter.adapters.IR.down_sampler.weight\n",
      "decoder.block.10.layer.2.ff_adapter.adapters.IR.down_sampler.bias\n",
      "decoder.block.10.layer.2.ff_adapter.adapters.IR.up_sampler.weight\n",
      "decoder.block.10.layer.2.ff_adapter.adapters.IR.up_sampler.bias\n",
      "decoder.block.11.layer.0.attn_adapter.adapters.IR.down_sampler.weight\n",
      "decoder.block.11.layer.0.attn_adapter.adapters.IR.down_sampler.bias\n",
      "decoder.block.11.layer.0.attn_adapter.adapters.IR.up_sampler.weight\n",
      "decoder.block.11.layer.0.attn_adapter.adapters.IR.up_sampler.bias\n",
      "decoder.block.11.layer.2.ff_adapter.adapters.IR.down_sampler.weight\n",
      "decoder.block.11.layer.2.ff_adapter.adapters.IR.down_sampler.bias\n",
      "decoder.block.11.layer.2.ff_adapter.adapters.IR.up_sampler.weight\n",
      "decoder.block.11.layer.2.ff_adapter.adapters.IR.up_sampler.bias\n"
     ]
    }
   ],
   "source": [
    "for n, p in trainer_clipt5.model.image_passage_encoder.named_parameters():\n",
    "    if p.requires_grad==True:\n",
    "        print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk, disable_caching\n",
    "disable_caching()\n",
    "path_dataset = \"data_model/dataset/small_dataset\"\n",
    "dataset = load_from_disk(path_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 11])\n",
      "torch.Size([1, 36, 2048])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from cvlep.utils import device\n",
    "index = 1\n",
    "item = dataset[1]\n",
    "key_vision_features = \"vlt5_features\"\n",
    "key_boxes = \"vlt5_normalized_boxes\"\n",
    "key_text=\"input\"\n",
    "tokenizer = trainer_model.tokenizer_question\n",
    "\n",
    "vision_features = torch.Tensor(item[key_vision_features]).to(device)\n",
    "boxes = torch.Tensor(item[key_boxes]).to(device)\n",
    "vision_features = torch.squeeze(vision_features, dim=1)\n",
    "vis_inputs = (vision_features,boxes)\n",
    "boxes = torch.squeeze(boxes, dim=2)\n",
    "input_ids = tokenizer(\n",
    "    item[key_text], return_tensors='pt', padding=True, truncation=True).input_ids.to(device)\n",
    "return_pooled_output=True,\n",
    "pool_strategy=\"avg\"\n",
    "print(input_ids.size())\n",
    "print(vision_features.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = trainer_model.embedding_question( input_ids = input_ids,\n",
    "    vis_inputs = vis_inputs,\n",
    "    pool_strategy = pool_strategy,\n",
    "    return_pooled_output = return_pooled_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.pooler_output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(32200, 768)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_model.model.image_passage_encoder.encoder.embed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.0044,  0.0702, -0.1362,  ..., -0.0093, -0.0669,  0.2152],\n",
       "         [-0.3346, -0.0321, -0.2023,  ..., -0.0346, -0.2422,  0.1325],\n",
       "         [-0.3988, -0.1186, -0.2928,  ..., -0.1005, -0.0245,  0.0725],\n",
       "         ...,\n",
       "         [ 0.0027,  0.0135,  0.0049,  ...,  0.0231, -0.0006, -0.0090],\n",
       "         [ 0.0780, -0.0152,  0.0395,  ...,  0.0947,  0.0811,  0.1531],\n",
       "         [ 0.0100,  0.0192,  0.0097,  ...,  0.0264, -0.0009, -0.0107]]]), pooler_output=tensor([[ 5.6648e-02, -7.1184e-02, -1.5301e-01, -3.6746e-02, -1.1649e-01,\n",
       "          3.5650e-02,  4.6196e-02, -9.3280e-02, -1.2167e-01,  1.7649e-01,\n",
       "         -1.1129e-01, -5.8460e-02,  9.5520e-02,  1.1314e-02,  4.1585e-02,\n",
       "         -1.2416e-02, -1.0474e-01,  3.2099e-02,  6.2653e-02,  4.1071e-02,\n",
       "          8.7517e-02, -2.3704e-01,  7.3841e-02, -1.6983e-01, -1.1024e-01,\n",
       "          1.1043e-01, -8.2290e-02, -2.5255e-02, -3.4988e-01,  1.7297e-01,\n",
       "          1.3527e-01,  2.0638e-01, -9.9829e-02,  2.6702e-02, -5.7706e-02,\n",
       "         -3.6114e-02, -2.1935e-01, -1.2021e-01,  4.0559e-02,  1.3619e-01,\n",
       "          1.1547e-01,  2.1434e-01,  4.3598e-02,  8.4799e-02, -1.3910e-01,\n",
       "         -7.6862e-02,  4.0068e-02,  8.2855e-02, -1.2356e-01, -2.3356e-02,\n",
       "          8.9281e-02, -2.4328e-01,  9.7265e-02,  6.4604e-02, -6.1479e-02,\n",
       "         -7.6788e-02,  2.0053e-01, -3.4412e-02, -4.9680e-02,  7.8987e-02,\n",
       "         -5.9437e-02, -1.5208e-01, -4.1240e-02, -1.8452e-03,  9.6731e-02,\n",
       "          2.5129e-03, -2.7472e-01,  6.3285e-03, -1.7972e-02,  5.6221e-02,\n",
       "          7.1725e-04,  1.1230e-01, -1.5480e-01,  1.7283e-01,  1.7725e-02,\n",
       "         -8.6306e-03, -5.8082e-02, -5.2759e-02, -1.4953e-02, -3.4251e-02,\n",
       "         -1.8136e-02, -7.2912e-02,  8.0153e-02, -4.5119e-02,  1.3014e-01,\n",
       "          1.7871e-01,  2.4108e-01,  2.5335e-01, -9.1655e-02,  1.0435e-01,\n",
       "         -6.0266e-02, -1.0284e-02, -5.8338e-02, -1.3664e-01,  4.4927e-03,\n",
       "          2.5686e-01,  1.0977e-01,  6.0540e-03,  9.0139e-02, -3.6682e-02,\n",
       "          1.0168e-01, -9.5243e-02,  6.0577e-02, -2.1505e-02, -2.4120e-01,\n",
       "         -3.9939e-02, -2.8749e-02,  1.3322e-02,  6.9176e-02,  1.3201e-01,\n",
       "          7.8436e-02, -1.1388e-03, -7.8952e-02, -1.1538e-01, -1.4526e-01,\n",
       "         -2.6577e-02,  1.1842e-01, -9.1086e-02, -1.2194e-01,  8.6370e-02,\n",
       "         -6.8970e-02,  1.9308e-02,  2.5355e-02, -1.7590e-01, -2.4092e-01,\n",
       "         -4.3160e-03,  5.0034e-02, -1.3436e-01, -4.2643e-02, -1.6965e-01,\n",
       "         -1.9567e-01,  1.1454e-01, -1.0889e-02, -1.8757e-01,  9.9909e-02,\n",
       "         -1.1825e-01, -3.9800e-02,  8.9019e-02,  3.1374e-02, -2.2346e-01,\n",
       "          4.8511e-03, -1.9611e-03, -8.6634e-02,  2.3533e-02,  1.2248e-01,\n",
       "         -1.0433e-01, -6.2481e-02,  3.6984e-02, -1.5841e-01,  1.0640e-01,\n",
       "         -1.6670e-01,  7.9076e-02,  6.0324e-02, -7.1101e-02,  8.4890e-02,\n",
       "         -2.3785e-01,  9.3966e-02,  1.5102e-01, -3.0517e-01, -1.9984e-01,\n",
       "         -8.9604e-02,  3.3380e-02, -8.0320e-02,  1.7646e-01,  1.9851e-02,\n",
       "          1.0962e-01, -2.9159e-02, -1.4942e-01,  1.0430e-01,  1.3476e-01,\n",
       "         -2.5006e-02,  4.2738e-02,  9.7129e-02, -4.5894e-02,  1.9123e-01,\n",
       "         -1.9203e-01,  6.1338e-02, -1.3621e-01, -6.9330e-02,  1.9044e-02,\n",
       "          1.7211e-02, -1.8426e-01, -7.0352e-03, -3.2355e-02, -1.2859e-01,\n",
       "          1.2835e-01, -8.0131e-02,  2.7095e-02, -1.6080e-01, -5.7696e-02,\n",
       "         -7.1342e-02,  6.3420e-02, -1.0779e-01,  1.9965e-01, -9.6445e-02,\n",
       "         -6.2895e-02,  7.8046e-02,  1.7269e-01, -6.3947e-02,  2.3196e-03,\n",
       "          5.7397e-02, -2.3577e-02,  3.3009e-02,  9.0933e-02, -1.3351e-01,\n",
       "          4.1424e-02,  3.3542e-03, -4.5304e-02, -1.7649e-01,  1.3891e-01,\n",
       "          1.1829e-01, -1.3549e-01,  1.1509e-01,  5.2406e-02,  8.0264e-04,\n",
       "         -2.5466e-01,  1.9854e-01,  2.8487e-02, -1.5047e-01, -2.2250e-01,\n",
       "         -4.1130e-01, -1.3324e-01,  5.2766e-03, -3.3753e-02,  2.8282e-02,\n",
       "          9.0963e-02,  1.0388e-01,  3.5608e-01, -3.8365e-02, -7.9780e-03,\n",
       "          5.2078e-02,  2.4794e-01,  1.6939e-01,  4.0255e-02,  1.3860e-01,\n",
       "         -5.7090e-02, -4.2355e-02, -2.3558e-02,  4.7343e-02, -6.7799e-02,\n",
       "         -1.1853e-01, -1.3973e-01, -5.0139e-02,  1.0560e-01,  1.6904e-01,\n",
       "          5.3939e-02,  1.1163e-01, -4.3946e-02,  1.6269e-01, -2.6001e-01,\n",
       "          1.8189e-01, -1.9251e-01,  1.9725e-01,  5.3467e-02,  4.2570e-02,\n",
       "          2.4926e-02,  1.0339e-01, -4.6494e-02,  5.0573e-02, -1.0053e-01,\n",
       "         -3.5200e-02,  3.5145e-01,  3.4159e-03, -1.5362e-01, -9.6309e-03,\n",
       "          2.7890e-01, -6.3908e-02,  1.6790e-01, -7.0743e-02,  1.1093e-01,\n",
       "         -4.0014e-02, -8.8798e-02, -1.0509e-01, -2.9235e-02,  3.5896e-02,\n",
       "         -1.4439e-01,  2.7185e-02, -3.8404e-02, -1.4742e-02,  1.1536e-01,\n",
       "         -2.5055e-01,  7.6182e-02,  8.4615e-05,  1.0779e-01, -1.9729e-01,\n",
       "         -4.8202e-03, -1.1150e-01, -1.5042e-01, -6.8012e-03,  6.0199e-02,\n",
       "          1.0188e-01, -1.0791e-01,  1.1259e-01, -4.7317e-02,  7.9064e-02,\n",
       "         -8.2617e-02, -1.8037e-01, -5.9022e-02, -5.2889e-02,  4.6451e-02,\n",
       "          1.3843e-01, -1.2509e-01,  6.5980e-02, -1.9412e-02,  8.8937e-02,\n",
       "         -8.0666e-02,  4.3589e-02, -1.7870e-01, -7.4802e-03,  8.0770e-02,\n",
       "          1.0898e-01, -1.6556e-02,  9.7671e-02, -1.6894e-01,  1.0011e-01,\n",
       "          1.3348e-01, -3.3382e-02, -2.6455e-02,  4.9345e-02, -1.8567e-03,\n",
       "         -2.6573e-01,  9.9391e-02,  2.0684e-01, -1.5813e-02,  6.2283e-02,\n",
       "         -1.0348e-01,  2.5368e-01, -8.7927e-02, -3.6937e-02, -2.9318e-02,\n",
       "          9.8028e-02, -1.4213e-02, -9.6586e-02,  5.4695e-02, -1.5071e-01,\n",
       "         -4.2884e-02,  2.1310e-02,  5.0642e-02,  3.1646e-02, -1.3338e-01,\n",
       "          1.0224e-01, -2.3936e-01, -6.6592e-02, -2.0546e-01, -1.2527e-01,\n",
       "          1.1854e-01, -1.9952e-02,  9.4907e-02, -1.0361e-01, -7.1104e-02,\n",
       "         -1.6286e-01, -6.7716e-02,  1.6073e-01,  5.2321e-02,  1.1655e-01,\n",
       "          1.4920e-01, -1.7601e-01, -4.5349e-02, -1.0495e-01,  1.7389e-02,\n",
       "         -2.6232e-02,  2.5699e-01,  2.4474e-02,  1.0477e-01, -7.6520e-02,\n",
       "          9.9629e-02, -8.4367e-02, -9.9720e-03,  1.4627e-01,  1.5038e-01,\n",
       "         -8.4939e-02, -3.8132e-02, -5.5359e-02, -1.5582e-01, -1.0076e-01,\n",
       "          1.2360e-02, -6.2420e-02,  1.6432e-01,  1.5075e-01, -5.8578e-02,\n",
       "         -4.8055e-02,  1.3318e-02,  1.2016e-01, -3.3128e-02,  1.2273e-01,\n",
       "          2.2809e-02, -3.5356e-02,  1.0881e-01,  2.1894e-01, -8.9597e-02,\n",
       "         -5.1360e-02, -4.0064e-02, -3.1258e-02, -2.8262e-02,  1.0273e-01,\n",
       "         -3.8696e-02,  9.5733e-02, -7.1016e-02, -2.5011e-03,  9.0620e-02,\n",
       "          8.7721e-02,  5.9105e-02,  1.4423e-01,  8.5796e-02, -4.0406e-02,\n",
       "         -1.1256e-01,  2.5545e-02,  1.5949e-01,  7.2592e-02,  9.6070e-02,\n",
       "          3.1162e-03, -5.0051e-02, -5.5794e-03, -5.9596e-03, -9.1406e-02,\n",
       "          1.7434e-01,  7.1470e-02,  1.0839e-01, -2.6532e-03,  1.6383e-02,\n",
       "         -4.1331e-02,  1.4214e-02,  1.1198e-02,  1.6541e-02,  8.2796e-02,\n",
       "         -4.0043e-02, -1.0858e-01,  4.2423e-02,  1.2854e-01,  7.3324e-03,\n",
       "         -1.8378e-01, -9.4893e-03, -1.1000e-01,  1.4518e-01,  1.2635e-01,\n",
       "          1.1003e-02, -8.8769e-02, -4.3025e-02,  2.7864e-02,  1.2064e-01,\n",
       "          1.0577e-01, -1.5479e-01,  1.0298e-01,  1.8620e-02,  2.3587e-01,\n",
       "          4.6578e-03, -7.9362e-02, -2.6344e-02, -1.1929e-01, -1.4926e-02,\n",
       "          1.8412e-01, -3.6151e-02, -2.1832e-01, -1.3871e-01, -3.2910e-02,\n",
       "          8.7190e-02, -3.3584e-02, -5.9691e-02,  8.6242e-04, -2.4600e-01,\n",
       "          4.5402e-02,  1.5846e-01,  1.1855e-01,  4.0482e-02, -5.7792e-02,\n",
       "         -1.0772e-01,  1.7057e-01,  1.4509e-01, -1.4244e-02,  8.8743e-02,\n",
       "         -1.6682e-02,  2.1597e-01, -7.3645e-02,  8.4626e-02, -2.0321e-01,\n",
       "          5.3152e-02, -4.1354e-02, -1.1845e-01, -6.6002e-02, -2.3721e-01,\n",
       "          2.0754e-01, -2.4641e-01,  1.1394e-01,  7.3031e-02, -2.1068e-01,\n",
       "         -9.9003e-02, -1.0719e-01, -3.6220e-02,  3.8718e-02, -4.2863e-02,\n",
       "         -4.2863e-02, -4.4179e-02,  3.2717e-01,  1.8560e-01, -3.3248e-02,\n",
       "         -7.4413e-02,  9.2814e-02,  2.3129e-02, -5.6121e-02,  5.1840e-02,\n",
       "          2.8309e-02, -1.3142e-01,  2.4845e-02,  7.2732e-02,  3.9975e-02,\n",
       "          1.7479e-01, -1.3004e-01,  2.3654e-02,  5.0932e-02, -1.2324e-01,\n",
       "          7.7021e-02,  4.7436e-02, -2.6862e-01,  4.3295e-02,  6.1174e-02,\n",
       "          1.2143e-01,  4.1469e-01, -1.9343e-01,  1.1031e-01,  2.1305e-02,\n",
       "          1.2391e-02, -1.6763e-01, -1.6243e-01,  1.0325e-02, -1.5955e-01,\n",
       "         -1.9574e-01, -1.2972e-01,  2.8366e-04,  2.0014e-01, -1.5067e-01,\n",
       "         -1.3287e-01, -9.0149e-02, -2.6842e-01,  7.0291e-02,  2.4570e-02,\n",
       "         -3.4946e-02, -1.5780e-01, -2.4727e-01,  2.2608e-01, -6.9311e-02,\n",
       "          1.6046e-01, -1.3901e-01, -3.2820e-02,  2.8448e-01,  1.9665e-01,\n",
       "          1.8996e-01, -5.9038e-02, -1.6209e-01, -7.8300e-02,  1.0704e-01,\n",
       "          1.1561e-01,  3.5261e-02,  1.1100e-01,  6.1467e-02,  6.9499e-02,\n",
       "          1.4634e-01, -1.5710e-01,  3.1434e-02, -8.9579e-02, -1.1669e-01,\n",
       "         -5.8055e-02, -4.0590e-02,  9.0771e-02, -8.0851e-02, -1.1806e-01,\n",
       "         -2.5185e-02,  1.4855e-02, -5.9336e-02, -6.5623e-02,  9.7041e-02,\n",
       "          2.7690e-01, -1.9987e-02, -2.8658e-02, -1.4633e-01, -8.7065e-03,\n",
       "         -3.3145e-02,  1.4271e-02, -1.1132e-01,  5.0628e-02, -7.1439e-02,\n",
       "          1.4279e-01,  2.0576e-01,  1.3376e-01, -1.3618e-01,  1.0161e-01,\n",
       "         -6.6048e-02,  1.7167e-01,  6.4942e-02, -2.0768e-01,  9.5287e-02,\n",
       "          6.1489e-03, -4.3719e-02,  1.4714e-01,  1.7106e-02,  2.5687e-01,\n",
       "         -6.8975e-02,  6.0700e-02,  1.1607e-01, -8.8286e-02, -2.5594e-02,\n",
       "          1.4360e-01, -9.5792e-02, -6.7379e-02,  1.5858e-01, -1.8343e-01,\n",
       "         -4.1773e-03,  1.1835e-01, -1.2830e-01, -4.7252e-03, -6.9979e-02,\n",
       "         -4.3474e-03, -1.5716e-01, -7.8922e-03,  5.4170e-02, -8.5557e-02,\n",
       "         -1.3081e-01, -1.4872e-01, -1.4497e-02,  2.2065e-01, -1.3079e-01,\n",
       "         -5.6204e-02,  1.4542e-01,  7.2989e-02,  2.0288e-01,  8.7270e-02,\n",
       "         -2.5098e-02, -1.4824e-01, -1.6793e-01, -1.1220e-02, -2.1143e-01,\n",
       "         -8.6873e-02,  1.9970e-01,  1.1379e-01, -2.0107e-02,  1.3206e-01,\n",
       "          2.0396e-01,  1.4635e-01,  5.7540e-02,  2.1541e-01,  1.1418e-01,\n",
       "         -5.5772e-02,  2.7615e-01, -1.5337e-01, -5.8670e-03,  2.5867e-01,\n",
       "         -6.3398e-02,  3.8931e-01,  2.4190e-01,  1.5852e-01, -1.0393e-02,\n",
       "          2.4114e-02,  1.8368e-01,  1.6601e-01, -5.4443e-02, -1.8606e-01,\n",
       "         -2.8720e-01, -5.0861e-02, -2.4094e-02, -9.5253e-04,  1.2215e-02,\n",
       "          1.4893e-03, -9.4679e-02, -1.0176e-01,  7.2016e-02, -1.4162e-01,\n",
       "          8.9151e-04, -5.5513e-02,  2.1170e-01, -4.1009e-03, -5.7914e-02,\n",
       "         -8.8402e-02,  5.2346e-03, -8.2523e-02,  2.2422e-02, -7.5668e-02,\n",
       "         -1.3940e-02,  7.4526e-02,  5.4616e-02, -8.9429e-02,  8.5130e-02,\n",
       "         -1.8144e-02, -1.8300e-03,  1.3815e-01, -2.0179e-01, -1.2929e-01,\n",
       "         -2.5350e-01, -3.8829e-02, -6.4619e-02, -9.1117e-02, -1.3473e-01,\n",
       "          1.5278e-01, -2.3307e-01,  5.3524e-02,  9.3714e-02, -1.2227e-02,\n",
       "          7.9154e-02,  1.2854e-01, -8.1355e-02, -2.4539e-01,  1.7300e-02,\n",
       "          1.5464e-01, -3.2712e-01,  1.0083e-01, -3.0230e-02,  7.0219e-02,\n",
       "         -1.2951e-01, -2.4582e-03,  1.6231e-01, -1.1312e-01, -5.8386e-02,\n",
       "          1.9637e-01,  3.1791e-02, -3.1436e-02, -2.4544e-02,  2.0815e-02,\n",
       "         -7.6786e-02, -1.9849e-02,  6.4449e-03, -5.7112e-02, -4.5437e-02,\n",
       "         -1.0695e-01,  1.0656e-01,  4.3160e-02, -2.0950e-01,  2.1037e-01,\n",
       "         -1.7241e-02,  5.1989e-02,  1.6865e-02,  7.8504e-02,  1.9873e-03,\n",
       "          1.2072e-01,  1.2359e-01, -1.5053e-01,  1.2013e-01, -6.5865e-03,\n",
       "          8.9148e-02,  2.5787e-01, -8.8401e-02,  1.0379e-01,  2.1067e-01,\n",
       "         -1.3900e-02, -8.2743e-02,  1.4271e-02,  1.0033e-01, -1.3111e-01,\n",
       "          1.2430e-01, -4.0163e-02, -1.1760e-01,  1.0495e-01, -2.5412e-01,\n",
       "          3.0550e-02,  2.1363e-01, -6.0252e-02, -9.7014e-02, -2.9208e-02,\n",
       "          1.3326e-01,  5.4465e-02,  6.5086e-02,  1.4199e-01, -3.0988e-02,\n",
       "          1.9695e-01,  2.3529e-02, -1.9665e-01,  1.0276e-01, -1.8946e-01,\n",
       "         -1.2714e-01,  1.4359e-01, -3.7140e-02]]), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_model.encoder_question.encoder.forward(\n",
    "    input_ids = input_ids,\n",
    "    vis_inputs = vis_inputs,\n",
    "    pool_strategy = pool_strategy,\n",
    "    return_pooled_output = return_pooled_output\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk, disable_caching\n",
    "disable_caching()\n",
    "path_dataset = \"data_model/dataset/small_dataset\"\n",
    "dataset = load_from_disk(path_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from cvlep.utils import device\n",
    "index = 2\n",
    "item = dataset[1]\n",
    "key_vision_features = \"vlt5_features\"\n",
    "key_boxes = \"vlt5_normalized_boxes\"\n",
    "key_text=\"input\"\n",
    "tokenizer = trainer_clipt5.tokenizer_question\n",
    "\n",
    "vision_features = torch.Tensor(item[key_vision_features]).to(device)\n",
    "boxes = torch.Tensor(item[key_boxes]).to(device)\n",
    "vision_features = torch.squeeze(vision_features, dim=1)\n",
    "boxes = torch.squeeze(boxes, dim=1)\n",
    "input_ids = tokenizer(\n",
    "    item[key_text], return_tensors='pt', padding=True, truncation=True).input_ids.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_encoder_vlt5 = trainer_vlt5.model.image_question_encoder.encoder.forward(\n",
    "    input_ids = input_ids,\n",
    "    vis_inputs=(vision_features,boxes)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_encoder_clipT5 = trainer_clipt5.model.image_question_encoder.encoder.forward(\n",
    "    input_ids=input_ids,\n",
    "    vis_inputs=(vision_features,boxes)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0044,  0.0702, -0.1362,  ..., -0.0093, -0.0669,  0.2152],\n",
       "         [-0.3346, -0.0321, -0.2023,  ..., -0.0346, -0.2422,  0.1325],\n",
       "         [-0.3988, -0.1186, -0.2928,  ..., -0.1005, -0.0245,  0.0725],\n",
       "         ...,\n",
       "         [ 0.0027,  0.0135,  0.0049,  ...,  0.0231, -0.0006, -0.0090],\n",
       "         [ 0.0780, -0.0152,  0.0395,  ...,  0.0947,  0.0811,  0.1531],\n",
       "         [ 0.0100,  0.0192,  0.0097,  ...,  0.0264, -0.0009, -0.0107]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_encoder_vlt5[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = output_encoder_clipT5[0] == output_encoder_vlt5[0]\n",
    "torch.equal(output_encoder_clipT5[0],output_encoder_vlt5[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pgrimal/miniconda3/envs/cvlp/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of VLT5 were not initialized from the model checkpoint at data_model/t5_pretrained and are newly initialized: ['encoder.visual_embedding.feat_embedding.0.weight', 'encoder.visual_embedding.feat_embedding.0.bias', 'encoder.visual_embedding.feat_embedding.1.weight', 'encoder.visual_embedding.absolute_vis_pos_embedding.0.weight', 'encoder.visual_embedding.absolute_vis_pos_embedding.0.bias', 'encoder.visual_embedding.absolute_vis_pos_embedding.1.weight', 'encoder.visual_embedding.obj_order_embedding.weight', 'encoder.visual_embedding.img_order_embedding.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of VLT5 were not initialized from the model checkpoint at data_model/t5_pretrained and are newly initialized: ['encoder.visual_embedding.feat_embedding.0.weight', 'encoder.visual_embedding.feat_embedding.0.bias', 'encoder.visual_embedding.feat_embedding.1.weight', 'encoder.visual_embedding.absolute_vis_pos_embedding.0.weight', 'encoder.visual_embedding.absolute_vis_pos_embedding.0.bias', 'encoder.visual_embedding.absolute_vis_pos_embedding.1.weight', 'encoder.visual_embedding.obj_order_embedding.weight', 'encoder.visual_embedding.img_order_embedding.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from  data_model/whole_model/vlt5/VLT5Epoch30.pth\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['encoder.visual_embedding.layer_norm.weight'])\n",
      "Model loaded from  data_model/whole_model/vlt5/VLT5Epoch30.pth\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['encoder.visual_embedding.layer_norm.weight'])\n",
      "Configurations\n",
      "{'share_embedding': True, 'share_vis_embedding': True}\n"
     ]
    }
   ],
   "source": [
    "from cvlep.trainer_base_vladapter import Trainer\n",
    "\n",
    "config_encoder_question = \"experiments/config_vladapter/model_only/args_vlt5_wo_adap.json\"\n",
    "config_encoder_passage = \"experiments/config_vladapter/model_only/args_vlt5_wo_adap.json\"\n",
    "config_model = \"experiments/config_vladapter/model_only/config_model.json\"\n",
    "config_training = \"experiments/config_vladapter/model_only/training_vlt5.json\"\n",
    "\n",
    "trainer_model = Trainer(config_encoder_question, config_encoder_passage,config_model, config_training)\n",
    "\n",
    "from datasets import load_from_disk, disable_caching\n",
    "disable_caching()\n",
    "path_dataset = \"data_model/dataset/small_dataset\"\n",
    "dataset = load_from_disk(path_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from cvlep.utils import device\n",
    "\n",
    "\n",
    "def get_feat(item):\n",
    "    key_vision_features = \"vlt5_features\"\n",
    "    key_boxes = \"vlt5_normalized_boxes\"\n",
    "    key_text=\"input\"\n",
    "    vision_features = torch.Tensor(item[key_vision_features])\n",
    "    boxes = torch.Tensor(item[key_boxes])\n",
    "    vision_features = torch.squeeze(vision_features, dim=0)\n",
    "    boxes = torch.squeeze(boxes, dim=0)\n",
    "    return {\"text\":item[key_text], \"feats\":vision_features, \"boxes\":boxes, \"size\":boxes.size()[0]}\n",
    "\n",
    "B = 2\n",
    "question1 = get_feat(dataset[0])\n",
    "relevant1 = get_feat(dataset[0])   \n",
    "irrelevant1 = get_feat(dataset[1])\n",
    "question2 = get_feat(dataset[2])\n",
    "relevant2 = get_feat(dataset[2])\n",
    "irrelevant2 = get_feat(dataset[3])\n",
    "\n",
    "item1 = {}\n",
    "item1[\"question_text\"] = question1[\"text\"]\n",
    "item1[\"passage_relevant_text\"] = relevant1['text']\n",
    "item1['passage_irrelevant_text'] = irrelevant1[\"text\"]\n",
    "item1[\"n_boxes_question\"] = question1[\"size\"]\n",
    "item1[\"n_boxes_passage_relevant\"] = relevant1[\"size\"]\n",
    "item1[\"n_boxes_passage_irrelevant\"] = irrelevant1[\"size\"]\n",
    "item1[\"question_image_features\"] = question1['feats']\n",
    "item1[\"question_image_boxes\"] = question1[\"boxes\"]\n",
    "item1[\"passage_relevant_image_features\"]=relevant1[\"feats\"]\n",
    "item1[\"passage_relevant_image_boxes\"]=relevant1[\"boxes\"]\n",
    "item1[\"passage_irrelevant_image_features\"]=irrelevant1['feats']\n",
    "item1[\"passage_irrelevant_image_boxes\"]=irrelevant1[\"boxes\"]\n",
    "\n",
    "item2 = {}\n",
    "item2[\"question_text\"] = question2[\"text\"]\n",
    "item2[\"passage_relevant_text\"] = relevant2['text']\n",
    "item2['passage_irrelevant_text'] = irrelevant2[\"text\"]\n",
    "item2[\"n_boxes_question\"] = question2[\"size\"]\n",
    "item2[\"n_boxes_passage_relevant\"] = relevant2[\"size\"]\n",
    "item2[\"n_boxes_passage_irrelevant\"] = irrelevant2[\"size\"]\n",
    "item2[\"question_image_features\"] = question2['feats']\n",
    "item2[\"question_image_boxes\"] = question2[\"boxes\"]\n",
    "item2[\"passage_relevant_image_features\"]=relevant2[\"feats\"]\n",
    "item2[\"passage_relevant_image_boxes\"]=relevant2[\"boxes\"]\n",
    "item2[\"passage_irrelevant_image_features\"]=irrelevant2['feats']\n",
    "item2[\"passage_irrelevant_image_boxes\"]=irrelevant2[\"boxes\"]\n",
    "\n",
    "batch = [item1, item2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 2\n",
    "\n",
    "tokenizer = trainer_model.tokenizer_question\n",
    "V_L_question = max(item['n_boxes_question'] for item in batch)\n",
    "V_L_context = max(max(item['n_boxes_passage_relevant'],\n",
    "                    item['n_boxes_passage_irrelevant']) for item in batch)\n",
    "feat_dim = batch[0]['question_image_features'].shape[-1]\n",
    "# boxes are represented by 4 points\n",
    "question_boxes = torch.zeros(B, V_L_question, 4, dtype=torch.float)\n",
    "question_vis_feats = torch.zeros(\n",
    "    B, V_L_question, feat_dim, dtype=torch.float)\n",
    "relevant_boxes = torch.zeros(B, V_L_context, 4, dtype=torch.float)\n",
    "relevant_vis_feats = torch.zeros(\n",
    "    B, V_L_context, feat_dim, dtype=torch.float)\n",
    "irrelevant_boxes = torch.zeros(\n",
    "    B, V_L_context, 4, dtype=torch.float)\n",
    "irrelevant_vis_feats = torch.zeros(\n",
    "    B, V_L_context, feat_dim, dtype=torch.float)\n",
    "\n",
    "relevant_text, irrelevant_text, question_text, labels = list(), list(), list(), list()\n",
    "for i, item in enumerate(batch):\n",
    "    # TODO: voir si besoin de changer gestion pour le text, car a un impact sur attention mask\n",
    "    question_text.append(item['question_text'])\n",
    "    relevant_text.append(item['passage_relevant_text'])\n",
    "    irrelevant_text.append(item['passage_irrelevant_text'])\n",
    "    if tokenizer:\n",
    "        n_boxes_relevant = item['n_boxes_passage_relevant']\n",
    "        n_boxes_irrelevant = item['n_boxes_passage_irrelevant']\n",
    "        n_boxes_question = item['n_boxes_question']\n",
    "        question_boxes[i,\n",
    "                        :n_boxes_question] = item['question_image_boxes']\n",
    "        question_vis_feats[i,\n",
    "                            :n_boxes_question] = item['question_image_features']\n",
    "        relevant_boxes[i,\n",
    "                        :n_boxes_relevant] = item['passage_relevant_image_boxes']\n",
    "        relevant_vis_feats[i,\n",
    "                            :n_boxes_relevant] = item['passage_relevant_image_features']\n",
    "        irrelevant_boxes[i,\n",
    "                            :n_boxes_irrelevant] = item['passage_irrelevant_image_boxes']\n",
    "        irrelevant_vis_feats[i,\n",
    "                                :n_boxes_irrelevant] = item['passage_irrelevant_image_features']\n",
    "    if item['passage_relevant_text'] is None:\n",
    "        labels.append(-100)  # ignore index when computing the loss\n",
    "    else:\n",
    "        labels.append(i)\n",
    "\n",
    "question_input = tokenizer(\n",
    "    question_text, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "context_input = tokenizer(\n",
    "    relevant_text + irrelevant_text, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "labels = torch.tensor(labels)\n",
    "visual_feats_context = torch.concat(\n",
    "    [relevant_vis_feats, irrelevant_vis_feats])\n",
    "context_image_boxes = torch.concat(\n",
    "    [relevant_boxes, irrelevant_boxes])\n",
    "results = {\n",
    "    \"input_ids_question\": question_input.input_ids,\n",
    "    \"attention_mask_question\": question_input.attention_mask,\n",
    "    \"input_ids_context\": context_input.input_ids,\n",
    "    \"attention_mask_context\": context_input.attention_mask,\n",
    "    \"labels\": labels,\n",
    "    \"visual_feats_question\": question_vis_feats,\n",
    "    \"visual_feats_context\": visual_feats_context,\n",
    "    \"question_image_boxes\": question_boxes,\n",
    "    \"context_image_boxes\": context_image_boxes\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1641)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loss doit tre faible car on met exactement mme embedding pour question \n",
    "# et relevant passage\n",
    "trainer_model.model.train_step(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test gather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0, 1, 2, 3]), tensor([   0,    1, -100,    3])]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "loss_fct = nn.NLLLoss(reduction='mean')\n",
    "batch1 ={\n",
    "\"labels\": torch.tensor([0,1,2,3])\n",
    "}\n",
    "batch2 = {\n",
    "\"labels\": torch.tensor([0,1,-100,3])\n",
    "}\n",
    "labels1 = batch1.pop('labels')\n",
    "labels2 = batch2.pop('labels')\n",
    "labels_gatherer = [labels1, labels2]\n",
    "gatherers = zip(labels_gatherer)\n",
    "labels_gatherer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([0, 1, 2, 3]), tensor([   8,    9, -100,   11])]\n"
     ]
    }
   ],
   "source": [
    "# N questions taille du batch\n",
    "N = 4\n",
    "# nombre relevant et irrelevant 1 et 1\n",
    "global_labels = []\n",
    "label_shift = 0\n",
    "for i, (received_labels) in enumerate(gatherers):\n",
    "    received_labels = received_labels[0]\n",
    "    received_labels[received_labels!=-100] += label_shift\n",
    "    label_shift += 4 * 2 # N * M\n",
    "    global_labels.append(received_labels)\n",
    "print(global_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "exemple d'un batch de 1\n",
    "```\n",
    "| question1 | * [relevant1, irr1, relevant2, irr2]\n",
    "| question2 |\n",
    "```\n",
    "label devra donc etre [0, 2]\n",
    "ce qui explique le besoin du shift\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "m = nn.LogSoftmax(dim=1)\n",
    "loss = nn.NLLLoss()\n",
    "# input is of size N x C = 3 x 5\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "# each element in target has to have 0 <= value < C\n",
    "target = torch.tensor([1, 0, 4])\n",
    "output = loss(m(input), target)\n",
    "output.backward()\n",
    "# 2D loss example (used, for example, with image inputs)\n",
    "N, C = 5, 4\n",
    "loss = nn.NLLLoss()\n",
    "# input is of size N x C x height x width\n",
    "data = torch.randn(N, 16, 10, 10)\n",
    "conv = nn.Conv2d(16, C, (3, 3))\n",
    "m = nn.LogSoftmax(dim=1)\n",
    "# each element in target has to have 0 <= value < C\n",
    "target = torch.empty(N, 8, 8, dtype=torch.long).random_(0, C)\n",
    "output = loss(m(conv(data)), target)\n",
    "output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5051658153533936"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " epoch 0 | Loss 2.5: 100%|| 2/2 [00:02<00:00,  1.00s/it]\n",
      " epoch 1 | Loss 0.625: 100%|| 2/2 [00:02<00:00,  1.00s/it]\n",
      " epoch 2 | Loss 0.15625: 100%|| 2/2 [00:02<00:00,  1.00s/it]\n",
      " epoch 3 | Loss 0.0390625: 100%|| 2/2 [00:02<00:00,  1.00s/it]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "loss = 10\n",
    "for epoch in range(4):\n",
    "    pbar = tqdm(total=2, ncols=120)\n",
    "    for batch in range(2):\n",
    "        loss *= 0.5\n",
    "        time.sleep(1)\n",
    "        desc_str = f' epoch {epoch} | Loss {loss}'\n",
    "        pbar.set_description(desc_str)\n",
    "        pbar.update(1)\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvlep.VLT5.param import Config\n",
    "config = Config.load_json(\"experiments/config_vladapter/prompt/training_prompt.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.rank = 'yes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.local = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/sentence-transformers/sentence-t5-base\n",
    "\n",
    "https://github.com/UKPLab/sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pgrimal/miniconda3/envs/cvlp/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/sentence-t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = model.__getitem__(3)\n",
    "dense = model.__getitem__(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T_destination',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_backward_hooks',\n",
       " '_buffers',\n",
       " '_call_impl',\n",
       " '_forward_hooks',\n",
       " '_forward_pre_hooks',\n",
       " '_get_backward_hooks',\n",
       " '_get_name',\n",
       " '_is_full_backward_hook',\n",
       " '_load_from_state_dict',\n",
       " '_load_state_dict_pre_hooks',\n",
       " '_maybe_warn_non_full_backward_hook',\n",
       " '_modules',\n",
       " '_named_members',\n",
       " '_non_persistent_buffers_set',\n",
       " '_parameters',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_replicate_for_data_parallel',\n",
       " '_save_to_state_dict',\n",
       " '_slow_forward',\n",
       " '_state_dict_hooks',\n",
       " '_version',\n",
       " 'add_module',\n",
       " 'apply',\n",
       " 'bfloat16',\n",
       " 'buffers',\n",
       " 'children',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'double',\n",
       " 'dump_patches',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'float',\n",
       " 'forward',\n",
       " 'get_buffer',\n",
       " 'get_extra_state',\n",
       " 'get_parameter',\n",
       " 'get_submodule',\n",
       " 'half',\n",
       " 'load_state_dict',\n",
       " 'modules',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'parameters',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_full_backward_hook',\n",
       " 'register_module',\n",
       " 'register_parameter',\n",
       " 'requires_grad_',\n",
       " 'set_extra_state',\n",
       " 'share_memory',\n",
       " 'state_dict',\n",
       " 'to',\n",
       " 'to_empty',\n",
       " 'train',\n",
       " 'training',\n",
       " 'type',\n",
       " 'xpu',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense.activation_function.__dir__()\n",
    "# https://github.com/UKPLab/sentence-transformers/blob/46a149433fe9af0851f7fa6f9bf37b5ffa2c891c/sentence_transformers/models/Dense.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Normalize()"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TriviaQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration PaulLerner--triviaqa_for_viquae-ab2ffed0dcbf3b5f\n",
      "Reusing dataset parquet (/home/pgrimal/.cache/huggingface/datasets/PaulLerner___parquet/PaulLerner--triviaqa_for_viquae-ab2ffed0dcbf3b5f/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n",
      "100%|| 4/4 [00:00<00:00, 172.60it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"PaulLerner/triviaqa_for_viquae\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    with_viquae_test: Dataset({\n",
       "        features: ['id', 'input', 'meta', 'output'],\n",
       "        num_rows: 1247\n",
       "    })\n",
       "    with_viquae_train: Dataset({\n",
       "        features: ['id', 'input', 'meta', 'output'],\n",
       "        num_rows: 1181\n",
       "    })\n",
       "    with_viquae_validation: Dataset({\n",
       "        features: ['id', 'input', 'meta', 'output'],\n",
       "        num_rows: 1234\n",
       "    })\n",
       "    without_viquae: Dataset({\n",
       "        features: ['id', 'input', 'meta', 'output'],\n",
       "        num_rows: 47000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "dataset_kilt = load_from_disk(\"/home/pgrimal/Documents/data_viquae/kilt_trivia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['kilt_id', 'wikipedia_id', 'wikipedia_title', 'text', 'anchors', 'categories', 'wikidata_info', 'history'],\n",
       "    num_rows: 5903530\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_kilt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['summarization', 'translation_en_to_de', 'translation_en_to_fr', 'translation_en_to_ro'])\n"
     ]
    }
   ],
   "source": [
    "print(model.config.task_specific_params.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "British spy.\n"
     ]
    }
   ],
   "source": [
    "# inference\n",
    "input_ids = tokenizer(\n",
    "    \"James Bond is a <extra_id_0> <extra_id_1>\", return_tensors=\"pt\"\n",
    ").input_ids  # Batch size 1\n",
    "outputs = model.generate(input_ids)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ship built in 1912.\n"
     ]
    }
   ],
   "source": [
    "# inference\n",
    "input_ids = tokenizer(\n",
    "    \"Titanic is a <extra_id_0> <extra_id_1>\", return_tensors=\"pt\"\n",
    ").input_ids  # Batch size 1\n",
    "outputs = model.generate(input_ids)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLIP preprocessing\n",
    "\n",
    "But : comprendre comment sont gnrs les features actuellement avec clip\n",
    "Si bonne facon d'utiliser clip on laisse comme a sinon on essaie de fournir juste un embedding d'image et pas plusieurs embedding en fonction des boxes\n",
    "\n",
    "Fichier utilis cvlep.CLIPT5.clip_prepro_feats\n",
    "\n",
    "charger le modle :\n",
    "\n",
    "Dans VLT5 il y a cette fonction\n",
    "```py\n",
    "def vis_forward(self, batch, device):\n",
    "    if hasattr(self, \"vis_encoder\"):\n",
    "        # self.vis_encoder.eval() # freeze the batchnorm statistics\n",
    "        images = batch[\"images\"].to(device)\n",
    "\n",
    "        if self.config.vis_pooling_output:\n",
    "            _, vis_feats = self.vis_encoder(images)\n",
    "        else:\n",
    "            vis_feats, _ = self.vis_encoder(images)\n",
    "        # vis_feats: (B, dim, L ** 0.5, L ** 0.5)\n",
    "        B, L, D = vis_feats.shape\n",
    "        vis_pos = torch.zeros(B, L, 4, dtype=vis_feats.dtype)\n",
    "\n",
    "        batch[\"vis_feats\"] = vis_feats\n",
    "        batch[\"boxes\"] = vis_pos\n",
    "```\n",
    "\n",
    "Voir ce que renvoie le modle avec la fonction\n",
    "\n",
    "```py\n",
    "self.vis_encoder = get_vis_encoder(\n",
    "        backbone=vis_encoder_type, \n",
    "        image_size=eval(self.args.image_size)[0],\n",
    "        adapter_type=None,\n",
    "    )\n",
    "self.model.vis_encoder = self.vis_encoder\n",
    "```\n",
    "\n",
    "get_vis encoder dans vis_encoder.py\n",
    "\n",
    "Use CLIP-ResNet50 for fair comparaison\n",
    "\n",
    "format en entre de vis_forward\n",
    "regarder dans vlt5 cococaption T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvlep.CLIPT5.vis_encoder import get_vis_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pgrimal/miniconda3/envs/cvlp2/lib/python3.7/site-packages/torchvision/transforms/transforms.py:333: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
     ]
    }
   ],
   "source": [
    "model = get_vis_encoder(backbone='RN101', adapter_type=None, image_size=eval(\"(224,224)\")[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pgrimal/miniconda3/envs/cvlp2/lib/python3.7/site-packages/ipykernel_launcher.py:10: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/home/pgrimal/miniconda3/envs/cvlp2/lib/python3.7/site-packages/ipykernel_launcher.py:21: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n"
     ]
    }
   ],
   "source": [
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision.transforms import (\n",
    "    Compose, Resize, CenterCrop, ToTensor, Normalize, RandomCrop, RandomHorizontalFlip, RandomErasing\n",
    ")\n",
    "def _transform(n_px):\n",
    "    return Compose([\n",
    "        # PadToSquare(),\n",
    "        Resize(n_px, interpolation=Image.BICUBIC),\n",
    "        CenterCrop(n_px),\n",
    "        # MinMaxResize(*n_px),\n",
    "        lambda image: image.convert(\"RGB\"),\n",
    "        ToTensor(),\n",
    "        Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "    ])\n",
    "\n",
    "\n",
    "def augmentation_transform(image_size):\n",
    "    return Compose([\n",
    "        Resize(image_size, interpolation=Image.BICUBIC),\n",
    "        RandomHorizontalFlip(),\n",
    "        RandomCrop(image_size, padding=int(image_size[0]*0.0625), padding_mode='reflect'),\n",
    "        lambda image: image.convert(\"RGB\"),\n",
    "        ToTensor(),\n",
    "        Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "        RandomErasing(),\n",
    "    ])\n",
    "\n",
    "\n",
    "transform = _transform(eval(\"(224,224)\")[0])\n",
    "transform2 = augmentation_transform(eval(\"(224,224)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"/home/pgrimal/Documents/Projects/visual_language_representation/results/best_zs_vlbart/images/folder_50/512px-Joseph_Stalin-TIME-1930.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = transform(image)\n",
    "t2 = transform2(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "print(t.shape)\n",
    "print(t2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_boxes = 36 #self.args.n_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fake batch\n",
    "t = torch.unsqueeze(t, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pooling torch.Size([1, 1, 512])\n",
      "output classic torch.Size([1, 49, 2048])\n"
     ]
    }
   ],
   "source": [
    "_, pooling_output = output\n",
    "output_normal, _ = output\n",
    "\n",
    "print(\"pooling\", pooling_output.shape)\n",
    "print(\"output classic\", output_normal.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2048])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_normal[:,0,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 0. Expected size 25 but got size 24 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3515/1658407024.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_normal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (2B, L/2, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (B, L, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 0. Expected size 25 but got size 24 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "def downsample(inputs):\n",
    "        pool = torch.nn.AdaptiveMaxPool2d(6) # racine de 36\n",
    "        B, L, dim = inputs.shape\n",
    "\n",
    "        inputs = inputs.permute(0, 2, 1) # (2B, dim, L/2)\n",
    "        print(inputs.shape)\n",
    "        sqrt_L = int(L ** 0.5)\n",
    "        inputs = inputs.reshape(B, dim, sqrt_L, sqrt_L)\n",
    "        print(inputs.shape)\n",
    "        inputs = pool(inputs)\n",
    "        print(inputs.shape)\n",
    "        inputs = inputs.reshape(B, dim, -1)\n",
    "        print(inputs.shape)\n",
    "        inputs = inputs.permute(0, 2, 1)\n",
    "        print(inputs.shape)\n",
    "        return inputs\n",
    "\n",
    "inputs = output_normal\n",
    "inputs = torch.cat(torch.chunk(inputs, 2, 1), 0) # (2B, L/2, dim)\n",
    "inputs = downsample(inputs)\n",
    "inputs = torch.cat(torch.chunk(inputs, 2, 0), 1) # (B, L, dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cas pooling\n",
    "B, L, D = pooling_output.shape\n",
    "vis_pos = torch.zeros(B, L, 4, dtype=pooling_output.dtype)\n",
    "boxes = vis_pos\n",
    "print(boxes.shape)\n",
    "boxes = boxes[:, :pooling_output.shape[1]]\n",
    "boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 49, 4])\n"
     ]
    }
   ],
   "source": [
    "# cas non pooling\n",
    "B, L, D = output_normal.shape\n",
    "vis_pos = torch.zeros(B, L, 4, dtype=output_normal.dtype)\n",
    "boxes = vis_pos\n",
    "print(boxes.shape)\n",
    "# on coonsidre dans ce cas qu'on utilise 49 images ?\n",
    "# multiplier les features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 18, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boxes = torch.cat(torch.chunk(boxes[:,:36,:], 2, 1), 0)\n",
    "boxes = boxes[:, :output_normal.shape[1]//2]\n",
    "boxes = torch.cat(torch.chunk(boxes, 2, 0), 1)\n",
    "print(boxes.shape)\n",
    "36**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "choix des dimensions en entr en utilisant commentaire (mme si on ne veut vraiment se comparer car pas mme tche)\n",
    "\n",
    "Par exemple pooling pas forcmenet intressant pour nous de le faire\n",
    "on peut peut tre mieux comprendre l'image mme si on risque de s'y perdre\n",
    "\n",
    "pour une faire comparaison vaut peut tre mieux utiliser pooling (mme dimension pour clip)\n",
    "\n",
    "```\n",
    "Input images are resized to 224  224\n",
    "for the memory efficiency. We extract the 7  7 grid fea-\n",
    "tures produced by the last convolutional layer, and then ap-\n",
    "ply adaptive maximum-pooling over the features for down-\n",
    "sampling then to 6  6 for a fair comparison to [7].\n",
    "```\n",
    "\n",
    "Dans leur cas n'utilise pas la dernire couche\n",
    "\n",
    "Mme si on pourrait nous essayer de l'utiliser car peut tre suffisant\n",
    "Peut tre essayer les deux avec et sans pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_area(pos):\n",
    "    height = pos[:, :, 3] - pos[:, :, 2]\n",
    "    width = pos[:, :, 1] - pos[:, :, 0]\n",
    "    area = height * width\n",
    "    return area\n",
    "\n",
    "get_area(boxes)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "025084c40b588eb05019761c48ebfbdc57758708fc0433510479ba20b4a83a56"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('cvlp2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
