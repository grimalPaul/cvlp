{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test load model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLIP T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from cvlep.VLT5.param import Config\n",
    "\n",
    "if \"RANK\" in os.environ and \"WORLD_SIZE\" in os.environ:  # torchrun launch\n",
    "    rank = int(os.environ[\"RANK\"])\n",
    "    local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "    world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "elif int(os.environ.get('SLURM_NPROCS', 1)) > 1:  # slurm launch\n",
    "    rank = int(os.environ[\"SLURM_PROCID\"])\n",
    "    local_rank = int(os.environ[\"SLURM_LOCALID\"])\n",
    "    world_size = int(os.environ[\"SLURM_NPROCS\"])\n",
    "else:  # single gpu & process launch\n",
    "    rank = 0\n",
    "    local_rank = 0\n",
    "    world_size = 0\n",
    "\n",
    "config_encoder_question = \"experiments/config_vladapter/local/encoder_simple_adapter.json\"\n",
    "config_encoder_passage = \"experiments/config_vladapter/local/encoder_simple_adapter.json\"\n",
    "config_model = \"experiments/config_vladapter/local/config_model.json\"\n",
    "config_training = \"experiments/config_vladapter/local/training_simple_adapter.json\"\n",
    "\n",
    "# Training config\n",
    "config_training = Config.load_json(config_training)\n",
    "config_training.world_size = world_size\n",
    "config_training.rank = rank\n",
    "config_training.local_rank = local_rank\n",
    "if world_size > 1:\n",
    "    config_training.distributed = True\n",
    "    config_training.multiGPU = True\n",
    "else:\n",
    "    config_training.distributed = False\n",
    "    config_training.multiGPU = False\n",
    "\n",
    "config_encoder_question = Config.load_json(config_encoder_question)\n",
    "config_encoder_passage = Config.load_json(config_encoder_passage)\n",
    "config_model = Config.load_json(config_model)\n",
    "\n",
    "from cvlep.trainer_base_vladapter import Trainer\n",
    "\n",
    "trainer = Trainer(config_encoder_question,config_encoder_passage, config_model, config_training, train=False, local=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "a = nn.Linear(768, 10, bias= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_26852/4112195018.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_parameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "add_code_sample_docstrings() got an unexpected keyword argument 'tokenizer_class'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5679/1841660083.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcvlep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer_base_vladapter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_encoder_question\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconfig_encoder_passage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_training\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Projects/visual_language_representation/cvlep/trainer_base_vladapter.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDistributedDataParallel\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mDDP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistributed\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcvlep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCLIPT5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodeling_t5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVLT5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcvlep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCLIPT5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodeling_t5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mJointEncoder\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mencoderVLT5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcvlep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVLT5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Projects/visual_language_representation/cvlep/CLIPT5/modeling_t5.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBeamScorer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBeamSearchScorer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcvlep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCLIPT5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodeling_bart\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDownsample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSparseSample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOneDDownsample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m from .adapters import (\n",
      "\u001b[0;32m~/Documents/Projects/visual_language_representation/cvlep/CLIPT5/modeling_bart.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmy_transformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodeling_bart\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBartModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBartForConditionalGeneration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBartDecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBartEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Projects/visual_language_representation/cvlep/CLIPT5/my_transformers/modeling_bart.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0mBART_START_DOCSTRING\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1292\u001b[0m )\n\u001b[0;32m-> 1293\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mBartModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBartPretrainedModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mBartConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Projects/visual_language_representation/cvlep/CLIPT5/my_transformers/modeling_bart.py\u001b[0m in \u001b[0;36mBartModel\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1327\u001b[0m         \u001b[0mcheckpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"facebook/bart-large\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m         \u001b[0moutput_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSeq2SeqModelOutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m         \u001b[0mconfig_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_CONFIG_FOR_DOC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1330\u001b[0m     )\n\u001b[1;32m   1331\u001b[0m     def forward(\n",
      "\u001b[0;31mTypeError\u001b[0m: add_code_sample_docstrings() got an unexpected keyword argument 'tokenizer_class'"
     ]
    }
   ],
   "source": [
    "from cvlep.trainer_base_vladapter import Trainer\n",
    "\n",
    "trainer = Trainer(config_encoder_question,config_encoder_passage, config_model, config_training, train=False, local=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = trainer.model.image_passage_encoder.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VLT5(\n",
      "  (shared): Embedding(32200, 768)\n",
      "  (encoder): JointEncoder(\n",
      "    (embed_tokens): Embedding(32200, 768)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 12)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (4): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (5): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (6): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (7): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (8): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (9): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (10): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (11): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (visual_embedding): VisualEmbedding(\n",
      "      (feat_embedding): Sequential(\n",
      "        (0): Linear(in_features=2048, out_features=768, bias=True)\n",
      "        (1): T5LayerNorm()\n",
      "      )\n",
      "      (absolute_vis_pos_embedding): Sequential(\n",
      "        (0): Linear(in_features=5, out_features=768, bias=True)\n",
      "        (1): T5LayerNorm()\n",
      "      )\n",
      "      (obj_order_embedding): Embedding(32200, 768)\n",
      "      (img_order_embedding): Embedding(2, 768)\n",
      "    )\n",
      "  )\n",
      "  (decoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32200, 768)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 12)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (4): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (5): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (6): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (7): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (8): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (9): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (10): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (11): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=32200, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected 1 arguments, got 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_25082/4206866470.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_passage_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: expected 1 arguments, got 0"
     ]
    }
   ],
   "source": [
    "trainer.model.image_passage_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VLT5 without adapter or prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvlep.trainer_base_vladapter import Trainer\n",
    "\n",
    "\n",
    "\n",
    "trainer_model = Trainer(config_encoder_question, config_encoder_passage,config_model, config_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, p in trainer_model.model.image_passage_encoder.named_parameters():\n",
    "    if p.requires_grad==True:\n",
    "        print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvlep.trainer_base_vladapter import Trainer\n",
    "\n",
    "config_encoder_question = \"experiments/config_vladapter/prompt/encoder_prompting.json\"\n",
    "config_encoder_passage = \"experiments/config_vladapter/prompt/encoder_prompting.json\"\n",
    "config_training = \"experiments/config_vladapter/prompt/training_prompt.json\"\n",
    "\n",
    "trainer_clipt5 = Trainer(config_encoder_question, config_encoder_passage, config_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.prompt_modules.prompts.IR.prefix_embedding.0.weight\n",
      "encoder.prompt_modules.prompts.IR.prefix_embedding.1.weight\n",
      "encoder.prompt_modules.prompts.IR.prefix_embedding.1.bias\n",
      "encoder.prompt_modules.prompts.IR.prefix_embedding.3.weight\n",
      "encoder.prompt_modules.prompts.IR.prefix_embedding.3.bias\n"
     ]
    }
   ],
   "source": [
    "for n, p in trainer_clipt5.model.image_passage_encoder.named_parameters():\n",
    "    if p.requires_grad==True:\n",
    "        print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adapter config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvlep.trainer_base_vladapter import Trainer\n",
    "\n",
    "config_encoder_question = \"experiments/config_vladapter/adapter_for_contrastive/encoder_simple_adapter.json\"\n",
    "config_encoder_passage = \"experiments/config_vladapter/adapter_for_contrastive/encoder_simple_adapter.json\"\n",
    "config_training = \"experiments/config_vladapter/adapter_for_contrastive/training_simple_adapter.json\"\n",
    "\n",
    "trainer_clipt5 = Trainer(config_encoder_question, config_encoder_passage, config_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, p in trainer_clipt5.model.image_passage_encoder.named_parameters():\n",
    "    if p.requires_grad==True:\n",
    "        print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk, disable_caching\n",
    "disable_caching()\n",
    "path_dataset = \"data_model/dataset/small_dataset\"\n",
    "dataset = load_from_disk(path_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 11])\n",
      "torch.Size([1, 36, 2048])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from cvlep.utils import device\n",
    "index = 1\n",
    "item = dataset[1]\n",
    "key_vision_features = \"vlt5_features\"\n",
    "key_boxes = \"vlt5_normalized_boxes\"\n",
    "key_text=\"input\"\n",
    "tokenizer = trainer_model.tokenizer_question\n",
    "\n",
    "vision_features = torch.Tensor(item[key_vision_features]).to(device)\n",
    "boxes = torch.Tensor(item[key_boxes]).to(device)\n",
    "vision_features = torch.squeeze(vision_features, dim=1)\n",
    "vis_inputs = (vision_features,boxes)\n",
    "boxes = torch.squeeze(boxes, dim=2)\n",
    "input_ids = tokenizer(\n",
    "    item[key_text], return_tensors='pt', padding=True, truncation=True).input_ids.to(device)\n",
    "return_pooled_output=True,\n",
    "pool_strategy=\"avg\"\n",
    "print(input_ids.size())\n",
    "print(vision_features.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = trainer_model.embedding_question( input_ids = input_ids,\n",
    "    vis_inputs = vis_inputs,\n",
    "    pool_strategy = pool_strategy,\n",
    "    return_pooled_output = return_pooled_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.pooler_output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(32200, 768)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_model.model.image_passage_encoder.encoder.embed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.0044,  0.0702, -0.1362,  ..., -0.0093, -0.0669,  0.2152],\n",
       "         [-0.3346, -0.0321, -0.2023,  ..., -0.0346, -0.2422,  0.1325],\n",
       "         [-0.3988, -0.1186, -0.2928,  ..., -0.1005, -0.0245,  0.0725],\n",
       "         ...,\n",
       "         [ 0.0027,  0.0135,  0.0049,  ...,  0.0231, -0.0006, -0.0090],\n",
       "         [ 0.0780, -0.0152,  0.0395,  ...,  0.0947,  0.0811,  0.1531],\n",
       "         [ 0.0100,  0.0192,  0.0097,  ...,  0.0264, -0.0009, -0.0107]]]), pooler_output=tensor([[ 5.6648e-02, -7.1184e-02, -1.5301e-01, -3.6746e-02, -1.1649e-01,\n",
       "          3.5650e-02,  4.6196e-02, -9.3280e-02, -1.2167e-01,  1.7649e-01,\n",
       "         -1.1129e-01, -5.8460e-02,  9.5520e-02,  1.1314e-02,  4.1585e-02,\n",
       "         -1.2416e-02, -1.0474e-01,  3.2099e-02,  6.2653e-02,  4.1071e-02,\n",
       "          8.7517e-02, -2.3704e-01,  7.3841e-02, -1.6983e-01, -1.1024e-01,\n",
       "          1.1043e-01, -8.2290e-02, -2.5255e-02, -3.4988e-01,  1.7297e-01,\n",
       "          1.3527e-01,  2.0638e-01, -9.9829e-02,  2.6702e-02, -5.7706e-02,\n",
       "         -3.6114e-02, -2.1935e-01, -1.2021e-01,  4.0559e-02,  1.3619e-01,\n",
       "          1.1547e-01,  2.1434e-01,  4.3598e-02,  8.4799e-02, -1.3910e-01,\n",
       "         -7.6862e-02,  4.0068e-02,  8.2855e-02, -1.2356e-01, -2.3356e-02,\n",
       "          8.9281e-02, -2.4328e-01,  9.7265e-02,  6.4604e-02, -6.1479e-02,\n",
       "         -7.6788e-02,  2.0053e-01, -3.4412e-02, -4.9680e-02,  7.8987e-02,\n",
       "         -5.9437e-02, -1.5208e-01, -4.1240e-02, -1.8452e-03,  9.6731e-02,\n",
       "          2.5129e-03, -2.7472e-01,  6.3285e-03, -1.7972e-02,  5.6221e-02,\n",
       "          7.1725e-04,  1.1230e-01, -1.5480e-01,  1.7283e-01,  1.7725e-02,\n",
       "         -8.6306e-03, -5.8082e-02, -5.2759e-02, -1.4953e-02, -3.4251e-02,\n",
       "         -1.8136e-02, -7.2912e-02,  8.0153e-02, -4.5119e-02,  1.3014e-01,\n",
       "          1.7871e-01,  2.4108e-01,  2.5335e-01, -9.1655e-02,  1.0435e-01,\n",
       "         -6.0266e-02, -1.0284e-02, -5.8338e-02, -1.3664e-01,  4.4927e-03,\n",
       "          2.5686e-01,  1.0977e-01,  6.0540e-03,  9.0139e-02, -3.6682e-02,\n",
       "          1.0168e-01, -9.5243e-02,  6.0577e-02, -2.1505e-02, -2.4120e-01,\n",
       "         -3.9939e-02, -2.8749e-02,  1.3322e-02,  6.9176e-02,  1.3201e-01,\n",
       "          7.8436e-02, -1.1388e-03, -7.8952e-02, -1.1538e-01, -1.4526e-01,\n",
       "         -2.6577e-02,  1.1842e-01, -9.1086e-02, -1.2194e-01,  8.6370e-02,\n",
       "         -6.8970e-02,  1.9308e-02,  2.5355e-02, -1.7590e-01, -2.4092e-01,\n",
       "         -4.3160e-03,  5.0034e-02, -1.3436e-01, -4.2643e-02, -1.6965e-01,\n",
       "         -1.9567e-01,  1.1454e-01, -1.0889e-02, -1.8757e-01,  9.9909e-02,\n",
       "         -1.1825e-01, -3.9800e-02,  8.9019e-02,  3.1374e-02, -2.2346e-01,\n",
       "          4.8511e-03, -1.9611e-03, -8.6634e-02,  2.3533e-02,  1.2248e-01,\n",
       "         -1.0433e-01, -6.2481e-02,  3.6984e-02, -1.5841e-01,  1.0640e-01,\n",
       "         -1.6670e-01,  7.9076e-02,  6.0324e-02, -7.1101e-02,  8.4890e-02,\n",
       "         -2.3785e-01,  9.3966e-02,  1.5102e-01, -3.0517e-01, -1.9984e-01,\n",
       "         -8.9604e-02,  3.3380e-02, -8.0320e-02,  1.7646e-01,  1.9851e-02,\n",
       "          1.0962e-01, -2.9159e-02, -1.4942e-01,  1.0430e-01,  1.3476e-01,\n",
       "         -2.5006e-02,  4.2738e-02,  9.7129e-02, -4.5894e-02,  1.9123e-01,\n",
       "         -1.9203e-01,  6.1338e-02, -1.3621e-01, -6.9330e-02,  1.9044e-02,\n",
       "          1.7211e-02, -1.8426e-01, -7.0352e-03, -3.2355e-02, -1.2859e-01,\n",
       "          1.2835e-01, -8.0131e-02,  2.7095e-02, -1.6080e-01, -5.7696e-02,\n",
       "         -7.1342e-02,  6.3420e-02, -1.0779e-01,  1.9965e-01, -9.6445e-02,\n",
       "         -6.2895e-02,  7.8046e-02,  1.7269e-01, -6.3947e-02,  2.3196e-03,\n",
       "          5.7397e-02, -2.3577e-02,  3.3009e-02,  9.0933e-02, -1.3351e-01,\n",
       "          4.1424e-02,  3.3542e-03, -4.5304e-02, -1.7649e-01,  1.3891e-01,\n",
       "          1.1829e-01, -1.3549e-01,  1.1509e-01,  5.2406e-02,  8.0264e-04,\n",
       "         -2.5466e-01,  1.9854e-01,  2.8487e-02, -1.5047e-01, -2.2250e-01,\n",
       "         -4.1130e-01, -1.3324e-01,  5.2766e-03, -3.3753e-02,  2.8282e-02,\n",
       "          9.0963e-02,  1.0388e-01,  3.5608e-01, -3.8365e-02, -7.9780e-03,\n",
       "          5.2078e-02,  2.4794e-01,  1.6939e-01,  4.0255e-02,  1.3860e-01,\n",
       "         -5.7090e-02, -4.2355e-02, -2.3558e-02,  4.7343e-02, -6.7799e-02,\n",
       "         -1.1853e-01, -1.3973e-01, -5.0139e-02,  1.0560e-01,  1.6904e-01,\n",
       "          5.3939e-02,  1.1163e-01, -4.3946e-02,  1.6269e-01, -2.6001e-01,\n",
       "          1.8189e-01, -1.9251e-01,  1.9725e-01,  5.3467e-02,  4.2570e-02,\n",
       "          2.4926e-02,  1.0339e-01, -4.6494e-02,  5.0573e-02, -1.0053e-01,\n",
       "         -3.5200e-02,  3.5145e-01,  3.4159e-03, -1.5362e-01, -9.6309e-03,\n",
       "          2.7890e-01, -6.3908e-02,  1.6790e-01, -7.0743e-02,  1.1093e-01,\n",
       "         -4.0014e-02, -8.8798e-02, -1.0509e-01, -2.9235e-02,  3.5896e-02,\n",
       "         -1.4439e-01,  2.7185e-02, -3.8404e-02, -1.4742e-02,  1.1536e-01,\n",
       "         -2.5055e-01,  7.6182e-02,  8.4615e-05,  1.0779e-01, -1.9729e-01,\n",
       "         -4.8202e-03, -1.1150e-01, -1.5042e-01, -6.8012e-03,  6.0199e-02,\n",
       "          1.0188e-01, -1.0791e-01,  1.1259e-01, -4.7317e-02,  7.9064e-02,\n",
       "         -8.2617e-02, -1.8037e-01, -5.9022e-02, -5.2889e-02,  4.6451e-02,\n",
       "          1.3843e-01, -1.2509e-01,  6.5980e-02, -1.9412e-02,  8.8937e-02,\n",
       "         -8.0666e-02,  4.3589e-02, -1.7870e-01, -7.4802e-03,  8.0770e-02,\n",
       "          1.0898e-01, -1.6556e-02,  9.7671e-02, -1.6894e-01,  1.0011e-01,\n",
       "          1.3348e-01, -3.3382e-02, -2.6455e-02,  4.9345e-02, -1.8567e-03,\n",
       "         -2.6573e-01,  9.9391e-02,  2.0684e-01, -1.5813e-02,  6.2283e-02,\n",
       "         -1.0348e-01,  2.5368e-01, -8.7927e-02, -3.6937e-02, -2.9318e-02,\n",
       "          9.8028e-02, -1.4213e-02, -9.6586e-02,  5.4695e-02, -1.5071e-01,\n",
       "         -4.2884e-02,  2.1310e-02,  5.0642e-02,  3.1646e-02, -1.3338e-01,\n",
       "          1.0224e-01, -2.3936e-01, -6.6592e-02, -2.0546e-01, -1.2527e-01,\n",
       "          1.1854e-01, -1.9952e-02,  9.4907e-02, -1.0361e-01, -7.1104e-02,\n",
       "         -1.6286e-01, -6.7716e-02,  1.6073e-01,  5.2321e-02,  1.1655e-01,\n",
       "          1.4920e-01, -1.7601e-01, -4.5349e-02, -1.0495e-01,  1.7389e-02,\n",
       "         -2.6232e-02,  2.5699e-01,  2.4474e-02,  1.0477e-01, -7.6520e-02,\n",
       "          9.9629e-02, -8.4367e-02, -9.9720e-03,  1.4627e-01,  1.5038e-01,\n",
       "         -8.4939e-02, -3.8132e-02, -5.5359e-02, -1.5582e-01, -1.0076e-01,\n",
       "          1.2360e-02, -6.2420e-02,  1.6432e-01,  1.5075e-01, -5.8578e-02,\n",
       "         -4.8055e-02,  1.3318e-02,  1.2016e-01, -3.3128e-02,  1.2273e-01,\n",
       "          2.2809e-02, -3.5356e-02,  1.0881e-01,  2.1894e-01, -8.9597e-02,\n",
       "         -5.1360e-02, -4.0064e-02, -3.1258e-02, -2.8262e-02,  1.0273e-01,\n",
       "         -3.8696e-02,  9.5733e-02, -7.1016e-02, -2.5011e-03,  9.0620e-02,\n",
       "          8.7721e-02,  5.9105e-02,  1.4423e-01,  8.5796e-02, -4.0406e-02,\n",
       "         -1.1256e-01,  2.5545e-02,  1.5949e-01,  7.2592e-02,  9.6070e-02,\n",
       "          3.1162e-03, -5.0051e-02, -5.5794e-03, -5.9596e-03, -9.1406e-02,\n",
       "          1.7434e-01,  7.1470e-02,  1.0839e-01, -2.6532e-03,  1.6383e-02,\n",
       "         -4.1331e-02,  1.4214e-02,  1.1198e-02,  1.6541e-02,  8.2796e-02,\n",
       "         -4.0043e-02, -1.0858e-01,  4.2423e-02,  1.2854e-01,  7.3324e-03,\n",
       "         -1.8378e-01, -9.4893e-03, -1.1000e-01,  1.4518e-01,  1.2635e-01,\n",
       "          1.1003e-02, -8.8769e-02, -4.3025e-02,  2.7864e-02,  1.2064e-01,\n",
       "          1.0577e-01, -1.5479e-01,  1.0298e-01,  1.8620e-02,  2.3587e-01,\n",
       "          4.6578e-03, -7.9362e-02, -2.6344e-02, -1.1929e-01, -1.4926e-02,\n",
       "          1.8412e-01, -3.6151e-02, -2.1832e-01, -1.3871e-01, -3.2910e-02,\n",
       "          8.7190e-02, -3.3584e-02, -5.9691e-02,  8.6242e-04, -2.4600e-01,\n",
       "          4.5402e-02,  1.5846e-01,  1.1855e-01,  4.0482e-02, -5.7792e-02,\n",
       "         -1.0772e-01,  1.7057e-01,  1.4509e-01, -1.4244e-02,  8.8743e-02,\n",
       "         -1.6682e-02,  2.1597e-01, -7.3645e-02,  8.4626e-02, -2.0321e-01,\n",
       "          5.3152e-02, -4.1354e-02, -1.1845e-01, -6.6002e-02, -2.3721e-01,\n",
       "          2.0754e-01, -2.4641e-01,  1.1394e-01,  7.3031e-02, -2.1068e-01,\n",
       "         -9.9003e-02, -1.0719e-01, -3.6220e-02,  3.8718e-02, -4.2863e-02,\n",
       "         -4.2863e-02, -4.4179e-02,  3.2717e-01,  1.8560e-01, -3.3248e-02,\n",
       "         -7.4413e-02,  9.2814e-02,  2.3129e-02, -5.6121e-02,  5.1840e-02,\n",
       "          2.8309e-02, -1.3142e-01,  2.4845e-02,  7.2732e-02,  3.9975e-02,\n",
       "          1.7479e-01, -1.3004e-01,  2.3654e-02,  5.0932e-02, -1.2324e-01,\n",
       "          7.7021e-02,  4.7436e-02, -2.6862e-01,  4.3295e-02,  6.1174e-02,\n",
       "          1.2143e-01,  4.1469e-01, -1.9343e-01,  1.1031e-01,  2.1305e-02,\n",
       "          1.2391e-02, -1.6763e-01, -1.6243e-01,  1.0325e-02, -1.5955e-01,\n",
       "         -1.9574e-01, -1.2972e-01,  2.8366e-04,  2.0014e-01, -1.5067e-01,\n",
       "         -1.3287e-01, -9.0149e-02, -2.6842e-01,  7.0291e-02,  2.4570e-02,\n",
       "         -3.4946e-02, -1.5780e-01, -2.4727e-01,  2.2608e-01, -6.9311e-02,\n",
       "          1.6046e-01, -1.3901e-01, -3.2820e-02,  2.8448e-01,  1.9665e-01,\n",
       "          1.8996e-01, -5.9038e-02, -1.6209e-01, -7.8300e-02,  1.0704e-01,\n",
       "          1.1561e-01,  3.5261e-02,  1.1100e-01,  6.1467e-02,  6.9499e-02,\n",
       "          1.4634e-01, -1.5710e-01,  3.1434e-02, -8.9579e-02, -1.1669e-01,\n",
       "         -5.8055e-02, -4.0590e-02,  9.0771e-02, -8.0851e-02, -1.1806e-01,\n",
       "         -2.5185e-02,  1.4855e-02, -5.9336e-02, -6.5623e-02,  9.7041e-02,\n",
       "          2.7690e-01, -1.9987e-02, -2.8658e-02, -1.4633e-01, -8.7065e-03,\n",
       "         -3.3145e-02,  1.4271e-02, -1.1132e-01,  5.0628e-02, -7.1439e-02,\n",
       "          1.4279e-01,  2.0576e-01,  1.3376e-01, -1.3618e-01,  1.0161e-01,\n",
       "         -6.6048e-02,  1.7167e-01,  6.4942e-02, -2.0768e-01,  9.5287e-02,\n",
       "          6.1489e-03, -4.3719e-02,  1.4714e-01,  1.7106e-02,  2.5687e-01,\n",
       "         -6.8975e-02,  6.0700e-02,  1.1607e-01, -8.8286e-02, -2.5594e-02,\n",
       "          1.4360e-01, -9.5792e-02, -6.7379e-02,  1.5858e-01, -1.8343e-01,\n",
       "         -4.1773e-03,  1.1835e-01, -1.2830e-01, -4.7252e-03, -6.9979e-02,\n",
       "         -4.3474e-03, -1.5716e-01, -7.8922e-03,  5.4170e-02, -8.5557e-02,\n",
       "         -1.3081e-01, -1.4872e-01, -1.4497e-02,  2.2065e-01, -1.3079e-01,\n",
       "         -5.6204e-02,  1.4542e-01,  7.2989e-02,  2.0288e-01,  8.7270e-02,\n",
       "         -2.5098e-02, -1.4824e-01, -1.6793e-01, -1.1220e-02, -2.1143e-01,\n",
       "         -8.6873e-02,  1.9970e-01,  1.1379e-01, -2.0107e-02,  1.3206e-01,\n",
       "          2.0396e-01,  1.4635e-01,  5.7540e-02,  2.1541e-01,  1.1418e-01,\n",
       "         -5.5772e-02,  2.7615e-01, -1.5337e-01, -5.8670e-03,  2.5867e-01,\n",
       "         -6.3398e-02,  3.8931e-01,  2.4190e-01,  1.5852e-01, -1.0393e-02,\n",
       "          2.4114e-02,  1.8368e-01,  1.6601e-01, -5.4443e-02, -1.8606e-01,\n",
       "         -2.8720e-01, -5.0861e-02, -2.4094e-02, -9.5253e-04,  1.2215e-02,\n",
       "          1.4893e-03, -9.4679e-02, -1.0176e-01,  7.2016e-02, -1.4162e-01,\n",
       "          8.9151e-04, -5.5513e-02,  2.1170e-01, -4.1009e-03, -5.7914e-02,\n",
       "         -8.8402e-02,  5.2346e-03, -8.2523e-02,  2.2422e-02, -7.5668e-02,\n",
       "         -1.3940e-02,  7.4526e-02,  5.4616e-02, -8.9429e-02,  8.5130e-02,\n",
       "         -1.8144e-02, -1.8300e-03,  1.3815e-01, -2.0179e-01, -1.2929e-01,\n",
       "         -2.5350e-01, -3.8829e-02, -6.4619e-02, -9.1117e-02, -1.3473e-01,\n",
       "          1.5278e-01, -2.3307e-01,  5.3524e-02,  9.3714e-02, -1.2227e-02,\n",
       "          7.9154e-02,  1.2854e-01, -8.1355e-02, -2.4539e-01,  1.7300e-02,\n",
       "          1.5464e-01, -3.2712e-01,  1.0083e-01, -3.0230e-02,  7.0219e-02,\n",
       "         -1.2951e-01, -2.4582e-03,  1.6231e-01, -1.1312e-01, -5.8386e-02,\n",
       "          1.9637e-01,  3.1791e-02, -3.1436e-02, -2.4544e-02,  2.0815e-02,\n",
       "         -7.6786e-02, -1.9849e-02,  6.4449e-03, -5.7112e-02, -4.5437e-02,\n",
       "         -1.0695e-01,  1.0656e-01,  4.3160e-02, -2.0950e-01,  2.1037e-01,\n",
       "         -1.7241e-02,  5.1989e-02,  1.6865e-02,  7.8504e-02,  1.9873e-03,\n",
       "          1.2072e-01,  1.2359e-01, -1.5053e-01,  1.2013e-01, -6.5865e-03,\n",
       "          8.9148e-02,  2.5787e-01, -8.8401e-02,  1.0379e-01,  2.1067e-01,\n",
       "         -1.3900e-02, -8.2743e-02,  1.4271e-02,  1.0033e-01, -1.3111e-01,\n",
       "          1.2430e-01, -4.0163e-02, -1.1760e-01,  1.0495e-01, -2.5412e-01,\n",
       "          3.0550e-02,  2.1363e-01, -6.0252e-02, -9.7014e-02, -2.9208e-02,\n",
       "          1.3326e-01,  5.4465e-02,  6.5086e-02,  1.4199e-01, -3.0988e-02,\n",
       "          1.9695e-01,  2.3529e-02, -1.9665e-01,  1.0276e-01, -1.8946e-01,\n",
       "         -1.2714e-01,  1.4359e-01, -3.7140e-02]]), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_model.encoder_question.encoder.forward(\n",
    "    input_ids = input_ids,\n",
    "    vis_inputs = vis_inputs,\n",
    "    pool_strategy = pool_strategy,\n",
    "    return_pooled_output = return_pooled_output\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk, disable_caching\n",
    "disable_caching()\n",
    "path_dataset = \"data_model/dataset/small_dataset\"\n",
    "dataset = load_from_disk(path_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from cvlep.utils import device\n",
    "index = 2\n",
    "item = dataset[1]\n",
    "key_vision_features = \"vlt5_features\"\n",
    "key_boxes = \"vlt5_normalized_boxes\"\n",
    "key_text=\"input\"\n",
    "tokenizer = trainer_clipt5.tokenizer_question\n",
    "\n",
    "vision_features = torch.Tensor(item[key_vision_features]).to(device)\n",
    "boxes = torch.Tensor(item[key_boxes]).to(device)\n",
    "vision_features = torch.squeeze(vision_features, dim=1)\n",
    "boxes = torch.squeeze(boxes, dim=1)\n",
    "input_ids = tokenizer(\n",
    "    item[key_text], return_tensors='pt', padding=True, truncation=True).input_ids.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_encoder_vlt5 = trainer_vlt5.model.image_question_encoder.encoder.forward(\n",
    "    input_ids = input_ids,\n",
    "    vis_inputs=(vision_features,boxes)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_encoder_clipT5 = trainer_clipt5.model.image_question_encoder.encoder.forward(\n",
    "    input_ids=input_ids,\n",
    "    vis_inputs=(vision_features,boxes)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0044,  0.0702, -0.1362,  ..., -0.0093, -0.0669,  0.2152],\n",
       "         [-0.3346, -0.0321, -0.2023,  ..., -0.0346, -0.2422,  0.1325],\n",
       "         [-0.3988, -0.1186, -0.2928,  ..., -0.1005, -0.0245,  0.0725],\n",
       "         ...,\n",
       "         [ 0.0027,  0.0135,  0.0049,  ...,  0.0231, -0.0006, -0.0090],\n",
       "         [ 0.0780, -0.0152,  0.0395,  ...,  0.0947,  0.0811,  0.1531],\n",
       "         [ 0.0100,  0.0192,  0.0097,  ...,  0.0264, -0.0009, -0.0107]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_encoder_vlt5[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = output_encoder_clipT5[0] == output_encoder_vlt5[0]\n",
    "torch.equal(output_encoder_clipT5[0],output_encoder_vlt5[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pgrimal/miniconda3/envs/cvlp/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of VLT5 were not initialized from the model checkpoint at data_model/t5_pretrained and are newly initialized: ['encoder.visual_embedding.feat_embedding.0.weight', 'encoder.visual_embedding.feat_embedding.0.bias', 'encoder.visual_embedding.feat_embedding.1.weight', 'encoder.visual_embedding.absolute_vis_pos_embedding.0.weight', 'encoder.visual_embedding.absolute_vis_pos_embedding.0.bias', 'encoder.visual_embedding.absolute_vis_pos_embedding.1.weight', 'encoder.visual_embedding.obj_order_embedding.weight', 'encoder.visual_embedding.img_order_embedding.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of VLT5 were not initialized from the model checkpoint at data_model/t5_pretrained and are newly initialized: ['encoder.visual_embedding.feat_embedding.0.weight', 'encoder.visual_embedding.feat_embedding.0.bias', 'encoder.visual_embedding.feat_embedding.1.weight', 'encoder.visual_embedding.absolute_vis_pos_embedding.0.weight', 'encoder.visual_embedding.absolute_vis_pos_embedding.0.bias', 'encoder.visual_embedding.absolute_vis_pos_embedding.1.weight', 'encoder.visual_embedding.obj_order_embedding.weight', 'encoder.visual_embedding.img_order_embedding.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from  data_model/whole_model/vlt5/VLT5Epoch30.pth\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['encoder.visual_embedding.layer_norm.weight'])\n",
      "Model loaded from  data_model/whole_model/vlt5/VLT5Epoch30.pth\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['encoder.visual_embedding.layer_norm.weight'])\n",
      "Configurations\n",
      "{'share_embedding': True, 'share_vis_embedding': True}\n"
     ]
    }
   ],
   "source": [
    "from cvlep.trainer_base_vladapter import Trainer\n",
    "\n",
    "config_encoder_question = \"experiments/config_vladapter/model_only/args_vlt5_wo_adap.json\"\n",
    "config_encoder_passage = \"experiments/config_vladapter/model_only/args_vlt5_wo_adap.json\"\n",
    "config_model = \"experiments/config_vladapter/model_only/config_model.json\"\n",
    "config_training = \"experiments/config_vladapter/model_only/training_vlt5.json\"\n",
    "\n",
    "trainer_model = Trainer(config_encoder_question, config_encoder_passage,config_model, config_training)\n",
    "\n",
    "from datasets import load_from_disk, disable_caching\n",
    "disable_caching()\n",
    "path_dataset = \"data_model/dataset/small_dataset\"\n",
    "dataset = load_from_disk(path_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from cvlep.utils import device\n",
    "\n",
    "\n",
    "def get_feat(item):\n",
    "    key_vision_features = \"vlt5_features\"\n",
    "    key_boxes = \"vlt5_normalized_boxes\"\n",
    "    key_text=\"input\"\n",
    "    vision_features = torch.Tensor(item[key_vision_features])\n",
    "    boxes = torch.Tensor(item[key_boxes])\n",
    "    vision_features = torch.squeeze(vision_features, dim=0)\n",
    "    boxes = torch.squeeze(boxes, dim=0)\n",
    "    return {\"text\":item[key_text], \"feats\":vision_features, \"boxes\":boxes, \"size\":boxes.size()[0]}\n",
    "\n",
    "B = 2\n",
    "question1 = get_feat(dataset[0])\n",
    "relevant1 = get_feat(dataset[0])   \n",
    "irrelevant1 = get_feat(dataset[1])\n",
    "question2 = get_feat(dataset[2])\n",
    "relevant2 = get_feat(dataset[2])\n",
    "irrelevant2 = get_feat(dataset[3])\n",
    "\n",
    "item1 = {}\n",
    "item1[\"question_text\"] = question1[\"text\"]\n",
    "item1[\"passage_relevant_text\"] = relevant1['text']\n",
    "item1['passage_irrelevant_text'] = irrelevant1[\"text\"]\n",
    "item1[\"n_boxes_question\"] = question1[\"size\"]\n",
    "item1[\"n_boxes_passage_relevant\"] = relevant1[\"size\"]\n",
    "item1[\"n_boxes_passage_irrelevant\"] = irrelevant1[\"size\"]\n",
    "item1[\"question_image_features\"] = question1['feats']\n",
    "item1[\"question_image_boxes\"] = question1[\"boxes\"]\n",
    "item1[\"passage_relevant_image_features\"]=relevant1[\"feats\"]\n",
    "item1[\"passage_relevant_image_boxes\"]=relevant1[\"boxes\"]\n",
    "item1[\"passage_irrelevant_image_features\"]=irrelevant1['feats']\n",
    "item1[\"passage_irrelevant_image_boxes\"]=irrelevant1[\"boxes\"]\n",
    "\n",
    "item2 = {}\n",
    "item2[\"question_text\"] = question2[\"text\"]\n",
    "item2[\"passage_relevant_text\"] = relevant2['text']\n",
    "item2['passage_irrelevant_text'] = irrelevant2[\"text\"]\n",
    "item2[\"n_boxes_question\"] = question2[\"size\"]\n",
    "item2[\"n_boxes_passage_relevant\"] = relevant2[\"size\"]\n",
    "item2[\"n_boxes_passage_irrelevant\"] = irrelevant2[\"size\"]\n",
    "item2[\"question_image_features\"] = question2['feats']\n",
    "item2[\"question_image_boxes\"] = question2[\"boxes\"]\n",
    "item2[\"passage_relevant_image_features\"]=relevant2[\"feats\"]\n",
    "item2[\"passage_relevant_image_boxes\"]=relevant2[\"boxes\"]\n",
    "item2[\"passage_irrelevant_image_features\"]=irrelevant2['feats']\n",
    "item2[\"passage_irrelevant_image_boxes\"]=irrelevant2[\"boxes\"]\n",
    "\n",
    "batch = [item1, item2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 2\n",
    "\n",
    "tokenizer = trainer_model.tokenizer_question\n",
    "V_L_question = max(item['n_boxes_question'] for item in batch)\n",
    "V_L_context = max(max(item['n_boxes_passage_relevant'],\n",
    "                    item['n_boxes_passage_irrelevant']) for item in batch)\n",
    "feat_dim = batch[0]['question_image_features'].shape[-1]\n",
    "# boxes are represented by 4 points\n",
    "question_boxes = torch.zeros(B, V_L_question, 4, dtype=torch.float)\n",
    "question_vis_feats = torch.zeros(\n",
    "    B, V_L_question, feat_dim, dtype=torch.float)\n",
    "relevant_boxes = torch.zeros(B, V_L_context, 4, dtype=torch.float)\n",
    "relevant_vis_feats = torch.zeros(\n",
    "    B, V_L_context, feat_dim, dtype=torch.float)\n",
    "irrelevant_boxes = torch.zeros(\n",
    "    B, V_L_context, 4, dtype=torch.float)\n",
    "irrelevant_vis_feats = torch.zeros(\n",
    "    B, V_L_context, feat_dim, dtype=torch.float)\n",
    "\n",
    "relevant_text, irrelevant_text, question_text, labels = list(), list(), list(), list()\n",
    "for i, item in enumerate(batch):\n",
    "    # TODO: voir si besoin de changer gestion pour le text, car a un impact sur attention mask\n",
    "    question_text.append(item['question_text'])\n",
    "    relevant_text.append(item['passage_relevant_text'])\n",
    "    irrelevant_text.append(item['passage_irrelevant_text'])\n",
    "    if tokenizer:\n",
    "        n_boxes_relevant = item['n_boxes_passage_relevant']\n",
    "        n_boxes_irrelevant = item['n_boxes_passage_irrelevant']\n",
    "        n_boxes_question = item['n_boxes_question']\n",
    "        question_boxes[i,\n",
    "                        :n_boxes_question] = item['question_image_boxes']\n",
    "        question_vis_feats[i,\n",
    "                            :n_boxes_question] = item['question_image_features']\n",
    "        relevant_boxes[i,\n",
    "                        :n_boxes_relevant] = item['passage_relevant_image_boxes']\n",
    "        relevant_vis_feats[i,\n",
    "                            :n_boxes_relevant] = item['passage_relevant_image_features']\n",
    "        irrelevant_boxes[i,\n",
    "                            :n_boxes_irrelevant] = item['passage_irrelevant_image_boxes']\n",
    "        irrelevant_vis_feats[i,\n",
    "                                :n_boxes_irrelevant] = item['passage_irrelevant_image_features']\n",
    "    if item['passage_relevant_text'] is None:\n",
    "        labels.append(-100)  # ignore index when computing the loss\n",
    "    else:\n",
    "        labels.append(i)\n",
    "\n",
    "question_input = tokenizer(\n",
    "    question_text, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "context_input = tokenizer(\n",
    "    relevant_text + irrelevant_text, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "labels = torch.tensor(labels)\n",
    "visual_feats_context = torch.concat(\n",
    "    [relevant_vis_feats, irrelevant_vis_feats])\n",
    "context_image_boxes = torch.concat(\n",
    "    [relevant_boxes, irrelevant_boxes])\n",
    "results = {\n",
    "    \"input_ids_question\": question_input.input_ids,\n",
    "    \"attention_mask_question\": question_input.attention_mask,\n",
    "    \"input_ids_context\": context_input.input_ids,\n",
    "    \"attention_mask_context\": context_input.attention_mask,\n",
    "    \"labels\": labels,\n",
    "    \"visual_feats_question\": question_vis_feats,\n",
    "    \"visual_feats_context\": visual_feats_context,\n",
    "    \"question_image_boxes\": question_boxes,\n",
    "    \"context_image_boxes\": context_image_boxes\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1641)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loss doit tre faible car on met exactement mme embedding pour question \n",
    "# et relevant passage\n",
    "trainer_model.model.train_step(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test gather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0, 1, 2, 3]), tensor([   0,    1, -100,    3])]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "loss_fct = nn.NLLLoss(reduction='mean')\n",
    "batch1 ={\n",
    "\"labels\": torch.tensor([0,1,2,3])\n",
    "}\n",
    "batch2 = {\n",
    "\"labels\": torch.tensor([0,1,-100,3])\n",
    "}\n",
    "labels1 = batch1.pop('labels')\n",
    "labels2 = batch2.pop('labels')\n",
    "labels_gatherer = [labels1, labels2]\n",
    "gatherers = zip(labels_gatherer)\n",
    "labels_gatherer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([0, 1, 2, 3]), tensor([   8,    9, -100,   11])]\n"
     ]
    }
   ],
   "source": [
    "# N questions taille du batch\n",
    "N = 4\n",
    "# nombre relevant et irrelevant 1 et 1\n",
    "global_labels = []\n",
    "label_shift = 0\n",
    "for i, (received_labels) in enumerate(gatherers):\n",
    "    received_labels = received_labels[0]\n",
    "    received_labels[received_labels!=-100] += label_shift\n",
    "    label_shift += 4 * 2 # N * M\n",
    "    global_labels.append(received_labels)\n",
    "print(global_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "exemple d'un batch de 1\n",
    "```\n",
    "| question1 | * [relevant1, irr1, relevant2, irr2]\n",
    "| question2 |\n",
    "```\n",
    "label devra donc etre [0, 2]\n",
    "ce qui explique le besoin du shift\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "m = nn.LogSoftmax(dim=1)\n",
    "loss = nn.NLLLoss()\n",
    "# input is of size N x C = 3 x 5\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "# each element in target has to have 0 <= value < C\n",
    "target = torch.tensor([1, 0, 4])\n",
    "output = loss(m(input), target)\n",
    "output.backward()\n",
    "# 2D loss example (used, for example, with image inputs)\n",
    "N, C = 5, 4\n",
    "loss = nn.NLLLoss()\n",
    "# input is of size N x C x height x width\n",
    "data = torch.randn(N, 16, 10, 10)\n",
    "conv = nn.Conv2d(16, C, (3, 3))\n",
    "m = nn.LogSoftmax(dim=1)\n",
    "# each element in target has to have 0 <= value < C\n",
    "target = torch.empty(N, 8, 8, dtype=torch.long).random_(0, C)\n",
    "output = loss(m(conv(data)), target)\n",
    "output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5051658153533936"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " epoch 0 | Loss 2.5: 100%|| 2/2 [00:02<00:00,  1.00s/it]\n",
      " epoch 1 | Loss 0.625: 100%|| 2/2 [00:02<00:00,  1.00s/it]\n",
      " epoch 2 | Loss 0.15625: 100%|| 2/2 [00:02<00:00,  1.00s/it]\n",
      " epoch 3 | Loss 0.0390625: 100%|| 2/2 [00:02<00:00,  1.00s/it]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "loss = 10\n",
    "for epoch in range(4):\n",
    "    pbar = tqdm(total=2, ncols=120)\n",
    "    for batch in range(2):\n",
    "        loss *= 0.5\n",
    "        time.sleep(1)\n",
    "        desc_str = f' epoch {epoch} | Loss {loss}'\n",
    "        pbar.set_description(desc_str)\n",
    "        pbar.update(1)\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvlep.VLT5.param import Config\n",
    "config = Config.load_json(\"experiments/config_vladapter/prompt/training_prompt.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.rank = 'yes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.local = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/sentence-transformers/sentence-t5-base\n",
    "\n",
    "https://github.com/UKPLab/sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pgrimal/miniconda3/envs/cvlp/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/sentence-t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = model.__getitem__(3)\n",
    "dense = model.__getitem__(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T_destination',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_backward_hooks',\n",
       " '_buffers',\n",
       " '_call_impl',\n",
       " '_forward_hooks',\n",
       " '_forward_pre_hooks',\n",
       " '_get_backward_hooks',\n",
       " '_get_name',\n",
       " '_is_full_backward_hook',\n",
       " '_load_from_state_dict',\n",
       " '_load_state_dict_pre_hooks',\n",
       " '_maybe_warn_non_full_backward_hook',\n",
       " '_modules',\n",
       " '_named_members',\n",
       " '_non_persistent_buffers_set',\n",
       " '_parameters',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_replicate_for_data_parallel',\n",
       " '_save_to_state_dict',\n",
       " '_slow_forward',\n",
       " '_state_dict_hooks',\n",
       " '_version',\n",
       " 'add_module',\n",
       " 'apply',\n",
       " 'bfloat16',\n",
       " 'buffers',\n",
       " 'children',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'double',\n",
       " 'dump_patches',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'float',\n",
       " 'forward',\n",
       " 'get_buffer',\n",
       " 'get_extra_state',\n",
       " 'get_parameter',\n",
       " 'get_submodule',\n",
       " 'half',\n",
       " 'load_state_dict',\n",
       " 'modules',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'parameters',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_full_backward_hook',\n",
       " 'register_module',\n",
       " 'register_parameter',\n",
       " 'requires_grad_',\n",
       " 'set_extra_state',\n",
       " 'share_memory',\n",
       " 'state_dict',\n",
       " 'to',\n",
       " 'to_empty',\n",
       " 'train',\n",
       " 'training',\n",
       " 'type',\n",
       " 'xpu',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense.activation_function.__dir__()\n",
    "# https://github.com/UKPLab/sentence-transformers/blob/46a149433fe9af0851f7fa6f9bf37b5ffa2c891c/sentence_transformers/models/Dense.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Normalize()"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TriviaQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration PaulLerner--triviaqa_for_viquae-ab2ffed0dcbf3b5f\n",
      "Reusing dataset parquet (/home/pgrimal/.cache/huggingface/datasets/PaulLerner___parquet/PaulLerner--triviaqa_for_viquae-ab2ffed0dcbf3b5f/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n",
      "100%|| 4/4 [00:00<00:00, 172.60it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"PaulLerner/triviaqa_for_viquae\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    with_viquae_test: Dataset({\n",
       "        features: ['id', 'input', 'meta', 'output'],\n",
       "        num_rows: 1247\n",
       "    })\n",
       "    with_viquae_train: Dataset({\n",
       "        features: ['id', 'input', 'meta', 'output'],\n",
       "        num_rows: 1181\n",
       "    })\n",
       "    with_viquae_validation: Dataset({\n",
       "        features: ['id', 'input', 'meta', 'output'],\n",
       "        num_rows: 1234\n",
       "    })\n",
       "    without_viquae: Dataset({\n",
       "        features: ['id', 'input', 'meta', 'output'],\n",
       "        num_rows: 47000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "dataset_kilt = load_from_disk(\"/home/pgrimal/Documents/data_viquae/kilt_trivia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['kilt_id', 'wikipedia_id', 'wikipedia_title', 'text', 'anchors', 'categories', 'wikidata_info', 'history'],\n",
       "    num_rows: 5903530\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_kilt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['summarization', 'translation_en_to_de', 'translation_en_to_fr', 'translation_en_to_ro'])\n"
     ]
    }
   ],
   "source": [
    "print(model.config.task_specific_params.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "British spy.\n"
     ]
    }
   ],
   "source": [
    "# inference\n",
    "input_ids = tokenizer(\n",
    "    \"James Bond is a <extra_id_0> <extra_id_1>\", return_tensors=\"pt\"\n",
    ").input_ids  # Batch size 1\n",
    "outputs = model.generate(input_ids)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ship built in 1912.\n"
     ]
    }
   ],
   "source": [
    "# inference\n",
    "input_ids = tokenizer(\n",
    "    \"Titanic is a <extra_id_0> <extra_id_1>\", return_tensors=\"pt\"\n",
    ").input_ids  # Batch size 1\n",
    "outputs = model.generate(input_ids)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLIP preprocessing\n",
    "\n",
    "But : comprendre comment sont gnrs les features actuellement avec clip\n",
    "Si bonne facon d'utiliser clip on laisse comme a sinon on essaie de fournir juste un embedding d'image et pas plusieurs embedding en fonction des boxes\n",
    "\n",
    "Fichier utilis cvlep.CLIPT5.clip_prepro_feats\n",
    "\n",
    "charger le modle :\n",
    "\n",
    "Dans VLT5 il y a cette fonction\n",
    "```py\n",
    "def vis_forward(self, batch, device):\n",
    "    if hasattr(self, \"vis_encoder\"):\n",
    "        # self.vis_encoder.eval() # freeze the batchnorm statistics\n",
    "        images = batch[\"images\"].to(device)\n",
    "\n",
    "        if self.config.vis_pooling_output:\n",
    "            _, vis_feats = self.vis_encoder(images)\n",
    "        else:\n",
    "            vis_feats, _ = self.vis_encoder(images)\n",
    "        # vis_feats: (B, dim, L ** 0.5, L ** 0.5)\n",
    "        B, L, D = vis_feats.shape\n",
    "        vis_pos = torch.zeros(B, L, 4, dtype=vis_feats.dtype)\n",
    "\n",
    "        batch[\"vis_feats\"] = vis_feats\n",
    "        batch[\"boxes\"] = vis_pos\n",
    "```\n",
    "\n",
    "Voir ce que renvoie le modle avec la fonction\n",
    "\n",
    "```py\n",
    "self.vis_encoder = get_vis_encoder(\n",
    "        backbone=vis_encoder_type, \n",
    "        image_size=eval(self.args.image_size)[0],\n",
    "        adapter_type=None,\n",
    "    )\n",
    "self.model.vis_encoder = self.vis_encoder\n",
    "```\n",
    "\n",
    "get_vis encoder dans vis_encoder.py\n",
    "\n",
    "Use CLIP-ResNet50 for fair comparaison\n",
    "\n",
    "format en entre de vis_forward\n",
    "regarder dans vlt5 cococaption T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvlep.CLIPT5.vis_encoder import get_vis_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_vis_encoder(backbone='RN101', adapter_type=None, image_size=eval(\"(224,224)\")[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision.transforms import (\n",
    "    Compose, Resize, CenterCrop, ToTensor, Normalize, RandomCrop, RandomHorizontalFlip, RandomErasing\n",
    ")\n",
    "def _transform(n_px):\n",
    "    return Compose([\n",
    "        # PadToSquare(),\n",
    "        Resize(n_px, interpolation=Image.Resampling.BICUBIC),\n",
    "        CenterCrop(n_px),\n",
    "        # MinMaxResize(*n_px),\n",
    "        lambda image: image.convert(\"RGB\"),\n",
    "        ToTensor(),\n",
    "        Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "    ])\n",
    "\n",
    "\n",
    "def augmentation_transform(image_size):\n",
    "    return Compose([\n",
    "        Resize(image_size, interpolation=Image.Resampling.BICUBIC),\n",
    "        RandomHorizontalFlip(),\n",
    "        RandomCrop(image_size, padding=int(image_size[0]*0.0625), padding_mode='reflect'),\n",
    "        lambda image: image.convert(\"RGB\"),\n",
    "        ToTensor(),\n",
    "        Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "        RandomErasing(),\n",
    "    ])\n",
    "\n",
    "\n",
    "transform = _transform(eval(\"(224,224)\")[0])\n",
    "transform2 = augmentation_transform(eval(\"(224,224)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/pgrimal/Documents/Projects/visual_language_representation/results/best_zs_vlbart/images/folder_50/512px-Joseph_Stalin-TIME-1930.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_48917/299166209.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/pgrimal/Documents/Projects/visual_language_representation/results/best_zs_vlbart/images/folder_50/512px-Joseph_Stalin-TIME-1930.jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/cvlp2/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3067\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3068\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3069\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/pgrimal/Documents/Projects/visual_language_representation/results/best_zs_vlbart/images/folder_50/512px-Joseph_Stalin-TIME-1930.jpg'"
     ]
    }
   ],
   "source": [
    "path = \"/home/pgrimal/Documents/Projects/entity_image/data/Commons_wikimage/-%20Narcissus%20pseudonarcissus%2003%20-.jpg\"\n",
    "image = Image.open(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = transform(image)\n",
    "t2 = transform2(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "print(t.shape)\n",
    "print(t2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_boxes = 36 #self.args.n_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fake batch\n",
    "t = torch.unsqueeze(t, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pooling torch.Size([1, 1, 512])\n",
      "output classic torch.Size([1, 49, 2048])\n"
     ]
    }
   ],
   "source": [
    "_, pooling_output = output\n",
    "output_normal, _ = output\n",
    "\n",
    "print(\"pooling\", pooling_output.shape)\n",
    "print(\"output classic\", output_normal.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2048])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_normal[:,0,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 0. Expected size 25 but got size 24 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3515/1658407024.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_normal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (2B, L/2, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (B, L, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 0. Expected size 25 but got size 24 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "def downsample(inputs):\n",
    "        pool = torch.nn.AdaptiveMaxPool2d(6) # racine de 36\n",
    "        B, L, dim = inputs.shape\n",
    "\n",
    "        inputs = inputs.permute(0, 2, 1) # (2B, dim, L/2)\n",
    "        print(inputs.shape)\n",
    "        sqrt_L = int(L ** 0.5)\n",
    "        inputs = inputs.reshape(B, dim, sqrt_L, sqrt_L)\n",
    "        print(inputs.shape)\n",
    "        inputs = pool(inputs)\n",
    "        print(inputs.shape)\n",
    "        inputs = inputs.reshape(B, dim, -1)\n",
    "        print(inputs.shape)\n",
    "        inputs = inputs.permute(0, 2, 1)\n",
    "        print(inputs.shape)\n",
    "        return inputs\n",
    "\n",
    "inputs = output_normal\n",
    "inputs = torch.cat(torch.chunk(inputs, 2, 1), 0) # (2B, L/2, dim)\n",
    "inputs = downsample(inputs)\n",
    "inputs = torch.cat(torch.chunk(inputs, 2, 0), 1) # (B, L, dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cas pooling\n",
    "B, L, D = pooling_output.shape\n",
    "vis_pos = torch.zeros(B, L, 4, dtype=pooling_output.dtype)\n",
    "boxes = vis_pos\n",
    "print(boxes.shape)\n",
    "boxes = boxes[:, :pooling_output.shape[1]]\n",
    "boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 49, 4])\n"
     ]
    }
   ],
   "source": [
    "# cas non pooling\n",
    "B, L, D = output_normal.shape\n",
    "vis_pos = torch.zeros(B, L, 4, dtype=output_normal.dtype)\n",
    "boxes = vis_pos\n",
    "print(boxes.shape)\n",
    "# on coonsidre dans ce cas qu'on utilise 49 images ?\n",
    "# multiplier les features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 18, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boxes = torch.cat(torch.chunk(boxes[:,:36,:], 2, 1), 0)\n",
    "boxes = boxes[:, :output_normal.shape[1]//2]\n",
    "boxes = torch.cat(torch.chunk(boxes, 2, 0), 1)\n",
    "print(boxes.shape)\n",
    "36**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "choix des dimensions en entr en utilisant commentaire (mme si on ne veut vraiment se comparer car pas mme tche)\n",
    "\n",
    "Par exemple pooling pas forcmenet intressant pour nous de le faire\n",
    "on peut peut tre mieux comprendre l'image mme si on risque de s'y perdre\n",
    "\n",
    "pour une faire comparaison vaut peut tre mieux utiliser pooling (mme dimension pour clip)\n",
    "\n",
    "```\n",
    "Input images are resized to 224  224\n",
    "for the memory efficiency. We extract the 7  7 grid fea-\n",
    "tures produced by the last convolutional layer, and then ap-\n",
    "ply adaptive maximum-pooling over the features for down-\n",
    "sampling then to 6  6 for a fair comparison to [7].\n",
    "```\n",
    "\n",
    "Dans leur cas n'utilise pas la dernire couche\n",
    "\n",
    "Mme si on pourrait nous essayer de l'utiliser car peut tre suffisant\n",
    "Peut tre essayer les deux avec et sans pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_area(pos):\n",
    "    height = pos[:, :, 3] - pos[:, :, 2]\n",
    "    width = pos[:, :, 1] - pos[:, :, 0]\n",
    "    area = height * width\n",
    "    return area\n",
    "\n",
    "get_area(boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VLT5 with only image or only text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "import os\n",
    "from cvlep.VLT5.param import Config\n",
    "\n",
    "if \"RANK\" in os.environ and \"WORLD_SIZE\" in os.environ:  # torchrun launch\n",
    "    rank = int(os.environ[\"RANK\"])\n",
    "    local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "    world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "elif int(os.environ.get('SLURM_NPROCS', 1)) > 1:  # slurm launch\n",
    "    rank = int(os.environ[\"SLURM_PROCID\"])\n",
    "    local_rank = int(os.environ[\"SLURM_LOCALID\"])\n",
    "    world_size = int(os.environ[\"SLURM_NPROCS\"])\n",
    "else:  # single gpu & process launch\n",
    "    rank = 0\n",
    "    local_rank = 0\n",
    "    world_size = 0\n",
    "\n",
    "config_encoder_question = \"experiments/config_vladapter/local/encoder_simple.json\"\n",
    "config_encoder_passage = \"experiments/config_vladapter/local/encoder_simple.json\"\n",
    "config_model = \"experiments/config_vladapter/local/config_model.json\"\n",
    "config_training = \"experiments/config_vladapter/local/training_simple_adapter.json\"\n",
    "\n",
    "# Training config\n",
    "config_training = Config.load_json(config_training)\n",
    "config_training.world_size = world_size\n",
    "config_training.rank = rank\n",
    "config_training.local_rank = local_rank\n",
    "if world_size > 1:\n",
    "    config_training.distributed = True\n",
    "    config_training.multiGPU = True\n",
    "else:\n",
    "    config_training.distributed = False\n",
    "    config_training.multiGPU = False\n",
    "\n",
    "config_encoder_question = Config.load_json(config_encoder_question)\n",
    "config_encoder_passage = Config.load_json(config_encoder_passage)\n",
    "config_model = Config.load_json(config_model)\n",
    "\n",
    "from cvlep.trainer_base_vladapter import Trainer\n",
    "\n",
    "trainer = Trainer(config_encoder_question,config_encoder_passage, config_model, config_training, train=False, local=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from datasets import load_from_disk, disable_caching\n",
    "disable_caching()\n",
    "path_dataset = \"data_model/dataset/small_dataset\"\n",
    "dataset = load_from_disk(path_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 11])\n",
      "torch.Size([1, 36, 2048])\n"
     ]
    }
   ],
   "source": [
    "# prepare inputs\n",
    "import torch\n",
    "from cvlep.utils import device\n",
    "index = 1\n",
    "item = dataset[1]\n",
    "key_vision_features = \"vlt5_features\"\n",
    "key_boxes = \"vlt5_normalized_boxes\"\n",
    "key_text=\"input\"\n",
    "tokenizer = trainer.tokenizer_question\n",
    "\n",
    "vision_features = torch.Tensor(item[key_vision_features]).to(device)\n",
    "boxes = torch.Tensor(item[key_boxes]).to(device)\n",
    "vision_features = torch.squeeze(vision_features, dim=1)\n",
    "vis_inputs = (vision_features,boxes)\n",
    "boxes = torch.squeeze(boxes, dim=2)\n",
    "input_ids = tokenizer(\n",
    "    item[key_text], return_tensors='pt', padding=True, truncation=True)\n",
    "return_pooled_output=True,\n",
    "pool_strategy=\"avg\"\n",
    "print(input_ids.input_ids.size())\n",
    "print(vision_features.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = {}\n",
    "batch.update(input_ids = input_ids.input_ids.to(device),\n",
    "    attention_mask = input_ids.attention_mask.to(device),\n",
    "    vis_inputs = vis_inputs,\n",
    "    pool_strategy = pool_strategy,\n",
    "    return_pooled_output = return_pooled_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.encoder_question.eval()\n",
    "output = trainer.encoder_question(\n",
    "        input_ids=batch['input_ids'],\n",
    "        attention_mask=batch['attention_mask'],\n",
    "\n",
    "        vis_inputs=None,\n",
    "        vis_attention_mask=None,\n",
    "        task ='IR',\n",
    "        return_pooled_output=True,\n",
    "        pool_strategy=\"avg\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.1837, -0.1131, -0.1383,  ...,  0.1381, -0.0979,  0.4198],\n",
       "         [-0.3265, -0.2219, -0.3252,  ..., -0.1107, -0.0176,  0.1044],\n",
       "         [-0.3971, -0.1212, -0.3707,  ..., -0.1000,  0.0895,  0.0319],\n",
       "         ...,\n",
       "         [-0.1491, -0.0147, -0.0670,  ..., -0.2671,  0.2634,  0.3088],\n",
       "         [ 0.3494,  0.0269, -0.0432,  ..., -0.2269, -0.1812, -0.1007],\n",
       "         [ 0.0186,  0.0036,  0.0015,  ..., -0.0018, -0.0079,  0.0117]]],\n",
       "       grad_fn=<MulBackward0>), pooler_output=tensor([[-4.4331e-02,  3.1851e-02, -2.3499e-02,  5.1483e-02, -5.6845e-02,\n",
       "         -2.9331e-02, -3.9744e-02, -5.5451e-02,  1.0618e-02,  6.0566e-02,\n",
       "          4.1690e-02,  5.0878e-02, -3.9815e-02, -9.9753e-03, -1.2090e-01,\n",
       "         -4.8758e-02,  1.6853e-02, -2.9192e-02,  6.2281e-02,  4.9744e-02,\n",
       "         -4.4918e-02,  7.3101e-02,  1.5778e-02,  6.3280e-02, -2.0026e-02,\n",
       "          1.7612e-02,  1.8098e-02,  8.5740e-02, -1.0331e-02,  6.4627e-02,\n",
       "         -1.3997e-02, -9.7808e-03, -2.8626e-02, -1.4850e-02,  4.0261e-02,\n",
       "         -2.0786e-02, -3.6672e-02, -3.0913e-02, -4.1168e-02, -3.3456e-02,\n",
       "         -6.3082e-02,  5.7390e-02, -2.0961e-02,  1.9174e-02, -3.5786e-02,\n",
       "         -2.1434e-02,  2.7591e-02,  2.4352e-02,  9.4332e-03, -3.0063e-02,\n",
       "          8.6216e-02, -4.4252e-02, -4.0918e-02, -3.3864e-02,  1.3878e-03,\n",
       "          7.3108e-02,  1.5460e-02,  5.0594e-03, -1.0930e-03, -7.0086e-03,\n",
       "         -9.3910e-02,  3.4259e-02,  5.1808e-02,  1.3559e-02, -3.8858e-03,\n",
       "         -3.5755e-02,  4.3395e-03, -7.5503e-03, -1.7557e-02, -2.4629e-02,\n",
       "          6.6472e-02,  6.0270e-04,  2.2084e-02, -5.7799e-02,  5.7566e-03,\n",
       "          3.5775e-02,  2.5164e-02,  3.5898e-02,  1.1729e-02,  3.3399e-02,\n",
       "          4.2945e-02,  7.3262e-04, -3.2527e-02, -3.2606e-03, -1.9196e-02,\n",
       "          4.6093e-03,  2.5535e-03, -1.3255e-02, -6.3775e-02, -5.0779e-02,\n",
       "          3.3931e-02, -1.2245e-02,  3.1521e-02, -3.3818e-02,  4.6893e-03,\n",
       "          3.3750e-02,  7.2579e-03,  1.5880e-02,  9.6964e-03,  2.8649e-02,\n",
       "         -1.6387e-02, -8.3244e-02,  9.1472e-03, -3.4311e-02, -3.9517e-03,\n",
       "         -4.0684e-02, -1.7856e-02,  9.1555e-04,  1.4070e-02,  3.0052e-02,\n",
       "          1.1959e-02, -5.9075e-02,  8.0215e-03, -2.9454e-02, -6.7154e-03,\n",
       "         -2.3341e-02, -8.0837e-02, -2.9458e-02,  1.4574e-02,  5.0480e-03,\n",
       "          9.4994e-04, -6.1730e-02, -5.2881e-02, -3.9426e-02,  6.2093e-02,\n",
       "         -3.6168e-03,  1.8752e-02,  1.5891e-02,  4.5734e-02, -1.0220e-01,\n",
       "         -2.6736e-02,  4.6529e-03,  3.5038e-02,  2.5432e-02, -1.0281e-02,\n",
       "          1.3460e-02, -3.2143e-02,  1.5402e-03,  7.0350e-03,  3.6178e-02,\n",
       "         -4.5477e-03, -5.6726e-02,  9.4547e-02,  1.8569e-02, -1.0001e-02,\n",
       "          6.2552e-02, -2.8467e-03,  9.2019e-03, -6.5827e-02, -1.3581e-03,\n",
       "         -9.4889e-03,  1.6634e-02, -2.3195e-02,  2.1203e-02,  1.4703e-02,\n",
       "          1.7385e-02, -2.0814e-02, -1.2439e-02, -1.4022e-02, -6.7105e-02,\n",
       "          8.9155e-03, -7.8283e-02, -1.6531e-02, -1.0871e-02, -3.6254e-02,\n",
       "         -2.0199e-02, -2.8594e-02,  2.2258e-02, -2.6179e-02,  3.3361e-02,\n",
       "         -6.5491e-03,  3.5611e-02,  9.0289e-02,  1.9956e-02,  5.5226e-03,\n",
       "          3.4218e-02,  8.2117e-03,  4.9466e-03,  1.0433e-02, -3.0538e-02,\n",
       "         -2.1788e-02,  3.3872e-02, -2.6858e-02,  4.3911e-02, -5.9896e-02,\n",
       "          5.5798e-02, -1.1666e-02,  4.6275e-02, -9.8048e-03,  1.5412e-02,\n",
       "         -8.8019e-02, -1.4507e-02, -8.3186e-02,  2.0750e-02, -2.3288e-03,\n",
       "          5.2886e-03,  4.2956e-02,  6.4183e-02,  3.6659e-02,  4.5566e-02,\n",
       "          7.1778e-02, -7.0036e-02,  5.9858e-02,  3.5161e-02,  3.4861e-02,\n",
       "         -5.2697e-03, -5.2219e-03,  6.8571e-02,  1.3351e-02, -8.2970e-02,\n",
       "          2.6044e-02, -1.7895e-02,  3.2081e-02,  3.5141e-02, -5.1066e-02,\n",
       "          1.0236e-02, -2.5067e-02,  4.7739e-02,  5.1184e-02,  3.5882e-03,\n",
       "          1.2288e-02,  8.0432e-03,  1.5318e-02,  1.1649e-02,  9.3171e-03,\n",
       "         -2.1453e-02,  2.3164e-02, -1.2787e-02,  1.9615e-02, -1.6195e-02,\n",
       "          2.1553e-02, -4.6924e-02,  3.3743e-02,  2.7415e-02, -1.3518e-02,\n",
       "          1.3107e-02, -1.3057e-02, -1.7936e-02,  7.4650e-02,  1.2949e-02,\n",
       "          3.8665e-02,  1.7105e-02, -5.5368e-02, -9.8627e-04, -5.9431e-02,\n",
       "          5.0636e-02,  1.6507e-02, -2.3525e-02,  2.7516e-02, -1.3379e-02,\n",
       "         -3.9328e-02, -1.9242e-02, -1.3200e-02, -2.6373e-02,  2.0040e-02,\n",
       "         -1.8208e-02, -1.5365e-02,  5.8757e-02, -5.0679e-03, -2.7537e-03,\n",
       "         -4.3550e-02, -1.2165e-02, -1.3002e-02, -1.7009e-02,  4.6953e-03,\n",
       "          3.9555e-02,  1.1238e-02,  1.3671e-02, -4.2908e-02,  2.9242e-02,\n",
       "         -1.0758e-02, -7.7952e-03,  3.8581e-02,  3.6291e-02, -1.7557e-02,\n",
       "         -1.9884e-03,  3.0292e-02,  8.9838e-03, -4.6944e-03, -1.7025e-02,\n",
       "         -4.7757e-03, -1.1214e-02,  1.8457e-02,  8.1988e-03,  8.1118e-03,\n",
       "          3.0981e-02, -1.5520e-02,  5.7929e-03, -3.7901e-02,  1.6825e-02,\n",
       "          9.0826e-03, -7.6974e-05, -4.2497e-04, -3.8452e-02, -1.0296e-02,\n",
       "         -5.2980e-02,  5.2680e-02,  1.0240e-02,  3.4404e-02, -2.0925e-02,\n",
       "          7.9018e-02, -2.2525e-02, -2.9056e-03, -3.4664e-02, -4.6669e-02,\n",
       "          1.7917e-02, -1.0906e-02,  1.3056e-02, -8.8411e-02, -6.3400e-02,\n",
       "          1.2827e-02,  2.7930e-02, -1.3723e-02, -8.1399e-03,  2.9099e-03,\n",
       "          2.5328e-02,  6.7677e-02, -7.4037e-03,  2.4293e-02, -1.0899e-02,\n",
       "         -4.7496e-02,  1.2723e-02,  3.0504e-02,  8.4491e-03,  2.4971e-02,\n",
       "          3.3615e-03, -2.1901e-02, -2.4946e-02, -6.9042e-02, -4.9733e-02,\n",
       "         -3.6842e-02,  2.4720e-02, -1.5439e-02,  2.7464e-02, -1.6365e-02,\n",
       "          6.2581e-02,  4.6035e-03,  2.1447e-02, -6.1799e-03,  2.8800e-02,\n",
       "         -2.7424e-02,  6.8818e-02, -1.5916e-03,  1.5491e-02, -1.9256e-02,\n",
       "          6.0741e-02, -2.4949e-02, -1.1145e-02,  5.4273e-02,  8.0370e-02,\n",
       "         -6.1237e-02,  4.7584e-02, -3.5218e-02, -6.2947e-03,  1.2844e-02,\n",
       "          1.1032e-02, -5.1791e-02,  4.5452e-04,  2.8014e-02,  3.2927e-02,\n",
       "         -2.9633e-02, -2.5689e-03,  2.1676e-02, -7.2707e-02,  1.9868e-02,\n",
       "         -1.7772e-02,  1.3589e-03,  3.7502e-02, -5.5546e-02,  7.2739e-03,\n",
       "          3.1893e-02, -3.1780e-02,  1.2546e-02,  2.1276e-02,  5.3960e-02,\n",
       "          4.1314e-02, -1.4915e-02,  1.1946e-02,  5.4576e-02,  2.5593e-02,\n",
       "          1.5120e-02,  9.8482e-03,  1.3213e-02, -1.8666e-02,  5.6190e-04,\n",
       "          7.2535e-03,  1.5636e-02,  2.0441e-02,  8.4670e-03, -1.7707e-03,\n",
       "         -5.0336e-02, -1.3052e-02,  3.3675e-02, -7.7873e-03,  5.6646e-03,\n",
       "          1.9813e-02, -5.4337e-02, -1.3326e-02, -1.3935e-02, -4.8323e-02,\n",
       "          3.7751e-02,  4.7692e-02,  5.1962e-03,  1.6805e-02,  3.1796e-02,\n",
       "          2.7701e-02,  3.1835e-02,  6.2300e-02, -4.1181e-02, -3.0625e-02,\n",
       "         -2.0789e-03,  4.0843e-03, -1.1299e-02,  1.0508e-02, -6.9478e-03,\n",
       "          2.2239e-02,  8.0809e-02, -4.1889e-02, -5.3350e-03,  6.4612e-03,\n",
       "         -3.6892e-03,  1.6262e-02, -2.8146e-02,  3.0653e-02,  2.9627e-02,\n",
       "         -6.5621e-03,  3.8218e-03, -5.4406e-02,  6.7478e-03,  4.7930e-02,\n",
       "         -8.7105e-02, -4.6570e-02,  3.4259e-02,  2.8225e-02,  6.8807e-03,\n",
       "         -1.0962e-02,  4.9500e-03,  5.1502e-02,  2.1678e-02, -2.3708e-02,\n",
       "         -1.9021e-02,  6.3057e-02,  2.9893e-02,  3.9595e-02,  3.0650e-02,\n",
       "          1.3160e-02, -1.6306e-02, -1.7856e-02,  3.4331e-02,  8.5179e-03,\n",
       "          1.7511e-02,  2.8440e-02,  2.0153e-02,  3.1485e-02,  8.5602e-02,\n",
       "          5.1442e-03,  7.2870e-04,  1.0729e-02, -1.1020e-02,  5.9523e-02,\n",
       "         -5.2818e-02,  4.9904e-02,  1.3660e-03, -4.7596e-02,  1.0659e-02,\n",
       "          7.5923e-02, -2.3261e-02,  6.0837e-03, -1.5451e-02, -4.3656e-02,\n",
       "          3.0609e-02,  1.6713e-02, -2.8167e-04, -3.0489e-02,  3.3124e-02,\n",
       "         -1.0763e-02,  2.6501e-02, -2.7236e-03,  4.5503e-02, -3.4354e-02,\n",
       "          2.9787e-02, -9.0864e-03, -2.3572e-02,  1.7019e-02,  6.5945e-03,\n",
       "          1.4673e-02, -7.1165e-03, -1.3766e-02, -1.2652e-02,  2.6568e-02,\n",
       "         -3.7704e-03, -4.4168e-03, -2.9849e-02, -1.5205e-02,  2.7768e-02,\n",
       "         -2.1030e-02, -4.5133e-02,  5.8733e-02, -1.1738e-02, -8.1420e-02,\n",
       "          4.8672e-02,  3.5135e-02,  2.8548e-03, -3.7481e-02, -2.4499e-04,\n",
       "         -1.5648e-02,  2.3567e-02,  6.7523e-03, -3.1738e-02,  2.9705e-02,\n",
       "         -2.8105e-02, -2.5096e-02, -7.7583e-02, -3.1859e-02,  6.9577e-03,\n",
       "          1.8035e-02,  6.0804e-02,  1.5328e-02,  2.9556e-02, -8.3125e-03,\n",
       "         -1.5660e-03, -7.6016e-03,  5.0016e-02, -5.3066e-04, -1.0123e-01,\n",
       "         -3.4261e-02, -1.6764e-02,  8.0784e-02, -6.0590e-02,  4.3703e-02,\n",
       "         -4.8163e-02,  2.9630e-02, -9.8194e-03, -3.2435e-02,  2.8069e-02,\n",
       "         -8.1740e-02, -3.7593e-02, -3.5746e-02,  4.3697e-02,  5.1802e-03,\n",
       "          2.6280e-02, -4.2894e-02,  6.5818e-02, -9.8956e-04,  7.6029e-02,\n",
       "         -3.9190e-02, -1.0197e-02, -2.7565e-02,  5.2982e-02, -3.3069e-02,\n",
       "          1.5596e-02, -3.6705e-02, -7.2769e-03, -2.1634e-02, -4.3595e-02,\n",
       "         -3.0160e-02,  3.3958e-02,  8.4055e-04, -1.5470e-02,  3.7538e-02,\n",
       "         -1.9612e-03,  3.1464e-03,  1.1847e-02, -3.3780e-03, -5.5846e-02,\n",
       "         -1.6643e-02,  3.4564e-02, -3.2070e-02,  3.6572e-03,  8.6988e-02,\n",
       "         -1.9257e-02,  2.7852e-02, -8.5046e-02,  6.6202e-03,  8.0709e-02,\n",
       "         -1.1762e-02,  4.4363e-03, -1.7274e-02, -3.1799e-02,  2.0210e-02,\n",
       "         -1.6225e-02,  5.5380e-02, -6.8267e-02, -4.7000e-02, -2.6559e-02,\n",
       "         -4.9076e-02,  3.2646e-02,  3.3983e-02,  7.3794e-03,  2.7200e-02,\n",
       "          1.1192e-02,  3.9850e-02,  1.2022e-02,  1.2831e-02, -1.5460e-02,\n",
       "          3.6529e-02,  8.5333e-02,  6.1242e-02,  2.3990e-02, -3.3810e-02,\n",
       "          5.0014e-03,  4.6097e-02,  4.8924e-02,  1.4333e-02, -3.3710e-02,\n",
       "         -7.5620e-03, -1.6749e-02,  3.9799e-03,  4.6868e-03, -4.8471e-02,\n",
       "          3.4514e-03, -2.7085e-02,  1.4288e-02,  8.0158e-03, -2.5361e-02,\n",
       "         -3.0592e-03,  1.1375e-02,  3.1387e-02, -3.6894e-02,  4.4320e-02,\n",
       "         -3.0957e-02, -2.8517e-02,  8.9328e-03, -2.1709e-02, -1.1764e-03,\n",
       "         -4.3868e-02, -3.3115e-02, -4.8253e-02, -5.8176e-02, -2.0509e-02,\n",
       "          7.3306e-03, -4.0062e-02,  3.3768e-02, -7.0797e-02,  1.2003e-02,\n",
       "         -2.2423e-02,  1.2494e-02, -6.6016e-02,  6.6627e-02, -4.2349e-02,\n",
       "         -1.7004e-02, -6.1677e-03,  3.8072e-03, -2.9734e-02, -2.2472e-03,\n",
       "         -2.3521e-02,  2.2438e-02, -1.8115e-02,  3.0222e-02,  4.4631e-02,\n",
       "          2.4358e-02, -6.7154e-02, -4.6808e-03,  1.7082e-02,  1.4598e-02,\n",
       "         -1.4880e-02,  8.0477e-02,  4.4577e-02, -1.6428e-02,  6.5086e-02,\n",
       "          2.4181e-02,  2.8383e-02, -1.1552e-02, -2.9540e-02, -4.3926e-02,\n",
       "          2.4107e-02,  4.7771e-02, -2.7550e-02,  2.8250e-04,  5.6373e-02,\n",
       "         -3.8275e-02, -3.1199e-02,  6.9286e-03, -6.5421e-03,  5.7701e-03,\n",
       "         -8.5229e-02,  1.1048e-02, -1.3073e-02, -2.8896e-02,  1.9903e-02,\n",
       "          2.5997e-02, -1.2264e-01,  2.0508e-02, -5.6526e-02, -1.2665e-02,\n",
       "          4.7749e-03, -3.5467e-04, -2.1540e-02, -5.0175e-02,  2.5137e-02,\n",
       "          8.4540e-03, -3.4946e-03, -4.6044e-02,  2.4937e-02, -2.6996e-02,\n",
       "         -1.7470e-02,  7.7163e-03,  1.5241e-02,  3.9761e-02, -1.1870e-02,\n",
       "          1.3548e-02,  1.3900e-03,  3.1472e-02, -2.4659e-03,  3.1534e-02,\n",
       "          3.1091e-02,  2.0830e-02, -3.8863e-02,  2.1007e-02, -2.5003e-02,\n",
       "         -1.5308e-02,  2.0855e-03,  2.9248e-02, -2.9544e-02, -6.1406e-02,\n",
       "         -3.4738e-02,  2.3149e-02, -2.8826e-02, -8.5622e-02, -4.8271e-02,\n",
       "         -4.3309e-02, -4.0588e-03, -2.6088e-02, -1.8146e-03,  4.1034e-02,\n",
       "          2.5210e-02,  7.5292e-02, -2.0984e-02,  6.4785e-03, -1.0422e-01,\n",
       "         -1.7558e-02,  2.9722e-02,  3.0791e-02, -9.3521e-03,  2.6240e-02,\n",
       "         -4.1017e-02,  4.9983e-02,  2.6485e-02, -4.6125e-02, -2.6796e-02,\n",
       "          4.5685e-02, -3.7525e-02, -1.2861e-02, -1.3869e-02,  8.8720e-03,\n",
       "         -5.0283e-02, -4.1417e-02,  5.9685e-02, -1.8461e-02, -1.0949e-02,\n",
       "          4.1777e-02, -2.6720e-02, -3.2131e-03,  3.0460e-02,  4.6009e-02,\n",
       "          1.6248e-02, -5.5532e-02,  4.5769e-02,  6.1631e-02,  1.7373e-02,\n",
       "          5.5053e-02, -2.9517e-02, -3.0814e-02, -3.8354e-02,  6.6268e-03,\n",
       "          6.1441e-02, -3.0376e-02,  5.9099e-02]], grad_fn=<DivBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.1837, -0.1131, -0.1383,  ...,  0.1381, -0.0979,  0.4198],\n",
       "         [-0.3265, -0.2219, -0.3252,  ..., -0.1107, -0.0176,  0.1044],\n",
       "         [-0.3971, -0.1212, -0.3707,  ..., -0.1000,  0.0895,  0.0319],\n",
       "         ...,\n",
       "         [-0.1491, -0.0147, -0.0670,  ..., -0.2671,  0.2634,  0.3088],\n",
       "         [ 0.3494,  0.0269, -0.0432,  ..., -0.2269, -0.1812, -0.1007],\n",
       "         [ 0.0186,  0.0036,  0.0015,  ..., -0.0018, -0.0079,  0.0117]]],\n",
       "       grad_fn=<MulBackward0>), pooler_output=tensor([[ 5.0939e-03, -4.5583e-02,  1.3953e-02,  5.5035e-02, -1.9640e-02,\n",
       "          6.8446e-02, -3.5597e-02, -1.3215e-02, -1.2456e-02, -1.4589e-02,\n",
       "          2.3402e-02,  9.6273e-04, -4.6322e-02, -4.0569e-02, -5.3397e-02,\n",
       "          4.8928e-02, -1.5481e-02, -4.5747e-02,  1.1134e-02, -1.8569e-02,\n",
       "          1.0229e-02, -1.4514e-03,  4.1421e-02, -4.0335e-03, -6.7227e-02,\n",
       "         -2.5481e-03, -3.0742e-02,  4.3212e-02, -2.0335e-02,  2.8729e-02,\n",
       "          3.8856e-02,  1.9383e-02, -2.9843e-02,  3.5616e-02, -2.7034e-02,\n",
       "         -4.5616e-02, -5.3162e-02, -2.5688e-02,  6.6500e-02,  3.4954e-02,\n",
       "         -2.5509e-02,  1.7330e-03, -4.0206e-03,  6.2396e-02, -8.4764e-02,\n",
       "         -4.2136e-02,  4.4064e-03, -1.5279e-02, -1.8980e-02,  4.6980e-02,\n",
       "          6.1846e-02, -6.2567e-02,  4.9719e-02, -5.4633e-02, -6.7629e-02,\n",
       "         -4.5151e-02,  7.5535e-02,  1.0145e-03,  1.1074e-02, -3.0724e-02,\n",
       "         -5.1562e-02,  1.2251e-02, -9.3240e-02,  1.0791e-02,  4.2273e-03,\n",
       "          2.0707e-02,  2.1738e-02, -2.1055e-02,  1.1163e-02,  1.7349e-02,\n",
       "         -2.0211e-02, -2.2436e-02,  1.8336e-02,  1.1398e-02,  2.3253e-02,\n",
       "         -3.9458e-02, -4.9708e-02, -7.9575e-03, -6.1620e-03, -1.5980e-03,\n",
       "          2.9303e-02, -6.2893e-02, -3.6213e-02,  1.5812e-02, -5.4854e-03,\n",
       "         -1.0273e-01,  8.4818e-03,  3.8098e-02, -3.5438e-02, -4.0816e-02,\n",
       "         -2.0797e-02,  1.0451e-02,  2.2653e-02, -3.7309e-02, -1.8021e-02,\n",
       "          4.2885e-02,  1.3748e-03, -2.8808e-02,  3.4298e-02, -5.9799e-02,\n",
       "          3.6273e-02,  3.3945e-02, -2.1322e-02,  7.7582e-02, -9.6788e-04,\n",
       "         -8.1560e-03,  3.1177e-02, -2.0208e-02,  4.0569e-02,  5.9051e-03,\n",
       "          3.0676e-02,  2.8018e-02,  4.8276e-02,  3.4911e-02, -3.4375e-02,\n",
       "          1.3329e-02,  4.7546e-02,  9.4013e-02,  2.3673e-02, -3.0293e-02,\n",
       "          6.0730e-02, -4.2435e-02,  9.9812e-04, -1.2494e-03, -2.1124e-02,\n",
       "         -7.4901e-02, -3.7482e-03,  1.4884e-02,  7.8277e-02, -1.9506e-02,\n",
       "         -3.9096e-02,  4.7456e-02, -3.4355e-02,  4.0358e-02,  4.0243e-02,\n",
       "         -1.5226e-02, -2.8473e-02, -2.6120e-02,  1.4820e-02, -7.2281e-03,\n",
       "         -2.4545e-02,  6.3931e-03, -1.6017e-02,  2.1647e-02, -4.6787e-02,\n",
       "         -3.6188e-02,  8.1998e-02, -5.6354e-02,  1.6734e-02,  5.2235e-02,\n",
       "          3.7093e-03,  2.1598e-02,  1.7179e-02, -3.3064e-02,  3.8042e-02,\n",
       "          4.0792e-02, -2.1317e-02, -2.8837e-03,  1.6107e-02,  4.8447e-02,\n",
       "          5.9682e-02,  6.3067e-02, -6.7972e-02,  2.4496e-02, -3.2616e-02,\n",
       "         -2.7718e-02, -5.4240e-02,  3.8007e-02, -1.9821e-02, -3.1163e-02,\n",
       "          6.2181e-02, -2.8856e-02,  7.6915e-04,  5.3873e-02,  9.5894e-03,\n",
       "          5.6859e-03,  7.8656e-03,  2.2342e-02,  7.3223e-03, -8.4865e-02,\n",
       "         -2.4371e-02, -6.7792e-03, -1.9615e-02, -9.6012e-02,  1.5523e-02,\n",
       "         -4.5398e-03,  6.2766e-02,  7.2555e-03,  5.4074e-02, -2.1508e-02,\n",
       "          4.4500e-02, -3.8155e-02, -3.0146e-02,  2.1658e-02, -2.1212e-02,\n",
       "         -3.3259e-02,  8.0455e-02,  3.1789e-02,  6.7300e-02,  2.3545e-02,\n",
       "         -4.3653e-02,  2.7173e-02, -1.5419e-02, -6.3687e-03, -1.2352e-02,\n",
       "         -4.2858e-02,  1.7372e-02, -3.6781e-02,  3.2176e-02,  1.8107e-04,\n",
       "         -2.2155e-02, -2.8835e-03,  2.3742e-02, -4.2611e-02, -9.8698e-03,\n",
       "          6.3795e-02, -2.4958e-02, -1.8015e-02, -4.4131e-02, -5.8909e-02,\n",
       "          2.4365e-03, -7.2103e-03, -1.6208e-02,  7.7465e-02,  1.8431e-02,\n",
       "          3.2246e-03, -3.3414e-02, -3.2247e-02,  4.4313e-02, -1.0124e-02,\n",
       "          2.8331e-02,  2.5179e-02,  4.6708e-02, -5.9231e-02, -1.9153e-02,\n",
       "          7.0412e-02,  9.2188e-03, -8.2518e-02, -4.9324e-04, -3.1591e-02,\n",
       "          4.0326e-03,  1.5329e-03, -2.9582e-03,  1.2239e-02,  3.5595e-02,\n",
       "         -2.1590e-02,  9.7879e-03, -6.1277e-02,  1.6987e-02,  8.4817e-03,\n",
       "         -1.3543e-02, -3.3121e-02, -3.7949e-02,  6.5426e-02,  1.3983e-03,\n",
       "          9.0290e-03,  9.8594e-03, -2.1316e-02, -2.7804e-02, -4.7069e-02,\n",
       "          8.7481e-03, -4.7445e-02,  5.5031e-02, -6.8614e-03, -3.6900e-02,\n",
       "         -1.6478e-02,  1.1831e-02, -1.8077e-02,  1.6524e-02,  2.2514e-02,\n",
       "          6.5499e-04, -2.2313e-04, -4.7386e-02,  2.4932e-02, -8.8782e-03,\n",
       "         -2.6834e-02, -5.2500e-02, -2.6624e-02,  9.3940e-03,  1.8999e-02,\n",
       "          1.8069e-02, -3.4295e-02,  4.0309e-02,  1.8985e-02, -2.1017e-02,\n",
       "          2.9269e-02, -6.7589e-04,  1.9967e-02, -2.8800e-02, -2.1846e-02,\n",
       "         -7.4177e-02, -4.8735e-02,  7.2922e-03, -3.5806e-05, -4.7924e-04,\n",
       "          3.9392e-02,  6.7816e-02, -5.4393e-02, -5.3894e-02,  3.5580e-02,\n",
       "         -1.9921e-02, -3.4224e-04,  5.3969e-02,  5.2929e-03,  1.1291e-03,\n",
       "          2.0130e-02, -5.4116e-02,  1.1784e-02, -7.4100e-03,  7.0579e-02,\n",
       "          2.5699e-02,  4.3101e-02, -6.2504e-02,  5.9470e-02,  2.2538e-02,\n",
       "         -4.7182e-02, -9.2501e-03,  1.5346e-02, -4.5285e-02,  5.2344e-02,\n",
       "          6.5523e-02,  1.2523e-02, -7.1004e-02,  1.3132e-02, -1.9653e-02,\n",
       "         -1.4451e-02,  4.5603e-03, -5.8461e-02, -1.0224e-02, -3.9206e-02,\n",
       "         -3.0052e-03, -1.7861e-02, -5.1372e-02, -3.7342e-02, -1.9824e-03,\n",
       "          4.6363e-03, -1.8194e-02, -5.1974e-02, -1.3525e-02,  7.0228e-02,\n",
       "         -1.8271e-02,  1.6419e-02,  1.7043e-02, -4.7004e-03, -3.0191e-02,\n",
       "         -8.0384e-03, -4.9339e-03,  4.9965e-04, -2.3284e-02,  1.8016e-02,\n",
       "          6.9161e-03,  1.3404e-02,  2.4065e-02, -4.2363e-04,  8.2852e-02,\n",
       "         -6.7366e-03,  3.6638e-02,  5.4333e-02, -7.7717e-03, -1.0366e-02,\n",
       "          6.0918e-02, -7.4365e-03, -1.8862e-02,  1.3995e-02, -3.3295e-02,\n",
       "         -3.9516e-03, -2.1042e-02,  4.0590e-02,  2.5671e-02,  1.0935e-02,\n",
       "         -1.6168e-02,  4.3460e-02, -2.4825e-02, -1.7689e-02,  3.9126e-03,\n",
       "         -3.2972e-02,  4.4488e-02,  4.3551e-02,  2.1280e-02, -3.1680e-03,\n",
       "          1.3690e-02, -5.6018e-02, -4.1529e-03,  1.9037e-02,  2.4139e-02,\n",
       "          6.1907e-03, -1.6659e-02,  6.4349e-03,  7.9402e-03,  1.0794e-02,\n",
       "          3.3849e-02, -4.7750e-03, -3.0214e-02, -1.8476e-02,  8.0781e-03,\n",
       "          1.6609e-02, -1.9408e-02,  3.4816e-02,  2.9243e-02, -1.8023e-02,\n",
       "          4.9384e-02, -5.9689e-02, -7.2710e-03, -2.8957e-02, -7.9505e-02,\n",
       "         -2.2537e-02,  3.7681e-03,  2.4043e-02, -8.3655e-03,  3.0752e-02,\n",
       "         -4.3304e-02,  4.5200e-03, -3.3513e-02, -1.7616e-03,  5.9445e-02,\n",
       "          1.5178e-02, -2.6188e-02, -4.3018e-02,  2.6419e-03,  7.4387e-03,\n",
       "          3.0750e-02,  2.8355e-02,  5.1363e-02, -1.1658e-02,  3.5880e-02,\n",
       "          5.7697e-04,  3.4429e-04,  4.8652e-03, -4.5350e-02, -8.3687e-02,\n",
       "         -2.8832e-02, -6.2582e-03, -1.5199e-02, -4.6590e-02,  3.4470e-02,\n",
       "         -2.6279e-02,  1.3303e-01,  2.6517e-02, -2.8930e-02, -3.2893e-02,\n",
       "          2.0368e-02, -1.9519e-02, -4.3283e-02, -1.1396e-02, -3.0999e-02,\n",
       "         -3.2296e-02, -1.4943e-02,  4.7349e-03,  5.4684e-02, -5.9592e-02,\n",
       "          8.0812e-04, -3.8357e-02,  5.2729e-02,  9.1336e-03,  1.0088e-01,\n",
       "          2.6696e-02,  1.3546e-03, -2.4426e-02, -4.1119e-02, -2.7848e-02,\n",
       "         -1.0533e-01, -6.5466e-02,  3.0170e-02, -4.4898e-03,  4.0552e-02,\n",
       "          3.8076e-02,  2.2963e-02, -5.8215e-02, -5.5410e-02,  3.1722e-02,\n",
       "          1.3995e-02,  2.3237e-03, -5.3128e-02,  8.9295e-02,  3.7524e-02,\n",
       "          2.6254e-02,  4.2942e-02,  3.4999e-02, -1.1395e-03, -9.2591e-03,\n",
       "         -1.5724e-02,  1.5303e-02,  7.0516e-03,  4.2627e-02,  1.4009e-02,\n",
       "          4.7370e-03,  7.2172e-03, -1.3129e-02,  4.4843e-02, -2.3381e-02,\n",
       "         -1.4451e-02, -3.4292e-02, -1.4192e-02,  2.9437e-03,  7.9207e-02,\n",
       "         -7.0840e-03,  2.1863e-02, -3.3702e-02, -3.0925e-02, -9.8073e-02,\n",
       "          5.3032e-02, -5.5683e-02, -3.2140e-02, -5.1123e-02, -2.5057e-02,\n",
       "          6.0340e-02,  8.0456e-03,  2.0380e-02,  3.1439e-02, -4.0972e-02,\n",
       "          2.5975e-02,  7.1088e-02, -2.1239e-02, -9.2475e-03,  5.3386e-02,\n",
       "          3.1608e-02,  1.6481e-02,  7.7678e-02, -2.0628e-02,  1.1470e-02,\n",
       "         -9.8909e-03,  5.6877e-03, -3.5550e-02, -1.9208e-02, -2.8685e-02,\n",
       "          5.5144e-02,  6.4579e-02,  2.4113e-02, -5.7467e-03,  3.8297e-02,\n",
       "          1.6830e-02, -1.0280e-02,  1.4440e-02,  2.2393e-02, -2.2927e-02,\n",
       "         -2.5990e-02,  1.0097e-02,  4.8705e-02, -1.2279e-02,  2.0969e-02,\n",
       "         -4.2579e-02,  1.6807e-02, -4.0873e-02,  2.0387e-02, -4.1855e-02,\n",
       "          3.1392e-02,  3.7895e-03, -1.4319e-02,  1.4614e-02, -4.2600e-03,\n",
       "         -2.5783e-02, -4.5793e-03,  6.1648e-03, -2.1314e-02, -4.0134e-02,\n",
       "          1.2729e-03,  3.7761e-02,  2.6299e-02, -3.5523e-02,  6.4755e-02,\n",
       "          3.3645e-03, -7.3980e-02, -2.2674e-02,  4.5453e-05,  9.4799e-02,\n",
       "          4.0701e-03,  2.3688e-02,  9.3756e-03, -3.8055e-02, -2.4561e-02,\n",
       "          6.1286e-02, -1.9958e-02,  3.0111e-04, -7.8568e-03, -5.5675e-02,\n",
       "         -4.2543e-02,  6.5024e-02, -1.0477e-02,  2.0097e-02, -2.8361e-02,\n",
       "          5.6513e-02,  1.1607e-03, -1.9099e-03, -1.5082e-02, -1.0203e-02,\n",
       "          1.9061e-02, -4.9228e-03, -6.5867e-02, -7.0903e-02, -1.0973e-02,\n",
       "          5.3494e-03,  5.2273e-02, -1.9221e-02, -7.0209e-02,  1.5107e-02,\n",
       "          3.2416e-03,  2.4516e-03, -4.5172e-02, -1.8358e-02, -3.6200e-02,\n",
       "          6.9973e-03, -3.6313e-02,  3.4279e-02, -1.5230e-03,  3.9798e-02,\n",
       "          1.1140e-03, -1.2257e-02,  2.9846e-02, -3.5628e-02,  4.0143e-02,\n",
       "          3.3567e-02,  2.3940e-03,  5.7617e-02, -1.2928e-02, -8.1572e-03,\n",
       "          2.1537e-03,  4.1906e-02,  4.2893e-02, -4.9203e-02, -1.6900e-03,\n",
       "          9.9207e-03,  4.7942e-02,  1.5985e-02, -1.4104e-02,  2.4546e-02,\n",
       "          4.0882e-02,  5.4704e-03,  3.1065e-02,  8.1397e-03,  1.0714e-02,\n",
       "         -1.4267e-02, -4.6965e-02,  1.2263e-02,  1.1267e-02,  7.2509e-02,\n",
       "          4.5263e-02,  3.2202e-02, -4.8798e-02,  4.7433e-02,  2.3749e-02,\n",
       "          3.5713e-02,  9.8114e-03, -1.3456e-02,  3.3799e-02,  7.5945e-02,\n",
       "          2.7602e-02,  1.7587e-02, -5.0252e-02,  2.4241e-02, -2.3912e-02,\n",
       "          1.4175e-02,  2.3222e-02, -6.7448e-02, -1.5257e-03, -4.0664e-02,\n",
       "         -3.4887e-02,  3.0037e-02, -6.1663e-02, -2.6633e-02, -6.4297e-02,\n",
       "          4.3680e-02,  6.9938e-02,  1.7035e-02, -1.6378e-02,  1.9431e-02,\n",
       "          3.1520e-02,  1.4787e-02,  3.3164e-02,  3.0938e-02, -9.3032e-02,\n",
       "         -4.6590e-02,  1.1541e-02,  5.3346e-02,  8.0498e-03,  5.1754e-02,\n",
       "          1.5859e-02, -4.5683e-02, -3.1900e-02,  6.8180e-03,  4.7133e-02,\n",
       "          8.3091e-02,  6.9468e-03, -4.5481e-02,  2.2512e-02, -1.5298e-03,\n",
       "         -3.5800e-03,  8.1183e-02, -2.7030e-02,  1.9014e-02, -1.0762e-02,\n",
       "          4.8534e-02, -1.1589e-02, -1.7367e-02,  2.9736e-04,  3.5355e-02,\n",
       "         -1.3016e-02,  2.7453e-02,  3.2721e-02, -2.0817e-02, -3.7046e-02,\n",
       "         -2.4548e-02, -4.9365e-02, -1.9554e-02,  4.6594e-02, -1.7066e-02,\n",
       "         -7.5235e-04,  5.9492e-02,  2.2652e-02, -1.7092e-02,  2.9840e-02,\n",
       "         -3.4441e-03, -4.6202e-02,  3.7496e-02, -6.1629e-02, -3.4153e-02,\n",
       "         -3.8148e-03,  2.4618e-02,  3.8847e-02,  6.0685e-02, -5.0241e-02,\n",
       "          2.4162e-03, -1.6658e-02,  1.4706e-02,  1.3212e-02, -3.1510e-02,\n",
       "         -2.4339e-02, -3.3606e-02, -1.6145e-03, -3.9091e-02,  8.5486e-03,\n",
       "          2.0367e-02,  3.0001e-02,  4.2017e-03, -1.6764e-02,  2.2375e-02,\n",
       "         -2.9194e-02, -2.2742e-02,  1.5436e-02, -2.5467e-03,  1.5806e-02,\n",
       "         -4.6071e-02,  9.1573e-03, -8.3703e-03,  2.2869e-02, -1.7076e-02,\n",
       "          1.4960e-02, -3.7580e-02,  5.7238e-02, -3.2436e-03,  3.6126e-02,\n",
       "         -5.0676e-02, -6.9882e-03, -1.1755e-02,  1.5275e-02, -1.3661e-02,\n",
       "         -2.8068e-03,  3.6540e-02, -7.9128e-03,  5.0757e-02,  4.4390e-02,\n",
       "         -2.9734e-02,  2.6943e-02, -4.0238e-02, -2.5142e-02,  1.9910e-02,\n",
       "         -5.5098e-02, -1.0850e-03,  2.1165e-02]], grad_fn=<DivBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = trainer.encoder_question(\n",
    "        vis_inputs=batch['vis_inputs'],\n",
    "        vis_attention_mask=None,\n",
    "        task ='IR',\n",
    "        return_pooled_output=True,\n",
    "        pool_strategy=\"avg\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.0528, -0.3212, -0.1362,  ..., -0.1120, -0.3062,  0.3349],\n",
       "         [ 0.3147, -0.2520, -0.1157,  ..., -0.2526,  0.0031,  0.2288],\n",
       "         [ 0.3366, -0.0863, -0.0805,  ..., -0.1821,  0.0955,  0.1816],\n",
       "         ...,\n",
       "         [ 0.1156, -0.1263, -0.1870,  ..., -0.1023, -0.1788,  0.2908],\n",
       "         [ 0.2343,  0.0855, -0.1409,  ..., -0.1332,  0.0280,  0.3021],\n",
       "         [ 0.0130,  0.0181,  0.0114,  ...,  0.0195,  0.0004, -0.0061]]],\n",
       "       grad_fn=<MulBackward0>), pooler_output=tensor([[ 2.3356e-02, -1.7369e-02,  1.6582e-02,  4.1078e-02,  7.5358e-03,\n",
       "         -1.6688e-02,  8.0122e-03, -1.2653e-02,  1.6703e-02,  6.3174e-02,\n",
       "         -2.2366e-02,  5.4921e-02, -6.0474e-02, -1.8742e-02, -9.7393e-02,\n",
       "         -8.1920e-03, -5.3701e-02, -4.0670e-03,  2.6532e-02, -3.7992e-02,\n",
       "         -2.0254e-02, -1.6186e-03, -8.5224e-02,  3.8603e-02,  1.8296e-02,\n",
       "          4.5341e-02,  2.6054e-02, -7.7998e-03, -2.9173e-02, -6.1508e-02,\n",
       "          1.5844e-02,  9.1783e-02,  8.3698e-02, -1.7280e-02, -4.8390e-02,\n",
       "         -2.9203e-02, -2.7521e-02,  1.1253e-02, -5.5875e-02,  4.6841e-02,\n",
       "         -6.2160e-02,  6.3605e-02, -6.8771e-02, -8.2992e-02, -7.7455e-03,\n",
       "         -5.8959e-02,  2.4709e-02, -2.8933e-02, -1.6212e-02, -6.0981e-03,\n",
       "          2.2528e-02,  1.4615e-02, -2.2320e-02,  4.7592e-02, -3.7647e-02,\n",
       "         -2.0981e-02, -1.2808e-02, -3.6676e-02, -5.6107e-02,  4.9033e-02,\n",
       "         -7.1163e-02,  1.5599e-02,  3.1998e-02,  9.9795e-03,  4.3814e-02,\n",
       "          8.6382e-02,  6.6939e-02, -8.7275e-04, -5.3572e-02,  4.5057e-02,\n",
       "          1.0453e-03, -2.3952e-02, -4.1298e-02, -2.1723e-02,  3.0726e-02,\n",
       "          4.4626e-02, -4.5385e-02, -4.0310e-02, -7.9401e-03,  2.8619e-02,\n",
       "         -1.6434e-02, -1.9374e-02, -9.1667e-03, -4.5610e-03, -9.4861e-02,\n",
       "          4.1352e-03, -5.5142e-02, -4.0788e-02, -2.2531e-02, -4.6477e-02,\n",
       "          3.7775e-06,  2.8159e-02,  1.8924e-02,  1.0910e-02,  2.4339e-02,\n",
       "          4.4328e-02, -2.1150e-02,  2.2780e-02,  3.5670e-02,  5.0074e-02,\n",
       "          3.3337e-03, -2.2225e-02,  2.8739e-02, -4.9186e-02,  3.2354e-02,\n",
       "         -3.1406e-04,  2.7195e-04,  7.2100e-03,  7.6908e-02, -5.5689e-02,\n",
       "         -2.3917e-03, -4.0906e-02,  7.3673e-03, -4.1233e-02, -6.6663e-02,\n",
       "         -3.5491e-02,  1.7206e-02,  2.4953e-02, -4.2429e-03, -4.8260e-02,\n",
       "         -1.0490e-02, -4.9444e-02, -2.3540e-02,  2.7399e-03,  2.2317e-02,\n",
       "         -1.4933e-04, -6.9404e-05,  2.0558e-02,  3.2364e-02, -3.4867e-02,\n",
       "         -1.7993e-02, -2.2791e-02,  1.1715e-02, -3.7663e-02,  2.9240e-02,\n",
       "          6.2165e-03, -5.8701e-02,  5.7432e-02, -6.3188e-02, -4.6997e-03,\n",
       "          2.7648e-02, -3.9382e-02,  1.9218e-02,  8.8087e-04,  6.4610e-02,\n",
       "         -6.4941e-02, -8.1477e-02,  4.5713e-02,  2.5983e-02,  3.9795e-02,\n",
       "         -2.8933e-02, -3.8562e-02, -4.6520e-02,  2.8674e-02,  2.2194e-05,\n",
       "         -3.1622e-02, -9.2083e-03,  1.4539e-02,  1.3001e-02,  4.9162e-02,\n",
       "         -1.0306e-02,  2.4067e-02, -3.9276e-04, -4.4751e-02,  4.6509e-02,\n",
       "         -6.9025e-03,  4.9970e-02, -8.7763e-03, -8.3072e-02,  2.0267e-02,\n",
       "         -1.3265e-02,  2.3599e-02, -4.5073e-03, -2.1056e-02,  9.7713e-03,\n",
       "         -1.7805e-03,  9.6593e-03,  9.3445e-03,  2.4950e-02, -6.9598e-02,\n",
       "          3.6655e-02,  1.3474e-02,  1.9479e-02, -2.8112e-02, -1.0311e-02,\n",
       "          2.2522e-03,  3.6960e-02,  3.2056e-02, -1.2592e-02, -3.3754e-02,\n",
       "         -1.9497e-02,  1.9191e-02, -3.2076e-02,  7.7855e-02, -1.6603e-02,\n",
       "          3.1282e-02, -4.3229e-02,  2.5257e-02,  1.7711e-02, -3.0940e-02,\n",
       "         -7.1953e-03, -8.8843e-03,  2.1813e-02,  1.2992e-02, -2.9698e-02,\n",
       "          2.5172e-02, -1.0739e-02, -1.2660e-02, -4.9395e-02, -2.6233e-03,\n",
       "         -4.6659e-02, -2.2499e-02,  2.2406e-03,  2.2945e-02, -6.5150e-02,\n",
       "          8.1818e-02,  6.7429e-03,  5.0998e-02, -2.6485e-03,  4.6841e-02,\n",
       "          8.7462e-03,  3.6525e-02, -2.0143e-02, -2.7294e-02, -2.5685e-02,\n",
       "         -2.9576e-02,  6.0969e-02,  1.4488e-02,  2.0184e-02,  4.1052e-02,\n",
       "         -3.8770e-02,  3.0582e-02, -8.3040e-03, -1.8027e-03, -7.4966e-03,\n",
       "          4.1684e-02,  1.9330e-02,  6.6933e-03,  3.2714e-03, -7.8578e-03,\n",
       "          2.4302e-02, -1.3996e-02, -1.1305e-02,  3.1901e-02, -1.6472e-02,\n",
       "          2.2030e-02,  3.6633e-02, -1.3247e-02,  1.6567e-02, -3.4998e-02,\n",
       "          6.9117e-03, -3.4473e-02, -4.3351e-02,  1.8592e-02, -5.4370e-03,\n",
       "          3.1571e-02, -2.6901e-02,  2.8913e-02, -1.4412e-02, -8.7646e-03,\n",
       "          2.3616e-02, -3.0144e-02, -2.6816e-02, -4.8618e-02, -4.2232e-02,\n",
       "          7.5559e-02, -5.4824e-03, -1.1995e-02, -2.0792e-02, -4.1269e-02,\n",
       "          3.4320e-02,  2.3096e-02, -5.9972e-02,  6.6683e-02, -1.9856e-02,\n",
       "         -1.0539e-02, -1.1912e-02,  1.3916e-02,  2.4488e-02, -2.9052e-03,\n",
       "          1.7083e-02,  1.7442e-02,  6.5452e-02, -7.9948e-03, -4.6943e-02,\n",
       "         -2.5332e-02, -2.9427e-02,  2.4139e-02,  4.9626e-02, -3.4274e-02,\n",
       "         -1.5242e-02,  2.3516e-02, -8.7458e-02,  1.5258e-02, -2.7954e-02,\n",
       "         -5.2578e-02, -3.2480e-02,  5.1201e-02, -4.4768e-05, -3.0780e-02,\n",
       "         -2.5230e-02,  6.8810e-03, -3.0544e-02,  2.0606e-02, -2.9239e-02,\n",
       "         -3.3382e-02,  2.8894e-02,  6.5767e-02,  5.8315e-02,  2.1441e-03,\n",
       "         -1.1991e-02, -1.1054e-02, -1.9956e-02, -2.8188e-02,  2.5098e-02,\n",
       "         -1.9006e-02,  7.2674e-02,  2.3612e-02,  1.1684e-02,  3.3625e-02,\n",
       "          8.4828e-03,  6.6092e-03, -5.5773e-02,  1.7600e-02,  6.7576e-02,\n",
       "          2.3293e-02,  3.1318e-02, -2.7985e-02, -2.6218e-02, -4.7892e-03,\n",
       "          1.0026e-02, -5.6681e-03,  1.0817e-02, -5.4095e-02,  9.2000e-03,\n",
       "         -4.2877e-02, -3.1729e-02,  2.9606e-02,  3.4520e-02,  2.8556e-02,\n",
       "          2.8925e-02,  3.9116e-03, -3.9054e-02, -1.8387e-02,  2.3178e-02,\n",
       "         -6.8412e-03, -2.3777e-02, -1.7257e-03,  3.0720e-02,  3.6924e-02,\n",
       "         -4.6618e-02, -2.0782e-02, -3.1593e-02,  1.4003e-01,  4.4368e-02,\n",
       "          3.2130e-02,  2.6255e-02,  1.2480e-02, -1.9281e-02,  2.5392e-02,\n",
       "         -5.4073e-02, -1.9745e-02, -2.6827e-02, -2.6306e-02,  7.6319e-02,\n",
       "          2.1419e-03, -9.3944e-03, -2.4790e-02, -2.5493e-02, -2.5064e-02,\n",
       "          2.1014e-02, -2.2036e-02, -4.2389e-02,  6.5589e-02,  1.0657e-02,\n",
       "         -2.1767e-02, -2.3946e-02, -7.8182e-03,  3.7443e-03,  3.4934e-02,\n",
       "          2.9791e-02, -1.8418e-02, -4.4354e-03,  1.2251e-02, -9.1946e-03,\n",
       "         -1.7528e-02, -4.3679e-02,  3.3482e-02, -5.1558e-03, -1.6009e-03,\n",
       "         -8.4541e-02,  3.7556e-02,  3.7052e-02, -5.4625e-02, -1.2902e-03,\n",
       "          6.9211e-02, -4.5545e-02,  5.3947e-02, -1.5216e-02,  1.4783e-02,\n",
       "          1.2056e-03,  2.6573e-02, -5.2492e-02,  3.0024e-03, -3.7918e-02,\n",
       "         -2.0403e-02,  3.1741e-02,  2.0069e-03, -1.2355e-01,  4.7525e-02,\n",
       "          2.7343e-02,  8.4600e-04, -1.0443e-02, -3.9493e-02, -8.2188e-02,\n",
       "          1.4103e-02,  1.5495e-02, -1.7386e-02, -4.9927e-03,  2.1783e-03,\n",
       "          4.9362e-03,  1.2882e-02,  7.6847e-02,  1.9129e-02, -9.5193e-03,\n",
       "          1.7026e-02,  2.1711e-02, -6.5297e-02, -9.9716e-03, -4.6677e-02,\n",
       "         -7.3430e-03, -9.1009e-03,  4.5013e-02, -1.3973e-02, -1.4526e-02,\n",
       "         -3.5265e-02, -1.3315e-02,  4.6138e-03,  5.1951e-02,  4.8545e-02,\n",
       "          1.9416e-02,  5.0178e-02,  6.2535e-03,  1.0298e-02, -3.1836e-02,\n",
       "          2.6593e-02, -9.9769e-03,  2.8398e-02,  5.3253e-03, -2.3406e-02,\n",
       "         -2.1023e-03, -2.0762e-02, -4.2048e-02,  1.0922e-02,  2.4759e-02,\n",
       "          1.3050e-02,  3.9690e-02, -1.0118e-02, -2.1476e-04,  5.6635e-02,\n",
       "          5.3794e-02,  7.8949e-03,  8.8381e-03, -3.0682e-02, -3.0401e-02,\n",
       "         -7.6688e-03,  2.0937e-02, -2.9048e-02, -1.5866e-02, -5.1610e-02,\n",
       "          2.7606e-02, -1.3791e-03,  5.1504e-02, -1.1982e-02, -4.5072e-02,\n",
       "          5.3831e-02, -8.4094e-02, -1.2891e-02, -2.8139e-02,  7.4836e-02,\n",
       "          3.8445e-02, -2.8336e-02,  9.2919e-03,  6.8864e-03, -3.2794e-02,\n",
       "         -1.7273e-02, -2.0730e-02, -1.0727e-02,  8.7326e-02, -2.7617e-02,\n",
       "         -4.2320e-03,  5.3842e-03,  3.7943e-02, -3.0363e-02,  5.4249e-02,\n",
       "          3.4914e-02, -1.2299e-02,  3.3306e-02,  2.7756e-02,  1.9696e-02,\n",
       "         -5.3986e-03, -1.8091e-02,  6.9119e-03, -4.1818e-02,  2.7886e-03,\n",
       "          2.3214e-02, -3.2158e-02, -1.1762e-02,  2.9304e-02, -6.3759e-02,\n",
       "         -3.2266e-03, -4.7305e-02,  9.6058e-03,  4.2762e-02, -8.2526e-03,\n",
       "          4.2057e-02, -2.6084e-02, -4.3360e-03,  4.7843e-02,  2.0579e-02,\n",
       "         -2.5232e-02,  1.8393e-02,  1.6740e-02, -1.2887e-02,  3.3923e-02,\n",
       "         -4.4814e-02, -7.1512e-03,  3.6545e-02,  5.3149e-02, -1.5262e-02,\n",
       "         -2.5564e-02, -2.0346e-04, -1.3373e-02,  3.2183e-02, -1.9160e-02,\n",
       "         -2.0352e-02, -5.9461e-02, -3.5257e-02,  2.5044e-02, -3.6163e-02,\n",
       "          1.6494e-03, -2.4023e-02, -5.2885e-02,  1.4571e-02,  2.5479e-06,\n",
       "          5.5786e-03, -2.3822e-02, -7.5921e-03, -2.2017e-02, -3.5587e-02,\n",
       "          3.0889e-02, -2.6468e-03,  1.3132e-02,  2.4917e-02, -8.6560e-02,\n",
       "         -3.6218e-02, -3.8965e-02, -6.1380e-02, -1.2220e-01,  7.1216e-03,\n",
       "         -1.2898e-02, -4.5205e-02,  1.9803e-03, -6.4531e-02,  3.1866e-02,\n",
       "          1.0010e-01,  1.7393e-02,  2.5841e-02, -4.2667e-02, -7.0814e-03,\n",
       "         -2.4060e-02, -3.3309e-02, -8.1083e-03, -5.2952e-02,  5.1597e-02,\n",
       "          3.1488e-02,  3.3590e-02, -1.5790e-02,  2.5742e-02, -4.1002e-02,\n",
       "         -1.1036e-03,  4.2776e-02,  2.2273e-03,  3.5170e-02,  1.2798e-03,\n",
       "          1.7570e-02,  4.5534e-02,  2.3501e-02, -9.2373e-03, -5.8957e-02,\n",
       "          4.9324e-02, -3.8973e-02,  7.7581e-02, -1.6833e-02, -2.1358e-02,\n",
       "          2.4337e-03, -1.7984e-02,  4.3693e-02,  4.7418e-02,  2.4958e-02,\n",
       "         -3.4779e-02, -4.7345e-02,  1.6302e-02,  7.4535e-02, -6.0941e-02,\n",
       "          1.2007e-02,  4.6035e-02, -5.4361e-03,  7.4148e-03,  1.0968e-02,\n",
       "          7.6190e-03,  2.2714e-02,  1.2990e-02, -2.9029e-02, -4.3566e-02,\n",
       "          4.2160e-02, -2.2288e-04, -4.3666e-02,  1.4346e-02, -3.7181e-02,\n",
       "          3.3038e-02,  4.2873e-02,  3.6083e-03, -3.1234e-02,  3.3625e-02,\n",
       "         -3.3107e-02, -5.1165e-03,  3.1678e-02, -6.7991e-02,  2.6991e-02,\n",
       "          4.4586e-02, -7.6678e-03,  1.7169e-02, -1.7131e-02,  7.6923e-03,\n",
       "         -8.1487e-02, -3.7467e-02,  4.5703e-02,  7.0856e-04,  4.7433e-02,\n",
       "         -5.4055e-02, -2.5926e-02, -4.5471e-02, -3.6329e-02,  4.3626e-02,\n",
       "         -2.2751e-02, -7.4843e-03, -1.3549e-02, -1.4684e-02,  1.0082e-01,\n",
       "         -3.9075e-02, -9.6457e-03,  2.8329e-03,  3.0531e-02, -2.9810e-02,\n",
       "         -4.4733e-02, -4.3566e-03, -4.0475e-03, -2.1657e-03,  4.7465e-03,\n",
       "          6.9436e-03,  4.6539e-02, -1.3227e-02,  4.2841e-02,  6.6269e-02,\n",
       "         -1.0193e-02, -2.0982e-02,  4.3911e-02,  3.5374e-02, -5.2774e-02,\n",
       "         -1.1711e-02, -5.8251e-02, -6.9212e-03,  3.7927e-02,  3.2595e-02,\n",
       "         -2.6086e-02, -7.4131e-02, -2.6186e-02, -9.8352e-03,  3.6038e-02,\n",
       "         -6.7700e-02,  4.5697e-02,  2.0643e-02, -1.9380e-02,  3.6280e-02,\n",
       "          3.3778e-02,  2.7048e-02, -1.9812e-02, -4.8735e-02, -5.4988e-02,\n",
       "         -1.9500e-02,  1.3271e-02,  5.4443e-03,  6.7554e-03, -1.8175e-02,\n",
       "         -2.3045e-02, -9.1778e-03, -7.1132e-02, -1.5173e-02, -5.9817e-02,\n",
       "         -1.8358e-02,  6.6799e-03, -5.7615e-02,  1.6466e-02,  3.0548e-02,\n",
       "          8.3685e-03,  4.0664e-02, -1.2971e-02, -2.4586e-03, -3.4188e-03,\n",
       "         -4.8285e-02, -2.4900e-02,  1.2635e-02,  5.2473e-03, -6.5900e-03,\n",
       "          7.4722e-03,  4.2114e-02, -1.1571e-01,  2.2533e-02, -4.1032e-02,\n",
       "         -2.1842e-03, -2.8145e-02,  3.8246e-02,  4.7583e-02, -5.0958e-02,\n",
       "          4.9336e-02,  1.9977e-02,  4.2821e-02, -1.5199e-02, -1.0774e-02,\n",
       "          3.8961e-02,  1.8231e-02,  1.9685e-02, -3.6049e-02,  2.7435e-02,\n",
       "         -5.9343e-02, -5.8662e-02,  1.5356e-03,  1.2068e-02, -2.3517e-02,\n",
       "          3.9313e-02,  1.7131e-02,  6.5513e-02, -2.9643e-02, -1.4203e-03,\n",
       "          7.5823e-03, -2.2100e-02, -1.2731e-02,  5.3993e-02, -5.5727e-02,\n",
       "          3.9222e-02,  4.6113e-02,  2.5527e-02, -4.0749e-04,  1.4505e-02,\n",
       "         -1.0054e-02, -5.1547e-02, -5.0222e-02,  6.3381e-02,  4.9488e-02,\n",
       "          9.7936e-03, -2.5373e-02, -3.2686e-02, -1.7051e-02, -1.2635e-02,\n",
       "          4.1527e-02, -4.9222e-02, -3.5919e-02]], grad_fn=<DivBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "025084c40b588eb05019761c48ebfbdc57758708fc0433510479ba20b4a83a56"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('cvlp2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
