{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test load model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLIP T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from cvlep.VLT5.param import Config\n",
    "\n",
    "if \"RANK\" in os.environ and \"WORLD_SIZE\" in os.environ:  # torchrun launch\n",
    "    rank = int(os.environ[\"RANK\"])\n",
    "    local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "    world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "elif int(os.environ.get('SLURM_NPROCS', 1)) > 1:  # slurm launch\n",
    "    rank = int(os.environ[\"SLURM_PROCID\"])\n",
    "    local_rank = int(os.environ[\"SLURM_LOCALID\"])\n",
    "    world_size = int(os.environ[\"SLURM_NPROCS\"])\n",
    "else:  # single gpu & process launch\n",
    "    rank = 0\n",
    "    local_rank = 0\n",
    "    world_size = 0\n",
    "\n",
    "config_encoder_question = \"experiments/config_vladapter/local/encoder_simple_adapter.json\"\n",
    "config_encoder_passage = \"experiments/config_vladapter/local/encoder_simple_adapter.json\"\n",
    "config_model = \"experiments/config_vladapter/local/config_model.json\"\n",
    "config_training = \"experiments/config_vladapter/local/training_simple_adapter.json\"\n",
    "\n",
    "# Training config\n",
    "config_training = Config.load_json(config_training)\n",
    "config_training.world_size = world_size\n",
    "config_training.rank = rank\n",
    "config_training.local_rank = local_rank\n",
    "if world_size > 1:\n",
    "    config_training.distributed = True\n",
    "    config_training.multiGPU = True\n",
    "else:\n",
    "    config_training.distributed = False\n",
    "    config_training.multiGPU = False\n",
    "\n",
    "config_encoder_question = Config.load_json(config_encoder_question)\n",
    "config_encoder_passage = Config.load_json(config_encoder_passage)\n",
    "config_model = Config.load_json(config_model)\n",
    "\n",
    "from cvlep.trainer_base_vladapter import Trainer\n",
    "\n",
    "trainer = Trainer(config_encoder_question,config_encoder_passage, config_model, config_training, train=False, local=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "a = nn.Linear(768, 10, bias= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_26852/4112195018.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_parameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "add_code_sample_docstrings() got an unexpected keyword argument 'tokenizer_class'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5679/1841660083.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcvlep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer_base_vladapter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_encoder_question\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconfig_encoder_passage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_training\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Projects/visual_language_representation/cvlep/trainer_base_vladapter.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDistributedDataParallel\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mDDP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistributed\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcvlep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCLIPT5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodeling_t5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVLT5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcvlep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCLIPT5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodeling_t5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mJointEncoder\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mencoderVLT5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcvlep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVLT5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Projects/visual_language_representation/cvlep/CLIPT5/modeling_t5.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBeamScorer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBeamSearchScorer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcvlep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCLIPT5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodeling_bart\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDownsample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSparseSample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOneDDownsample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m from .adapters import (\n",
      "\u001b[0;32m~/Documents/Projects/visual_language_representation/cvlep/CLIPT5/modeling_bart.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmy_transformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodeling_bart\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBartModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBartForConditionalGeneration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBartDecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBartEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Projects/visual_language_representation/cvlep/CLIPT5/my_transformers/modeling_bart.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0mBART_START_DOCSTRING\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1292\u001b[0m )\n\u001b[0;32m-> 1293\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mBartModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBartPretrainedModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mBartConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Projects/visual_language_representation/cvlep/CLIPT5/my_transformers/modeling_bart.py\u001b[0m in \u001b[0;36mBartModel\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1327\u001b[0m         \u001b[0mcheckpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"facebook/bart-large\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m         \u001b[0moutput_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSeq2SeqModelOutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m         \u001b[0mconfig_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_CONFIG_FOR_DOC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1330\u001b[0m     )\n\u001b[1;32m   1331\u001b[0m     def forward(\n",
      "\u001b[0;31mTypeError\u001b[0m: add_code_sample_docstrings() got an unexpected keyword argument 'tokenizer_class'"
     ]
    }
   ],
   "source": [
    "from cvlep.trainer_base_vladapter import Trainer\n",
    "\n",
    "trainer = Trainer(config_encoder_question,config_encoder_passage, config_model, config_training, train=False, local=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = trainer.model.image_passage_encoder.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VLT5(\n",
      "  (shared): Embedding(32200, 768)\n",
      "  (encoder): JointEncoder(\n",
      "    (embed_tokens): Embedding(32200, 768)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 12)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (4): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (5): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (6): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (7): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (8): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (9): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (10): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (11): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (visual_embedding): VisualEmbedding(\n",
      "      (feat_embedding): Sequential(\n",
      "        (0): Linear(in_features=2048, out_features=768, bias=True)\n",
      "        (1): T5LayerNorm()\n",
      "      )\n",
      "      (absolute_vis_pos_embedding): Sequential(\n",
      "        (0): Linear(in_features=5, out_features=768, bias=True)\n",
      "        (1): T5LayerNorm()\n",
      "      )\n",
      "      (obj_order_embedding): Embedding(32200, 768)\n",
      "      (img_order_embedding): Embedding(2, 768)\n",
      "    )\n",
      "  )\n",
      "  (decoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32200, 768)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 12)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (4): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (5): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (6): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (7): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (8): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (9): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (10): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (11): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=32200, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected 1 arguments, got 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_25082/4206866470.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_passage_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: expected 1 arguments, got 0"
     ]
    }
   ],
   "source": [
    "trainer.model.image_passage_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VLT5 without adapter or prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvlep.trainer_base_vladapter import Trainer\n",
    "\n",
    "\n",
    "\n",
    "trainer_model = Trainer(config_encoder_question, config_encoder_passage,config_model, config_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, p in trainer_model.model.image_passage_encoder.named_parameters():\n",
    "    if p.requires_grad==True:\n",
    "        print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvlep.trainer_base_vladapter import Trainer\n",
    "\n",
    "config_encoder_question = \"experiments/config_vladapter/prompt/encoder_prompting.json\"\n",
    "config_encoder_passage = \"experiments/config_vladapter/prompt/encoder_prompting.json\"\n",
    "config_training = \"experiments/config_vladapter/prompt/training_prompt.json\"\n",
    "\n",
    "trainer_clipt5 = Trainer(config_encoder_question, config_encoder_passage, config_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.prompt_modules.prompts.IR.prefix_embedding.0.weight\n",
      "encoder.prompt_modules.prompts.IR.prefix_embedding.1.weight\n",
      "encoder.prompt_modules.prompts.IR.prefix_embedding.1.bias\n",
      "encoder.prompt_modules.prompts.IR.prefix_embedding.3.weight\n",
      "encoder.prompt_modules.prompts.IR.prefix_embedding.3.bias\n"
     ]
    }
   ],
   "source": [
    "for n, p in trainer_clipt5.model.image_passage_encoder.named_parameters():\n",
    "    if p.requires_grad==True:\n",
    "        print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adapter config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvlep.trainer_base_vladapter import Trainer\n",
    "\n",
    "config_encoder_question = \"experiments/config_vladapter/adapter_for_contrastive/encoder_simple_adapter.json\"\n",
    "config_encoder_passage = \"experiments/config_vladapter/adapter_for_contrastive/encoder_simple_adapter.json\"\n",
    "config_training = \"experiments/config_vladapter/adapter_for_contrastive/training_simple_adapter.json\"\n",
    "\n",
    "trainer_clipt5 = Trainer(config_encoder_question, config_encoder_passage, config_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, p in trainer_clipt5.model.image_passage_encoder.named_parameters():\n",
    "    if p.requires_grad==True:\n",
    "        print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk, disable_caching\n",
    "disable_caching()\n",
    "path_dataset = \"data_model/dataset/small_dataset\"\n",
    "dataset = load_from_disk(path_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 11])\n",
      "torch.Size([1, 36, 2048])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from cvlep.utils import device\n",
    "index = 1\n",
    "item = dataset[1]\n",
    "key_vision_features = \"vlt5_features\"\n",
    "key_boxes = \"vlt5_normalized_boxes\"\n",
    "key_text=\"input\"\n",
    "tokenizer = trainer_model.tokenizer_question\n",
    "\n",
    "vision_features = torch.Tensor(item[key_vision_features]).to(device)\n",
    "boxes = torch.Tensor(item[key_boxes]).to(device)\n",
    "vision_features = torch.squeeze(vision_features, dim=1)\n",
    "vis_inputs = (vision_features,boxes)\n",
    "boxes = torch.squeeze(boxes, dim=2)\n",
    "input_ids = tokenizer(\n",
    "    item[key_text], return_tensors='pt', padding=True, truncation=True).input_ids.to(device)\n",
    "return_pooled_output=True,\n",
    "pool_strategy=\"avg\"\n",
    "print(input_ids.size())\n",
    "print(vision_features.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = trainer_model.embedding_question( input_ids = input_ids,\n",
    "    vis_inputs = vis_inputs,\n",
    "    pool_strategy = pool_strategy,\n",
    "    return_pooled_output = return_pooled_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.pooler_output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(32200, 768)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_model.model.image_passage_encoder.encoder.embed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.0044,  0.0702, -0.1362,  ..., -0.0093, -0.0669,  0.2152],\n",
       "         [-0.3346, -0.0321, -0.2023,  ..., -0.0346, -0.2422,  0.1325],\n",
       "         [-0.3988, -0.1186, -0.2928,  ..., -0.1005, -0.0245,  0.0725],\n",
       "         ...,\n",
       "         [ 0.0027,  0.0135,  0.0049,  ...,  0.0231, -0.0006, -0.0090],\n",
       "         [ 0.0780, -0.0152,  0.0395,  ...,  0.0947,  0.0811,  0.1531],\n",
       "         [ 0.0100,  0.0192,  0.0097,  ...,  0.0264, -0.0009, -0.0107]]]), pooler_output=tensor([[ 5.6648e-02, -7.1184e-02, -1.5301e-01, -3.6746e-02, -1.1649e-01,\n",
       "          3.5650e-02,  4.6196e-02, -9.3280e-02, -1.2167e-01,  1.7649e-01,\n",
       "         -1.1129e-01, -5.8460e-02,  9.5520e-02,  1.1314e-02,  4.1585e-02,\n",
       "         -1.2416e-02, -1.0474e-01,  3.2099e-02,  6.2653e-02,  4.1071e-02,\n",
       "          8.7517e-02, -2.3704e-01,  7.3841e-02, -1.6983e-01, -1.1024e-01,\n",
       "          1.1043e-01, -8.2290e-02, -2.5255e-02, -3.4988e-01,  1.7297e-01,\n",
       "          1.3527e-01,  2.0638e-01, -9.9829e-02,  2.6702e-02, -5.7706e-02,\n",
       "         -3.6114e-02, -2.1935e-01, -1.2021e-01,  4.0559e-02,  1.3619e-01,\n",
       "          1.1547e-01,  2.1434e-01,  4.3598e-02,  8.4799e-02, -1.3910e-01,\n",
       "         -7.6862e-02,  4.0068e-02,  8.2855e-02, -1.2356e-01, -2.3356e-02,\n",
       "          8.9281e-02, -2.4328e-01,  9.7265e-02,  6.4604e-02, -6.1479e-02,\n",
       "         -7.6788e-02,  2.0053e-01, -3.4412e-02, -4.9680e-02,  7.8987e-02,\n",
       "         -5.9437e-02, -1.5208e-01, -4.1240e-02, -1.8452e-03,  9.6731e-02,\n",
       "          2.5129e-03, -2.7472e-01,  6.3285e-03, -1.7972e-02,  5.6221e-02,\n",
       "          7.1725e-04,  1.1230e-01, -1.5480e-01,  1.7283e-01,  1.7725e-02,\n",
       "         -8.6306e-03, -5.8082e-02, -5.2759e-02, -1.4953e-02, -3.4251e-02,\n",
       "         -1.8136e-02, -7.2912e-02,  8.0153e-02, -4.5119e-02,  1.3014e-01,\n",
       "          1.7871e-01,  2.4108e-01,  2.5335e-01, -9.1655e-02,  1.0435e-01,\n",
       "         -6.0266e-02, -1.0284e-02, -5.8338e-02, -1.3664e-01,  4.4927e-03,\n",
       "          2.5686e-01,  1.0977e-01,  6.0540e-03,  9.0139e-02, -3.6682e-02,\n",
       "          1.0168e-01, -9.5243e-02,  6.0577e-02, -2.1505e-02, -2.4120e-01,\n",
       "         -3.9939e-02, -2.8749e-02,  1.3322e-02,  6.9176e-02,  1.3201e-01,\n",
       "          7.8436e-02, -1.1388e-03, -7.8952e-02, -1.1538e-01, -1.4526e-01,\n",
       "         -2.6577e-02,  1.1842e-01, -9.1086e-02, -1.2194e-01,  8.6370e-02,\n",
       "         -6.8970e-02,  1.9308e-02,  2.5355e-02, -1.7590e-01, -2.4092e-01,\n",
       "         -4.3160e-03,  5.0034e-02, -1.3436e-01, -4.2643e-02, -1.6965e-01,\n",
       "         -1.9567e-01,  1.1454e-01, -1.0889e-02, -1.8757e-01,  9.9909e-02,\n",
       "         -1.1825e-01, -3.9800e-02,  8.9019e-02,  3.1374e-02, -2.2346e-01,\n",
       "          4.8511e-03, -1.9611e-03, -8.6634e-02,  2.3533e-02,  1.2248e-01,\n",
       "         -1.0433e-01, -6.2481e-02,  3.6984e-02, -1.5841e-01,  1.0640e-01,\n",
       "         -1.6670e-01,  7.9076e-02,  6.0324e-02, -7.1101e-02,  8.4890e-02,\n",
       "         -2.3785e-01,  9.3966e-02,  1.5102e-01, -3.0517e-01, -1.9984e-01,\n",
       "         -8.9604e-02,  3.3380e-02, -8.0320e-02,  1.7646e-01,  1.9851e-02,\n",
       "          1.0962e-01, -2.9159e-02, -1.4942e-01,  1.0430e-01,  1.3476e-01,\n",
       "         -2.5006e-02,  4.2738e-02,  9.7129e-02, -4.5894e-02,  1.9123e-01,\n",
       "         -1.9203e-01,  6.1338e-02, -1.3621e-01, -6.9330e-02,  1.9044e-02,\n",
       "          1.7211e-02, -1.8426e-01, -7.0352e-03, -3.2355e-02, -1.2859e-01,\n",
       "          1.2835e-01, -8.0131e-02,  2.7095e-02, -1.6080e-01, -5.7696e-02,\n",
       "         -7.1342e-02,  6.3420e-02, -1.0779e-01,  1.9965e-01, -9.6445e-02,\n",
       "         -6.2895e-02,  7.8046e-02,  1.7269e-01, -6.3947e-02,  2.3196e-03,\n",
       "          5.7397e-02, -2.3577e-02,  3.3009e-02,  9.0933e-02, -1.3351e-01,\n",
       "          4.1424e-02,  3.3542e-03, -4.5304e-02, -1.7649e-01,  1.3891e-01,\n",
       "          1.1829e-01, -1.3549e-01,  1.1509e-01,  5.2406e-02,  8.0264e-04,\n",
       "         -2.5466e-01,  1.9854e-01,  2.8487e-02, -1.5047e-01, -2.2250e-01,\n",
       "         -4.1130e-01, -1.3324e-01,  5.2766e-03, -3.3753e-02,  2.8282e-02,\n",
       "          9.0963e-02,  1.0388e-01,  3.5608e-01, -3.8365e-02, -7.9780e-03,\n",
       "          5.2078e-02,  2.4794e-01,  1.6939e-01,  4.0255e-02,  1.3860e-01,\n",
       "         -5.7090e-02, -4.2355e-02, -2.3558e-02,  4.7343e-02, -6.7799e-02,\n",
       "         -1.1853e-01, -1.3973e-01, -5.0139e-02,  1.0560e-01,  1.6904e-01,\n",
       "          5.3939e-02,  1.1163e-01, -4.3946e-02,  1.6269e-01, -2.6001e-01,\n",
       "          1.8189e-01, -1.9251e-01,  1.9725e-01,  5.3467e-02,  4.2570e-02,\n",
       "          2.4926e-02,  1.0339e-01, -4.6494e-02,  5.0573e-02, -1.0053e-01,\n",
       "         -3.5200e-02,  3.5145e-01,  3.4159e-03, -1.5362e-01, -9.6309e-03,\n",
       "          2.7890e-01, -6.3908e-02,  1.6790e-01, -7.0743e-02,  1.1093e-01,\n",
       "         -4.0014e-02, -8.8798e-02, -1.0509e-01, -2.9235e-02,  3.5896e-02,\n",
       "         -1.4439e-01,  2.7185e-02, -3.8404e-02, -1.4742e-02,  1.1536e-01,\n",
       "         -2.5055e-01,  7.6182e-02,  8.4615e-05,  1.0779e-01, -1.9729e-01,\n",
       "         -4.8202e-03, -1.1150e-01, -1.5042e-01, -6.8012e-03,  6.0199e-02,\n",
       "          1.0188e-01, -1.0791e-01,  1.1259e-01, -4.7317e-02,  7.9064e-02,\n",
       "         -8.2617e-02, -1.8037e-01, -5.9022e-02, -5.2889e-02,  4.6451e-02,\n",
       "          1.3843e-01, -1.2509e-01,  6.5980e-02, -1.9412e-02,  8.8937e-02,\n",
       "         -8.0666e-02,  4.3589e-02, -1.7870e-01, -7.4802e-03,  8.0770e-02,\n",
       "          1.0898e-01, -1.6556e-02,  9.7671e-02, -1.6894e-01,  1.0011e-01,\n",
       "          1.3348e-01, -3.3382e-02, -2.6455e-02,  4.9345e-02, -1.8567e-03,\n",
       "         -2.6573e-01,  9.9391e-02,  2.0684e-01, -1.5813e-02,  6.2283e-02,\n",
       "         -1.0348e-01,  2.5368e-01, -8.7927e-02, -3.6937e-02, -2.9318e-02,\n",
       "          9.8028e-02, -1.4213e-02, -9.6586e-02,  5.4695e-02, -1.5071e-01,\n",
       "         -4.2884e-02,  2.1310e-02,  5.0642e-02,  3.1646e-02, -1.3338e-01,\n",
       "          1.0224e-01, -2.3936e-01, -6.6592e-02, -2.0546e-01, -1.2527e-01,\n",
       "          1.1854e-01, -1.9952e-02,  9.4907e-02, -1.0361e-01, -7.1104e-02,\n",
       "         -1.6286e-01, -6.7716e-02,  1.6073e-01,  5.2321e-02,  1.1655e-01,\n",
       "          1.4920e-01, -1.7601e-01, -4.5349e-02, -1.0495e-01,  1.7389e-02,\n",
       "         -2.6232e-02,  2.5699e-01,  2.4474e-02,  1.0477e-01, -7.6520e-02,\n",
       "          9.9629e-02, -8.4367e-02, -9.9720e-03,  1.4627e-01,  1.5038e-01,\n",
       "         -8.4939e-02, -3.8132e-02, -5.5359e-02, -1.5582e-01, -1.0076e-01,\n",
       "          1.2360e-02, -6.2420e-02,  1.6432e-01,  1.5075e-01, -5.8578e-02,\n",
       "         -4.8055e-02,  1.3318e-02,  1.2016e-01, -3.3128e-02,  1.2273e-01,\n",
       "          2.2809e-02, -3.5356e-02,  1.0881e-01,  2.1894e-01, -8.9597e-02,\n",
       "         -5.1360e-02, -4.0064e-02, -3.1258e-02, -2.8262e-02,  1.0273e-01,\n",
       "         -3.8696e-02,  9.5733e-02, -7.1016e-02, -2.5011e-03,  9.0620e-02,\n",
       "          8.7721e-02,  5.9105e-02,  1.4423e-01,  8.5796e-02, -4.0406e-02,\n",
       "         -1.1256e-01,  2.5545e-02,  1.5949e-01,  7.2592e-02,  9.6070e-02,\n",
       "          3.1162e-03, -5.0051e-02, -5.5794e-03, -5.9596e-03, -9.1406e-02,\n",
       "          1.7434e-01,  7.1470e-02,  1.0839e-01, -2.6532e-03,  1.6383e-02,\n",
       "         -4.1331e-02,  1.4214e-02,  1.1198e-02,  1.6541e-02,  8.2796e-02,\n",
       "         -4.0043e-02, -1.0858e-01,  4.2423e-02,  1.2854e-01,  7.3324e-03,\n",
       "         -1.8378e-01, -9.4893e-03, -1.1000e-01,  1.4518e-01,  1.2635e-01,\n",
       "          1.1003e-02, -8.8769e-02, -4.3025e-02,  2.7864e-02,  1.2064e-01,\n",
       "          1.0577e-01, -1.5479e-01,  1.0298e-01,  1.8620e-02,  2.3587e-01,\n",
       "          4.6578e-03, -7.9362e-02, -2.6344e-02, -1.1929e-01, -1.4926e-02,\n",
       "          1.8412e-01, -3.6151e-02, -2.1832e-01, -1.3871e-01, -3.2910e-02,\n",
       "          8.7190e-02, -3.3584e-02, -5.9691e-02,  8.6242e-04, -2.4600e-01,\n",
       "          4.5402e-02,  1.5846e-01,  1.1855e-01,  4.0482e-02, -5.7792e-02,\n",
       "         -1.0772e-01,  1.7057e-01,  1.4509e-01, -1.4244e-02,  8.8743e-02,\n",
       "         -1.6682e-02,  2.1597e-01, -7.3645e-02,  8.4626e-02, -2.0321e-01,\n",
       "          5.3152e-02, -4.1354e-02, -1.1845e-01, -6.6002e-02, -2.3721e-01,\n",
       "          2.0754e-01, -2.4641e-01,  1.1394e-01,  7.3031e-02, -2.1068e-01,\n",
       "         -9.9003e-02, -1.0719e-01, -3.6220e-02,  3.8718e-02, -4.2863e-02,\n",
       "         -4.2863e-02, -4.4179e-02,  3.2717e-01,  1.8560e-01, -3.3248e-02,\n",
       "         -7.4413e-02,  9.2814e-02,  2.3129e-02, -5.6121e-02,  5.1840e-02,\n",
       "          2.8309e-02, -1.3142e-01,  2.4845e-02,  7.2732e-02,  3.9975e-02,\n",
       "          1.7479e-01, -1.3004e-01,  2.3654e-02,  5.0932e-02, -1.2324e-01,\n",
       "          7.7021e-02,  4.7436e-02, -2.6862e-01,  4.3295e-02,  6.1174e-02,\n",
       "          1.2143e-01,  4.1469e-01, -1.9343e-01,  1.1031e-01,  2.1305e-02,\n",
       "          1.2391e-02, -1.6763e-01, -1.6243e-01,  1.0325e-02, -1.5955e-01,\n",
       "         -1.9574e-01, -1.2972e-01,  2.8366e-04,  2.0014e-01, -1.5067e-01,\n",
       "         -1.3287e-01, -9.0149e-02, -2.6842e-01,  7.0291e-02,  2.4570e-02,\n",
       "         -3.4946e-02, -1.5780e-01, -2.4727e-01,  2.2608e-01, -6.9311e-02,\n",
       "          1.6046e-01, -1.3901e-01, -3.2820e-02,  2.8448e-01,  1.9665e-01,\n",
       "          1.8996e-01, -5.9038e-02, -1.6209e-01, -7.8300e-02,  1.0704e-01,\n",
       "          1.1561e-01,  3.5261e-02,  1.1100e-01,  6.1467e-02,  6.9499e-02,\n",
       "          1.4634e-01, -1.5710e-01,  3.1434e-02, -8.9579e-02, -1.1669e-01,\n",
       "         -5.8055e-02, -4.0590e-02,  9.0771e-02, -8.0851e-02, -1.1806e-01,\n",
       "         -2.5185e-02,  1.4855e-02, -5.9336e-02, -6.5623e-02,  9.7041e-02,\n",
       "          2.7690e-01, -1.9987e-02, -2.8658e-02, -1.4633e-01, -8.7065e-03,\n",
       "         -3.3145e-02,  1.4271e-02, -1.1132e-01,  5.0628e-02, -7.1439e-02,\n",
       "          1.4279e-01,  2.0576e-01,  1.3376e-01, -1.3618e-01,  1.0161e-01,\n",
       "         -6.6048e-02,  1.7167e-01,  6.4942e-02, -2.0768e-01,  9.5287e-02,\n",
       "          6.1489e-03, -4.3719e-02,  1.4714e-01,  1.7106e-02,  2.5687e-01,\n",
       "         -6.8975e-02,  6.0700e-02,  1.1607e-01, -8.8286e-02, -2.5594e-02,\n",
       "          1.4360e-01, -9.5792e-02, -6.7379e-02,  1.5858e-01, -1.8343e-01,\n",
       "         -4.1773e-03,  1.1835e-01, -1.2830e-01, -4.7252e-03, -6.9979e-02,\n",
       "         -4.3474e-03, -1.5716e-01, -7.8922e-03,  5.4170e-02, -8.5557e-02,\n",
       "         -1.3081e-01, -1.4872e-01, -1.4497e-02,  2.2065e-01, -1.3079e-01,\n",
       "         -5.6204e-02,  1.4542e-01,  7.2989e-02,  2.0288e-01,  8.7270e-02,\n",
       "         -2.5098e-02, -1.4824e-01, -1.6793e-01, -1.1220e-02, -2.1143e-01,\n",
       "         -8.6873e-02,  1.9970e-01,  1.1379e-01, -2.0107e-02,  1.3206e-01,\n",
       "          2.0396e-01,  1.4635e-01,  5.7540e-02,  2.1541e-01,  1.1418e-01,\n",
       "         -5.5772e-02,  2.7615e-01, -1.5337e-01, -5.8670e-03,  2.5867e-01,\n",
       "         -6.3398e-02,  3.8931e-01,  2.4190e-01,  1.5852e-01, -1.0393e-02,\n",
       "          2.4114e-02,  1.8368e-01,  1.6601e-01, -5.4443e-02, -1.8606e-01,\n",
       "         -2.8720e-01, -5.0861e-02, -2.4094e-02, -9.5253e-04,  1.2215e-02,\n",
       "          1.4893e-03, -9.4679e-02, -1.0176e-01,  7.2016e-02, -1.4162e-01,\n",
       "          8.9151e-04, -5.5513e-02,  2.1170e-01, -4.1009e-03, -5.7914e-02,\n",
       "         -8.8402e-02,  5.2346e-03, -8.2523e-02,  2.2422e-02, -7.5668e-02,\n",
       "         -1.3940e-02,  7.4526e-02,  5.4616e-02, -8.9429e-02,  8.5130e-02,\n",
       "         -1.8144e-02, -1.8300e-03,  1.3815e-01, -2.0179e-01, -1.2929e-01,\n",
       "         -2.5350e-01, -3.8829e-02, -6.4619e-02, -9.1117e-02, -1.3473e-01,\n",
       "          1.5278e-01, -2.3307e-01,  5.3524e-02,  9.3714e-02, -1.2227e-02,\n",
       "          7.9154e-02,  1.2854e-01, -8.1355e-02, -2.4539e-01,  1.7300e-02,\n",
       "          1.5464e-01, -3.2712e-01,  1.0083e-01, -3.0230e-02,  7.0219e-02,\n",
       "         -1.2951e-01, -2.4582e-03,  1.6231e-01, -1.1312e-01, -5.8386e-02,\n",
       "          1.9637e-01,  3.1791e-02, -3.1436e-02, -2.4544e-02,  2.0815e-02,\n",
       "         -7.6786e-02, -1.9849e-02,  6.4449e-03, -5.7112e-02, -4.5437e-02,\n",
       "         -1.0695e-01,  1.0656e-01,  4.3160e-02, -2.0950e-01,  2.1037e-01,\n",
       "         -1.7241e-02,  5.1989e-02,  1.6865e-02,  7.8504e-02,  1.9873e-03,\n",
       "          1.2072e-01,  1.2359e-01, -1.5053e-01,  1.2013e-01, -6.5865e-03,\n",
       "          8.9148e-02,  2.5787e-01, -8.8401e-02,  1.0379e-01,  2.1067e-01,\n",
       "         -1.3900e-02, -8.2743e-02,  1.4271e-02,  1.0033e-01, -1.3111e-01,\n",
       "          1.2430e-01, -4.0163e-02, -1.1760e-01,  1.0495e-01, -2.5412e-01,\n",
       "          3.0550e-02,  2.1363e-01, -6.0252e-02, -9.7014e-02, -2.9208e-02,\n",
       "          1.3326e-01,  5.4465e-02,  6.5086e-02,  1.4199e-01, -3.0988e-02,\n",
       "          1.9695e-01,  2.3529e-02, -1.9665e-01,  1.0276e-01, -1.8946e-01,\n",
       "         -1.2714e-01,  1.4359e-01, -3.7140e-02]]), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_model.encoder_question.encoder.forward(\n",
    "    input_ids = input_ids,\n",
    "    vis_inputs = vis_inputs,\n",
    "    pool_strategy = pool_strategy,\n",
    "    return_pooled_output = return_pooled_output\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk, disable_caching\n",
    "disable_caching()\n",
    "path_dataset = \"data_model/dataset/small_dataset\"\n",
    "dataset = load_from_disk(path_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from cvlep.utils import device\n",
    "index = 2\n",
    "item = dataset[1]\n",
    "key_vision_features = \"vlt5_features\"\n",
    "key_boxes = \"vlt5_normalized_boxes\"\n",
    "key_text=\"input\"\n",
    "tokenizer = trainer_clipt5.tokenizer_question\n",
    "\n",
    "vision_features = torch.Tensor(item[key_vision_features]).to(device)\n",
    "boxes = torch.Tensor(item[key_boxes]).to(device)\n",
    "vision_features = torch.squeeze(vision_features, dim=1)\n",
    "boxes = torch.squeeze(boxes, dim=1)\n",
    "input_ids = tokenizer(\n",
    "    item[key_text], return_tensors='pt', padding=True, truncation=True).input_ids.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_encoder_vlt5 = trainer_vlt5.model.image_question_encoder.encoder.forward(\n",
    "    input_ids = input_ids,\n",
    "    vis_inputs=(vision_features,boxes)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_encoder_clipT5 = trainer_clipt5.model.image_question_encoder.encoder.forward(\n",
    "    input_ids=input_ids,\n",
    "    vis_inputs=(vision_features,boxes)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0044,  0.0702, -0.1362,  ..., -0.0093, -0.0669,  0.2152],\n",
       "         [-0.3346, -0.0321, -0.2023,  ..., -0.0346, -0.2422,  0.1325],\n",
       "         [-0.3988, -0.1186, -0.2928,  ..., -0.1005, -0.0245,  0.0725],\n",
       "         ...,\n",
       "         [ 0.0027,  0.0135,  0.0049,  ...,  0.0231, -0.0006, -0.0090],\n",
       "         [ 0.0780, -0.0152,  0.0395,  ...,  0.0947,  0.0811,  0.1531],\n",
       "         [ 0.0100,  0.0192,  0.0097,  ...,  0.0264, -0.0009, -0.0107]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_encoder_vlt5[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = output_encoder_clipT5[0] == output_encoder_vlt5[0]\n",
    "torch.equal(output_encoder_clipT5[0],output_encoder_vlt5[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pgrimal/miniconda3/envs/cvlp/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of VLT5 were not initialized from the model checkpoint at data_model/t5_pretrained and are newly initialized: ['encoder.visual_embedding.feat_embedding.0.weight', 'encoder.visual_embedding.feat_embedding.0.bias', 'encoder.visual_embedding.feat_embedding.1.weight', 'encoder.visual_embedding.absolute_vis_pos_embedding.0.weight', 'encoder.visual_embedding.absolute_vis_pos_embedding.0.bias', 'encoder.visual_embedding.absolute_vis_pos_embedding.1.weight', 'encoder.visual_embedding.obj_order_embedding.weight', 'encoder.visual_embedding.img_order_embedding.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of VLT5 were not initialized from the model checkpoint at data_model/t5_pretrained and are newly initialized: ['encoder.visual_embedding.feat_embedding.0.weight', 'encoder.visual_embedding.feat_embedding.0.bias', 'encoder.visual_embedding.feat_embedding.1.weight', 'encoder.visual_embedding.absolute_vis_pos_embedding.0.weight', 'encoder.visual_embedding.absolute_vis_pos_embedding.0.bias', 'encoder.visual_embedding.absolute_vis_pos_embedding.1.weight', 'encoder.visual_embedding.obj_order_embedding.weight', 'encoder.visual_embedding.img_order_embedding.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from  data_model/whole_model/vlt5/VLT5Epoch30.pth\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['encoder.visual_embedding.layer_norm.weight'])\n",
      "Model loaded from  data_model/whole_model/vlt5/VLT5Epoch30.pth\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['encoder.visual_embedding.layer_norm.weight'])\n",
      "Configurations\n",
      "{'share_embedding': True, 'share_vis_embedding': True}\n"
     ]
    }
   ],
   "source": [
    "from cvlep.trainer_base_vladapter import Trainer\n",
    "\n",
    "config_encoder_question = \"experiments/config_vladapter/model_only/args_vlt5_wo_adap.json\"\n",
    "config_encoder_passage = \"experiments/config_vladapter/model_only/args_vlt5_wo_adap.json\"\n",
    "config_model = \"experiments/config_vladapter/model_only/config_model.json\"\n",
    "config_training = \"experiments/config_vladapter/model_only/training_vlt5.json\"\n",
    "\n",
    "trainer_model = Trainer(config_encoder_question, config_encoder_passage,config_model, config_training)\n",
    "\n",
    "from datasets import load_from_disk, disable_caching\n",
    "disable_caching()\n",
    "path_dataset = \"data_model/dataset/small_dataset\"\n",
    "dataset = load_from_disk(path_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from cvlep.utils import device\n",
    "\n",
    "\n",
    "def get_feat(item):\n",
    "    key_vision_features = \"vlt5_features\"\n",
    "    key_boxes = \"vlt5_normalized_boxes\"\n",
    "    key_text=\"input\"\n",
    "    vision_features = torch.Tensor(item[key_vision_features])\n",
    "    boxes = torch.Tensor(item[key_boxes])\n",
    "    vision_features = torch.squeeze(vision_features, dim=0)\n",
    "    boxes = torch.squeeze(boxes, dim=0)\n",
    "    return {\"text\":item[key_text], \"feats\":vision_features, \"boxes\":boxes, \"size\":boxes.size()[0]}\n",
    "\n",
    "B = 2\n",
    "question1 = get_feat(dataset[0])\n",
    "relevant1 = get_feat(dataset[0])   \n",
    "irrelevant1 = get_feat(dataset[1])\n",
    "question2 = get_feat(dataset[2])\n",
    "relevant2 = get_feat(dataset[2])\n",
    "irrelevant2 = get_feat(dataset[3])\n",
    "\n",
    "item1 = {}\n",
    "item1[\"question_text\"] = question1[\"text\"]\n",
    "item1[\"passage_relevant_text\"] = relevant1['text']\n",
    "item1['passage_irrelevant_text'] = irrelevant1[\"text\"]\n",
    "item1[\"n_boxes_question\"] = question1[\"size\"]\n",
    "item1[\"n_boxes_passage_relevant\"] = relevant1[\"size\"]\n",
    "item1[\"n_boxes_passage_irrelevant\"] = irrelevant1[\"size\"]\n",
    "item1[\"question_image_features\"] = question1['feats']\n",
    "item1[\"question_image_boxes\"] = question1[\"boxes\"]\n",
    "item1[\"passage_relevant_image_features\"]=relevant1[\"feats\"]\n",
    "item1[\"passage_relevant_image_boxes\"]=relevant1[\"boxes\"]\n",
    "item1[\"passage_irrelevant_image_features\"]=irrelevant1['feats']\n",
    "item1[\"passage_irrelevant_image_boxes\"]=irrelevant1[\"boxes\"]\n",
    "\n",
    "item2 = {}\n",
    "item2[\"question_text\"] = question2[\"text\"]\n",
    "item2[\"passage_relevant_text\"] = relevant2['text']\n",
    "item2['passage_irrelevant_text'] = irrelevant2[\"text\"]\n",
    "item2[\"n_boxes_question\"] = question2[\"size\"]\n",
    "item2[\"n_boxes_passage_relevant\"] = relevant2[\"size\"]\n",
    "item2[\"n_boxes_passage_irrelevant\"] = irrelevant2[\"size\"]\n",
    "item2[\"question_image_features\"] = question2['feats']\n",
    "item2[\"question_image_boxes\"] = question2[\"boxes\"]\n",
    "item2[\"passage_relevant_image_features\"]=relevant2[\"feats\"]\n",
    "item2[\"passage_relevant_image_boxes\"]=relevant2[\"boxes\"]\n",
    "item2[\"passage_irrelevant_image_features\"]=irrelevant2['feats']\n",
    "item2[\"passage_irrelevant_image_boxes\"]=irrelevant2[\"boxes\"]\n",
    "\n",
    "batch = [item1, item2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 2\n",
    "\n",
    "tokenizer = trainer_model.tokenizer_question\n",
    "V_L_question = max(item['n_boxes_question'] for item in batch)\n",
    "V_L_context = max(max(item['n_boxes_passage_relevant'],\n",
    "                    item['n_boxes_passage_irrelevant']) for item in batch)\n",
    "feat_dim = batch[0]['question_image_features'].shape[-1]\n",
    "# boxes are represented by 4 points\n",
    "question_boxes = torch.zeros(B, V_L_question, 4, dtype=torch.float)\n",
    "question_vis_feats = torch.zeros(\n",
    "    B, V_L_question, feat_dim, dtype=torch.float)\n",
    "relevant_boxes = torch.zeros(B, V_L_context, 4, dtype=torch.float)\n",
    "relevant_vis_feats = torch.zeros(\n",
    "    B, V_L_context, feat_dim, dtype=torch.float)\n",
    "irrelevant_boxes = torch.zeros(\n",
    "    B, V_L_context, 4, dtype=torch.float)\n",
    "irrelevant_vis_feats = torch.zeros(\n",
    "    B, V_L_context, feat_dim, dtype=torch.float)\n",
    "\n",
    "relevant_text, irrelevant_text, question_text, labels = list(), list(), list(), list()\n",
    "for i, item in enumerate(batch):\n",
    "    # TODO: voir si besoin de changer gestion pour le text, car a un impact sur attention mask\n",
    "    question_text.append(item['question_text'])\n",
    "    relevant_text.append(item['passage_relevant_text'])\n",
    "    irrelevant_text.append(item['passage_irrelevant_text'])\n",
    "    if tokenizer:\n",
    "        n_boxes_relevant = item['n_boxes_passage_relevant']\n",
    "        n_boxes_irrelevant = item['n_boxes_passage_irrelevant']\n",
    "        n_boxes_question = item['n_boxes_question']\n",
    "        question_boxes[i,\n",
    "                        :n_boxes_question] = item['question_image_boxes']\n",
    "        question_vis_feats[i,\n",
    "                            :n_boxes_question] = item['question_image_features']\n",
    "        relevant_boxes[i,\n",
    "                        :n_boxes_relevant] = item['passage_relevant_image_boxes']\n",
    "        relevant_vis_feats[i,\n",
    "                            :n_boxes_relevant] = item['passage_relevant_image_features']\n",
    "        irrelevant_boxes[i,\n",
    "                            :n_boxes_irrelevant] = item['passage_irrelevant_image_boxes']\n",
    "        irrelevant_vis_feats[i,\n",
    "                                :n_boxes_irrelevant] = item['passage_irrelevant_image_features']\n",
    "    if item['passage_relevant_text'] is None:\n",
    "        labels.append(-100)  # ignore index when computing the loss\n",
    "    else:\n",
    "        labels.append(i)\n",
    "\n",
    "question_input = tokenizer(\n",
    "    question_text, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "context_input = tokenizer(\n",
    "    relevant_text + irrelevant_text, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "labels = torch.tensor(labels)\n",
    "visual_feats_context = torch.concat(\n",
    "    [relevant_vis_feats, irrelevant_vis_feats])\n",
    "context_image_boxes = torch.concat(\n",
    "    [relevant_boxes, irrelevant_boxes])\n",
    "results = {\n",
    "    \"input_ids_question\": question_input.input_ids,\n",
    "    \"attention_mask_question\": question_input.attention_mask,\n",
    "    \"input_ids_context\": context_input.input_ids,\n",
    "    \"attention_mask_context\": context_input.attention_mask,\n",
    "    \"labels\": labels,\n",
    "    \"visual_feats_question\": question_vis_feats,\n",
    "    \"visual_feats_context\": visual_feats_context,\n",
    "    \"question_image_boxes\": question_boxes,\n",
    "    \"context_image_boxes\": context_image_boxes\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1641)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loss doit tre faible car on met exactement mme embedding pour question \n",
    "# et relevant passage\n",
    "trainer_model.model.train_step(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test gather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0, 1, 2, 3]), tensor([   0,    1, -100,    3])]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "loss_fct = nn.NLLLoss(reduction='mean')\n",
    "batch1 ={\n",
    "\"labels\": torch.tensor([0,1,2,3])\n",
    "}\n",
    "batch2 = {\n",
    "\"labels\": torch.tensor([0,1,-100,3])\n",
    "}\n",
    "labels1 = batch1.pop('labels')\n",
    "labels2 = batch2.pop('labels')\n",
    "labels_gatherer = [labels1, labels2]\n",
    "gatherers = zip(labels_gatherer)\n",
    "labels_gatherer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([0, 1, 2, 3]), tensor([   8,    9, -100,   11])]\n"
     ]
    }
   ],
   "source": [
    "# N questions taille du batch\n",
    "N = 4\n",
    "# nombre relevant et irrelevant 1 et 1\n",
    "global_labels = []\n",
    "label_shift = 0\n",
    "for i, (received_labels) in enumerate(gatherers):\n",
    "    received_labels = received_labels[0]\n",
    "    received_labels[received_labels!=-100] += label_shift\n",
    "    label_shift += 4 * 2 # N * M\n",
    "    global_labels.append(received_labels)\n",
    "print(global_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "exemple d'un batch de 1\n",
    "```\n",
    "| question1 | * [relevant1, irr1, relevant2, irr2]\n",
    "| question2 |\n",
    "```\n",
    "label devra donc etre [0, 2]\n",
    "ce qui explique le besoin du shift\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "m = nn.LogSoftmax(dim=1)\n",
    "loss = nn.NLLLoss()\n",
    "# input is of size N x C = 3 x 5\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "# each element in target has to have 0 <= value < C\n",
    "target = torch.tensor([1, 0, 4])\n",
    "output = loss(m(input), target)\n",
    "output.backward()\n",
    "# 2D loss example (used, for example, with image inputs)\n",
    "N, C = 5, 4\n",
    "loss = nn.NLLLoss()\n",
    "# input is of size N x C x height x width\n",
    "data = torch.randn(N, 16, 10, 10)\n",
    "conv = nn.Conv2d(16, C, (3, 3))\n",
    "m = nn.LogSoftmax(dim=1)\n",
    "# each element in target has to have 0 <= value < C\n",
    "target = torch.empty(N, 8, 8, dtype=torch.long).random_(0, C)\n",
    "output = loss(m(conv(data)), target)\n",
    "output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5051658153533936"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " epoch 0 | Loss 2.5: 100%|| 2/2 [00:02<00:00,  1.00s/it]\n",
      " epoch 1 | Loss 0.625: 100%|| 2/2 [00:02<00:00,  1.00s/it]\n",
      " epoch 2 | Loss 0.15625: 100%|| 2/2 [00:02<00:00,  1.00s/it]\n",
      " epoch 3 | Loss 0.0390625: 100%|| 2/2 [00:02<00:00,  1.00s/it]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "loss = 10\n",
    "for epoch in range(4):\n",
    "    pbar = tqdm(total=2, ncols=120)\n",
    "    for batch in range(2):\n",
    "        loss *= 0.5\n",
    "        time.sleep(1)\n",
    "        desc_str = f' epoch {epoch} | Loss {loss}'\n",
    "        pbar.set_description(desc_str)\n",
    "        pbar.update(1)\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvlep.VLT5.param import Config\n",
    "config = Config.load_json(\"experiments/config_vladapter/prompt/training_prompt.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.rank = 'yes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.local = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/sentence-transformers/sentence-t5-base\n",
    "\n",
    "https://github.com/UKPLab/sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pgrimal/miniconda3/envs/cvlp/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/sentence-t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = model.__getitem__(3)\n",
    "dense = model.__getitem__(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T_destination',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_backward_hooks',\n",
       " '_buffers',\n",
       " '_call_impl',\n",
       " '_forward_hooks',\n",
       " '_forward_pre_hooks',\n",
       " '_get_backward_hooks',\n",
       " '_get_name',\n",
       " '_is_full_backward_hook',\n",
       " '_load_from_state_dict',\n",
       " '_load_state_dict_pre_hooks',\n",
       " '_maybe_warn_non_full_backward_hook',\n",
       " '_modules',\n",
       " '_named_members',\n",
       " '_non_persistent_buffers_set',\n",
       " '_parameters',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_replicate_for_data_parallel',\n",
       " '_save_to_state_dict',\n",
       " '_slow_forward',\n",
       " '_state_dict_hooks',\n",
       " '_version',\n",
       " 'add_module',\n",
       " 'apply',\n",
       " 'bfloat16',\n",
       " 'buffers',\n",
       " 'children',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'double',\n",
       " 'dump_patches',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'float',\n",
       " 'forward',\n",
       " 'get_buffer',\n",
       " 'get_extra_state',\n",
       " 'get_parameter',\n",
       " 'get_submodule',\n",
       " 'half',\n",
       " 'load_state_dict',\n",
       " 'modules',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'parameters',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_full_backward_hook',\n",
       " 'register_module',\n",
       " 'register_parameter',\n",
       " 'requires_grad_',\n",
       " 'set_extra_state',\n",
       " 'share_memory',\n",
       " 'state_dict',\n",
       " 'to',\n",
       " 'to_empty',\n",
       " 'train',\n",
       " 'training',\n",
       " 'type',\n",
       " 'xpu',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense.activation_function.__dir__()\n",
    "# https://github.com/UKPLab/sentence-transformers/blob/46a149433fe9af0851f7fa6f9bf37b5ffa2c891c/sentence_transformers/models/Dense.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Normalize()"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TriviaQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration PaulLerner--triviaqa_for_viquae-ab2ffed0dcbf3b5f\n",
      "Reusing dataset parquet (/home/pgrimal/.cache/huggingface/datasets/PaulLerner___parquet/PaulLerner--triviaqa_for_viquae-ab2ffed0dcbf3b5f/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n",
      "100%|| 4/4 [00:00<00:00, 172.60it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"PaulLerner/triviaqa_for_viquae\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    with_viquae_test: Dataset({\n",
       "        features: ['id', 'input', 'meta', 'output'],\n",
       "        num_rows: 1247\n",
       "    })\n",
       "    with_viquae_train: Dataset({\n",
       "        features: ['id', 'input', 'meta', 'output'],\n",
       "        num_rows: 1181\n",
       "    })\n",
       "    with_viquae_validation: Dataset({\n",
       "        features: ['id', 'input', 'meta', 'output'],\n",
       "        num_rows: 1234\n",
       "    })\n",
       "    without_viquae: Dataset({\n",
       "        features: ['id', 'input', 'meta', 'output'],\n",
       "        num_rows: 47000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "dataset_kilt = load_from_disk(\"/home/pgrimal/Documents/data_viquae/kilt_trivia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['kilt_id', 'wikipedia_id', 'wikipedia_title', 'text', 'anchors', 'categories', 'wikidata_info', 'history'],\n",
       "    num_rows: 5903530\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_kilt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['summarization', 'translation_en_to_de', 'translation_en_to_fr', 'translation_en_to_ro'])\n"
     ]
    }
   ],
   "source": [
    "print(model.config.task_specific_params.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "British spy.\n"
     ]
    }
   ],
   "source": [
    "# inference\n",
    "input_ids = tokenizer(\n",
    "    \"James Bond is a <extra_id_0> <extra_id_1>\", return_tensors=\"pt\"\n",
    ").input_ids  # Batch size 1\n",
    "outputs = model.generate(input_ids)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ship built in 1912.\n"
     ]
    }
   ],
   "source": [
    "# inference\n",
    "input_ids = tokenizer(\n",
    "    \"Titanic is a <extra_id_0> <extra_id_1>\", return_tensors=\"pt\"\n",
    ").input_ids  # Batch size 1\n",
    "outputs = model.generate(input_ids)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLIP preprocessing\n",
    "\n",
    "But : comprendre comment sont gnrs les features actuellement avec clip\n",
    "Si bonne facon d'utiliser clip on laisse comme a sinon on essaie de fournir juste un embedding d'image et pas plusieurs embedding en fonction des boxes\n",
    "\n",
    "Fichier utilis cvlep.CLIPT5.clip_prepro_feats\n",
    "\n",
    "charger le modle :\n",
    "\n",
    "Dans VLT5 il y a cette fonction\n",
    "```py\n",
    "def vis_forward(self, batch, device):\n",
    "    if hasattr(self, \"vis_encoder\"):\n",
    "        # self.vis_encoder.eval() # freeze the batchnorm statistics\n",
    "        images = batch[\"images\"].to(device)\n",
    "\n",
    "        if self.config.vis_pooling_output:\n",
    "            _, vis_feats = self.vis_encoder(images)\n",
    "        else:\n",
    "            vis_feats, _ = self.vis_encoder(images)\n",
    "        # vis_feats: (B, dim, L ** 0.5, L ** 0.5)\n",
    "        B, L, D = vis_feats.shape\n",
    "        vis_pos = torch.zeros(B, L, 4, dtype=vis_feats.dtype)\n",
    "\n",
    "        batch[\"vis_feats\"] = vis_feats\n",
    "        batch[\"boxes\"] = vis_pos\n",
    "```\n",
    "\n",
    "Voir ce que renvoie le modle avec la fonction\n",
    "\n",
    "```py\n",
    "self.vis_encoder = get_vis_encoder(\n",
    "        backbone=vis_encoder_type, \n",
    "        image_size=eval(self.args.image_size)[0],\n",
    "        adapter_type=None,\n",
    "    )\n",
    "self.model.vis_encoder = self.vis_encoder\n",
    "```\n",
    "\n",
    "get_vis encoder dans vis_encoder.py\n",
    "\n",
    "Use CLIP-ResNet50 for fair comparaison\n",
    "\n",
    "format en entre de vis_forward\n",
    "regarder dans vlt5 cococaption T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvlep.CLIPT5.vis_encoder import get_vis_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pgrimal/miniconda3/envs/cvlp2/lib/python3.7/site-packages/torchvision/transforms/transforms.py:333: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
     ]
    }
   ],
   "source": [
    "model = get_vis_encoder(backbone='RN101', adapter_type=None, image_size=eval(\"(224,224)\")[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pgrimal/miniconda3/envs/cvlp2/lib/python3.7/site-packages/ipykernel_launcher.py:10: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/home/pgrimal/miniconda3/envs/cvlp2/lib/python3.7/site-packages/ipykernel_launcher.py:21: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n"
     ]
    }
   ],
   "source": [
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision.transforms import (\n",
    "    Compose, Resize, CenterCrop, ToTensor, Normalize, RandomCrop, RandomHorizontalFlip, RandomErasing\n",
    ")\n",
    "def _transform(n_px):\n",
    "    return Compose([\n",
    "        # PadToSquare(),\n",
    "        Resize(n_px, interpolation=Image.BICUBIC),\n",
    "        CenterCrop(n_px),\n",
    "        # MinMaxResize(*n_px),\n",
    "        lambda image: image.convert(\"RGB\"),\n",
    "        ToTensor(),\n",
    "        Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "    ])\n",
    "\n",
    "\n",
    "def augmentation_transform(image_size):\n",
    "    return Compose([\n",
    "        Resize(image_size, interpolation=Image.BICUBIC),\n",
    "        RandomHorizontalFlip(),\n",
    "        RandomCrop(image_size, padding=int(image_size[0]*0.0625), padding_mode='reflect'),\n",
    "        lambda image: image.convert(\"RGB\"),\n",
    "        ToTensor(),\n",
    "        Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "        RandomErasing(),\n",
    "    ])\n",
    "\n",
    "\n",
    "transform = _transform(eval(\"(224,224)\")[0])\n",
    "transform2 = augmentation_transform(eval(\"(224,224)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"/home/pgrimal/Documents/Projects/visual_language_representation/results/best_zs_vlbart/images/folder_50/512px-Joseph_Stalin-TIME-1930.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = transform(image)\n",
    "t2 = transform2(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "print(t.shape)\n",
    "print(t2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_boxes = 36 #self.args.n_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fake batch\n",
    "t = torch.unsqueeze(t, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pooling torch.Size([1, 1, 512])\n",
      "output classic torch.Size([1, 49, 2048])\n"
     ]
    }
   ],
   "source": [
    "_, pooling_output = output\n",
    "output_normal, _ = output\n",
    "\n",
    "print(\"pooling\", pooling_output.shape)\n",
    "print(\"output classic\", output_normal.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2048])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_normal[:,0,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 0. Expected size 25 but got size 24 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3515/1658407024.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_normal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (2B, L/2, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (B, L, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 0. Expected size 25 but got size 24 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "def downsample(inputs):\n",
    "        pool = torch.nn.AdaptiveMaxPool2d(6) # racine de 36\n",
    "        B, L, dim = inputs.shape\n",
    "\n",
    "        inputs = inputs.permute(0, 2, 1) # (2B, dim, L/2)\n",
    "        print(inputs.shape)\n",
    "        sqrt_L = int(L ** 0.5)\n",
    "        inputs = inputs.reshape(B, dim, sqrt_L, sqrt_L)\n",
    "        print(inputs.shape)\n",
    "        inputs = pool(inputs)\n",
    "        print(inputs.shape)\n",
    "        inputs = inputs.reshape(B, dim, -1)\n",
    "        print(inputs.shape)\n",
    "        inputs = inputs.permute(0, 2, 1)\n",
    "        print(inputs.shape)\n",
    "        return inputs\n",
    "\n",
    "inputs = output_normal\n",
    "inputs = torch.cat(torch.chunk(inputs, 2, 1), 0) # (2B, L/2, dim)\n",
    "inputs = downsample(inputs)\n",
    "inputs = torch.cat(torch.chunk(inputs, 2, 0), 1) # (B, L, dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cas pooling\n",
    "B, L, D = pooling_output.shape\n",
    "vis_pos = torch.zeros(B, L, 4, dtype=pooling_output.dtype)\n",
    "boxes = vis_pos\n",
    "print(boxes.shape)\n",
    "boxes = boxes[:, :pooling_output.shape[1]]\n",
    "boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 49, 4])\n"
     ]
    }
   ],
   "source": [
    "# cas non pooling\n",
    "B, L, D = output_normal.shape\n",
    "vis_pos = torch.zeros(B, L, 4, dtype=output_normal.dtype)\n",
    "boxes = vis_pos\n",
    "print(boxes.shape)\n",
    "# on coonsidre dans ce cas qu'on utilise 49 images ?\n",
    "# multiplier les features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 18, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boxes = torch.cat(torch.chunk(boxes[:,:36,:], 2, 1), 0)\n",
    "boxes = boxes[:, :output_normal.shape[1]//2]\n",
    "boxes = torch.cat(torch.chunk(boxes, 2, 0), 1)\n",
    "print(boxes.shape)\n",
    "36**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "choix des dimensions en entr en utilisant commentaire (mme si on ne veut vraiment se comparer car pas mme tche)\n",
    "\n",
    "Par exemple pooling pas forcmenet intressant pour nous de le faire\n",
    "on peut peut tre mieux comprendre l'image mme si on risque de s'y perdre\n",
    "\n",
    "pour une faire comparaison vaut peut tre mieux utiliser pooling (mme dimension pour clip)\n",
    "\n",
    "```\n",
    "Input images are resized to 224  224\n",
    "for the memory efficiency. We extract the 7  7 grid fea-\n",
    "tures produced by the last convolutional layer, and then ap-\n",
    "ply adaptive maximum-pooling over the features for down-\n",
    "sampling then to 6  6 for a fair comparison to [7].\n",
    "```\n",
    "\n",
    "Dans leur cas n'utilise pas la dernire couche\n",
    "\n",
    "Mme si on pourrait nous essayer de l'utiliser car peut tre suffisant\n",
    "Peut tre essayer les deux avec et sans pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_area(pos):\n",
    "    height = pos[:, :, 3] - pos[:, :, 2]\n",
    "    width = pos[:, :, 1] - pos[:, :, 0]\n",
    "    area = height * width\n",
    "    return area\n",
    "\n",
    "get_area(boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VLT5 with only image or only text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pgrimal/miniconda3/envs/cvlp2/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at data_model/t5_pretrained were not used when initializing JointEncoder: ['shared.weight', 'encoder.embed_tokens.weight', 'encoder.block.0.layer.0.SelfAttention.q.weight', 'encoder.block.0.layer.0.SelfAttention.k.weight', 'encoder.block.0.layer.0.SelfAttention.v.weight', 'encoder.block.0.layer.0.SelfAttention.o.weight', 'encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'encoder.block.0.layer.0.layer_norm.weight', 'encoder.block.0.layer.1.DenseReluDense.wi.weight', 'encoder.block.0.layer.1.DenseReluDense.wo.weight', 'encoder.block.0.layer.1.layer_norm.weight', 'encoder.block.1.layer.0.SelfAttention.q.weight', 'encoder.block.1.layer.0.SelfAttention.k.weight', 'encoder.block.1.layer.0.SelfAttention.v.weight', 'encoder.block.1.layer.0.SelfAttention.o.weight', 'encoder.block.1.layer.0.layer_norm.weight', 'encoder.block.1.layer.1.DenseReluDense.wi.weight', 'encoder.block.1.layer.1.DenseReluDense.wo.weight', 'encoder.block.1.layer.1.layer_norm.weight', 'encoder.block.2.layer.0.SelfAttention.q.weight', 'encoder.block.2.layer.0.SelfAttention.k.weight', 'encoder.block.2.layer.0.SelfAttention.v.weight', 'encoder.block.2.layer.0.SelfAttention.o.weight', 'encoder.block.2.layer.0.layer_norm.weight', 'encoder.block.2.layer.1.DenseReluDense.wi.weight', 'encoder.block.2.layer.1.DenseReluDense.wo.weight', 'encoder.block.2.layer.1.layer_norm.weight', 'encoder.block.3.layer.0.SelfAttention.q.weight', 'encoder.block.3.layer.0.SelfAttention.k.weight', 'encoder.block.3.layer.0.SelfAttention.v.weight', 'encoder.block.3.layer.0.SelfAttention.o.weight', 'encoder.block.3.layer.0.layer_norm.weight', 'encoder.block.3.layer.1.DenseReluDense.wi.weight', 'encoder.block.3.layer.1.DenseReluDense.wo.weight', 'encoder.block.3.layer.1.layer_norm.weight', 'encoder.block.4.layer.0.SelfAttention.q.weight', 'encoder.block.4.layer.0.SelfAttention.k.weight', 'encoder.block.4.layer.0.SelfAttention.v.weight', 'encoder.block.4.layer.0.SelfAttention.o.weight', 'encoder.block.4.layer.0.layer_norm.weight', 'encoder.block.4.layer.1.DenseReluDense.wi.weight', 'encoder.block.4.layer.1.DenseReluDense.wo.weight', 'encoder.block.4.layer.1.layer_norm.weight', 'encoder.block.5.layer.0.SelfAttention.q.weight', 'encoder.block.5.layer.0.SelfAttention.k.weight', 'encoder.block.5.layer.0.SelfAttention.v.weight', 'encoder.block.5.layer.0.SelfAttention.o.weight', 'encoder.block.5.layer.0.layer_norm.weight', 'encoder.block.5.layer.1.DenseReluDense.wi.weight', 'encoder.block.5.layer.1.DenseReluDense.wo.weight', 'encoder.block.5.layer.1.layer_norm.weight', 'encoder.block.6.layer.0.SelfAttention.q.weight', 'encoder.block.6.layer.0.SelfAttention.k.weight', 'encoder.block.6.layer.0.SelfAttention.v.weight', 'encoder.block.6.layer.0.SelfAttention.o.weight', 'encoder.block.6.layer.0.layer_norm.weight', 'encoder.block.6.layer.1.DenseReluDense.wi.weight', 'encoder.block.6.layer.1.DenseReluDense.wo.weight', 'encoder.block.6.layer.1.layer_norm.weight', 'encoder.block.7.layer.0.SelfAttention.q.weight', 'encoder.block.7.layer.0.SelfAttention.k.weight', 'encoder.block.7.layer.0.SelfAttention.v.weight', 'encoder.block.7.layer.0.SelfAttention.o.weight', 'encoder.block.7.layer.0.layer_norm.weight', 'encoder.block.7.layer.1.DenseReluDense.wi.weight', 'encoder.block.7.layer.1.DenseReluDense.wo.weight', 'encoder.block.7.layer.1.layer_norm.weight', 'encoder.block.8.layer.0.SelfAttention.q.weight', 'encoder.block.8.layer.0.SelfAttention.k.weight', 'encoder.block.8.layer.0.SelfAttention.v.weight', 'encoder.block.8.layer.0.SelfAttention.o.weight', 'encoder.block.8.layer.0.layer_norm.weight', 'encoder.block.8.layer.1.DenseReluDense.wi.weight', 'encoder.block.8.layer.1.DenseReluDense.wo.weight', 'encoder.block.8.layer.1.layer_norm.weight', 'encoder.block.9.layer.0.SelfAttention.q.weight', 'encoder.block.9.layer.0.SelfAttention.k.weight', 'encoder.block.9.layer.0.SelfAttention.v.weight', 'encoder.block.9.layer.0.SelfAttention.o.weight', 'encoder.block.9.layer.0.layer_norm.weight', 'encoder.block.9.layer.1.DenseReluDense.wi.weight', 'encoder.block.9.layer.1.DenseReluDense.wo.weight', 'encoder.block.9.layer.1.layer_norm.weight', 'encoder.block.10.layer.0.SelfAttention.q.weight', 'encoder.block.10.layer.0.SelfAttention.k.weight', 'encoder.block.10.layer.0.SelfAttention.v.weight', 'encoder.block.10.layer.0.SelfAttention.o.weight', 'encoder.block.10.layer.0.layer_norm.weight', 'encoder.block.10.layer.1.DenseReluDense.wi.weight', 'encoder.block.10.layer.1.DenseReluDense.wo.weight', 'encoder.block.10.layer.1.layer_norm.weight', 'encoder.block.11.layer.0.SelfAttention.q.weight', 'encoder.block.11.layer.0.SelfAttention.k.weight', 'encoder.block.11.layer.0.SelfAttention.v.weight', 'encoder.block.11.layer.0.SelfAttention.o.weight', 'encoder.block.11.layer.0.layer_norm.weight', 'encoder.block.11.layer.1.DenseReluDense.wi.weight', 'encoder.block.11.layer.1.DenseReluDense.wo.weight', 'encoder.block.11.layer.1.layer_norm.weight', 'encoder.final_layer_norm.weight', 'decoder.embed_tokens.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.final_layer_norm.weight', 'lm_head.weight']\n",
      "- This IS expected if you are initializing JointEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing JointEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of JointEncoder were not initialized from the model checkpoint at data_model/t5_pretrained and are newly initialized: ['embed_tokens.weight', 'block.0.layer.0.SelfAttention.q.weight', 'block.0.layer.0.SelfAttention.k.weight', 'block.0.layer.0.SelfAttention.v.weight', 'block.0.layer.0.SelfAttention.o.weight', 'block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'block.0.layer.0.layer_norm.weight', 'block.0.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.0.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.0.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.0.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.0.layer.1.DenseReluDense.wi.weight', 'block.0.layer.1.DenseReluDense.wo.weight', 'block.0.layer.1.layer_norm.weight', 'block.0.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.0.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.0.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.0.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.1.layer.0.SelfAttention.q.weight', 'block.1.layer.0.SelfAttention.k.weight', 'block.1.layer.0.SelfAttention.v.weight', 'block.1.layer.0.SelfAttention.o.weight', 'block.1.layer.0.layer_norm.weight', 'block.1.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.1.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.1.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.1.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.1.layer.1.DenseReluDense.wi.weight', 'block.1.layer.1.DenseReluDense.wo.weight', 'block.1.layer.1.layer_norm.weight', 'block.1.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.1.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.1.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.1.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.2.layer.0.SelfAttention.q.weight', 'block.2.layer.0.SelfAttention.k.weight', 'block.2.layer.0.SelfAttention.v.weight', 'block.2.layer.0.SelfAttention.o.weight', 'block.2.layer.0.layer_norm.weight', 'block.2.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.2.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.2.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.2.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.2.layer.1.DenseReluDense.wi.weight', 'block.2.layer.1.DenseReluDense.wo.weight', 'block.2.layer.1.layer_norm.weight', 'block.2.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.2.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.2.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.2.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.3.layer.0.SelfAttention.q.weight', 'block.3.layer.0.SelfAttention.k.weight', 'block.3.layer.0.SelfAttention.v.weight', 'block.3.layer.0.SelfAttention.o.weight', 'block.3.layer.0.layer_norm.weight', 'block.3.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.3.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.3.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.3.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.3.layer.1.DenseReluDense.wi.weight', 'block.3.layer.1.DenseReluDense.wo.weight', 'block.3.layer.1.layer_norm.weight', 'block.3.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.3.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.3.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.3.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.4.layer.0.SelfAttention.q.weight', 'block.4.layer.0.SelfAttention.k.weight', 'block.4.layer.0.SelfAttention.v.weight', 'block.4.layer.0.SelfAttention.o.weight', 'block.4.layer.0.layer_norm.weight', 'block.4.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.4.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.4.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.4.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.4.layer.1.DenseReluDense.wi.weight', 'block.4.layer.1.DenseReluDense.wo.weight', 'block.4.layer.1.layer_norm.weight', 'block.4.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.4.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.4.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.4.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.5.layer.0.SelfAttention.q.weight', 'block.5.layer.0.SelfAttention.k.weight', 'block.5.layer.0.SelfAttention.v.weight', 'block.5.layer.0.SelfAttention.o.weight', 'block.5.layer.0.layer_norm.weight', 'block.5.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.5.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.5.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.5.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.5.layer.1.DenseReluDense.wi.weight', 'block.5.layer.1.DenseReluDense.wo.weight', 'block.5.layer.1.layer_norm.weight', 'block.5.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.5.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.5.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.5.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.6.layer.0.SelfAttention.q.weight', 'block.6.layer.0.SelfAttention.k.weight', 'block.6.layer.0.SelfAttention.v.weight', 'block.6.layer.0.SelfAttention.o.weight', 'block.6.layer.0.layer_norm.weight', 'block.6.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.6.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.6.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.6.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.6.layer.1.DenseReluDense.wi.weight', 'block.6.layer.1.DenseReluDense.wo.weight', 'block.6.layer.1.layer_norm.weight', 'block.6.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.6.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.6.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.6.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.7.layer.0.SelfAttention.q.weight', 'block.7.layer.0.SelfAttention.k.weight', 'block.7.layer.0.SelfAttention.v.weight', 'block.7.layer.0.SelfAttention.o.weight', 'block.7.layer.0.layer_norm.weight', 'block.7.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.7.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.7.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.7.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.7.layer.1.DenseReluDense.wi.weight', 'block.7.layer.1.DenseReluDense.wo.weight', 'block.7.layer.1.layer_norm.weight', 'block.7.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.7.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.7.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.7.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.8.layer.0.SelfAttention.q.weight', 'block.8.layer.0.SelfAttention.k.weight', 'block.8.layer.0.SelfAttention.v.weight', 'block.8.layer.0.SelfAttention.o.weight', 'block.8.layer.0.layer_norm.weight', 'block.8.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.8.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.8.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.8.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.8.layer.1.DenseReluDense.wi.weight', 'block.8.layer.1.DenseReluDense.wo.weight', 'block.8.layer.1.layer_norm.weight', 'block.8.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.8.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.8.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.8.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.9.layer.0.SelfAttention.q.weight', 'block.9.layer.0.SelfAttention.k.weight', 'block.9.layer.0.SelfAttention.v.weight', 'block.9.layer.0.SelfAttention.o.weight', 'block.9.layer.0.layer_norm.weight', 'block.9.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.9.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.9.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.9.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.9.layer.1.DenseReluDense.wi.weight', 'block.9.layer.1.DenseReluDense.wo.weight', 'block.9.layer.1.layer_norm.weight', 'block.9.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.9.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.9.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.9.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.10.layer.0.SelfAttention.q.weight', 'block.10.layer.0.SelfAttention.k.weight', 'block.10.layer.0.SelfAttention.v.weight', 'block.10.layer.0.SelfAttention.o.weight', 'block.10.layer.0.layer_norm.weight', 'block.10.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.10.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.10.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.10.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.10.layer.1.DenseReluDense.wi.weight', 'block.10.layer.1.DenseReluDense.wo.weight', 'block.10.layer.1.layer_norm.weight', 'block.10.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.10.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.10.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.10.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.11.layer.0.SelfAttention.q.weight', 'block.11.layer.0.SelfAttention.k.weight', 'block.11.layer.0.SelfAttention.v.weight', 'block.11.layer.0.SelfAttention.o.weight', 'block.11.layer.0.layer_norm.weight', 'block.11.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.11.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.11.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.11.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.11.layer.1.DenseReluDense.wi.weight', 'block.11.layer.1.DenseReluDense.wo.weight', 'block.11.layer.1.layer_norm.weight', 'block.11.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.11.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.11.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.11.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'final_layer_norm.weight', 'visual_embedding.feat_embedding.0.weight', 'visual_embedding.feat_embedding.0.bias', 'visual_embedding.feat_embedding.1.weight', 'visual_embedding.absolute_vis_pos_embedding.0.weight', 'visual_embedding.absolute_vis_pos_embedding.0.bias', 'visual_embedding.absolute_vis_pos_embedding.1.weight', 'visual_embedding.obj_order_embedding.weight', 'visual_embedding.img_order_embedding.weight', 'projection.projection.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at data_model/t5_pretrained were not used when initializing JointEncoder: ['shared.weight', 'encoder.embed_tokens.weight', 'encoder.block.0.layer.0.SelfAttention.q.weight', 'encoder.block.0.layer.0.SelfAttention.k.weight', 'encoder.block.0.layer.0.SelfAttention.v.weight', 'encoder.block.0.layer.0.SelfAttention.o.weight', 'encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'encoder.block.0.layer.0.layer_norm.weight', 'encoder.block.0.layer.1.DenseReluDense.wi.weight', 'encoder.block.0.layer.1.DenseReluDense.wo.weight', 'encoder.block.0.layer.1.layer_norm.weight', 'encoder.block.1.layer.0.SelfAttention.q.weight', 'encoder.block.1.layer.0.SelfAttention.k.weight', 'encoder.block.1.layer.0.SelfAttention.v.weight', 'encoder.block.1.layer.0.SelfAttention.o.weight', 'encoder.block.1.layer.0.layer_norm.weight', 'encoder.block.1.layer.1.DenseReluDense.wi.weight', 'encoder.block.1.layer.1.DenseReluDense.wo.weight', 'encoder.block.1.layer.1.layer_norm.weight', 'encoder.block.2.layer.0.SelfAttention.q.weight', 'encoder.block.2.layer.0.SelfAttention.k.weight', 'encoder.block.2.layer.0.SelfAttention.v.weight', 'encoder.block.2.layer.0.SelfAttention.o.weight', 'encoder.block.2.layer.0.layer_norm.weight', 'encoder.block.2.layer.1.DenseReluDense.wi.weight', 'encoder.block.2.layer.1.DenseReluDense.wo.weight', 'encoder.block.2.layer.1.layer_norm.weight', 'encoder.block.3.layer.0.SelfAttention.q.weight', 'encoder.block.3.layer.0.SelfAttention.k.weight', 'encoder.block.3.layer.0.SelfAttention.v.weight', 'encoder.block.3.layer.0.SelfAttention.o.weight', 'encoder.block.3.layer.0.layer_norm.weight', 'encoder.block.3.layer.1.DenseReluDense.wi.weight', 'encoder.block.3.layer.1.DenseReluDense.wo.weight', 'encoder.block.3.layer.1.layer_norm.weight', 'encoder.block.4.layer.0.SelfAttention.q.weight', 'encoder.block.4.layer.0.SelfAttention.k.weight', 'encoder.block.4.layer.0.SelfAttention.v.weight', 'encoder.block.4.layer.0.SelfAttention.o.weight', 'encoder.block.4.layer.0.layer_norm.weight', 'encoder.block.4.layer.1.DenseReluDense.wi.weight', 'encoder.block.4.layer.1.DenseReluDense.wo.weight', 'encoder.block.4.layer.1.layer_norm.weight', 'encoder.block.5.layer.0.SelfAttention.q.weight', 'encoder.block.5.layer.0.SelfAttention.k.weight', 'encoder.block.5.layer.0.SelfAttention.v.weight', 'encoder.block.5.layer.0.SelfAttention.o.weight', 'encoder.block.5.layer.0.layer_norm.weight', 'encoder.block.5.layer.1.DenseReluDense.wi.weight', 'encoder.block.5.layer.1.DenseReluDense.wo.weight', 'encoder.block.5.layer.1.layer_norm.weight', 'encoder.block.6.layer.0.SelfAttention.q.weight', 'encoder.block.6.layer.0.SelfAttention.k.weight', 'encoder.block.6.layer.0.SelfAttention.v.weight', 'encoder.block.6.layer.0.SelfAttention.o.weight', 'encoder.block.6.layer.0.layer_norm.weight', 'encoder.block.6.layer.1.DenseReluDense.wi.weight', 'encoder.block.6.layer.1.DenseReluDense.wo.weight', 'encoder.block.6.layer.1.layer_norm.weight', 'encoder.block.7.layer.0.SelfAttention.q.weight', 'encoder.block.7.layer.0.SelfAttention.k.weight', 'encoder.block.7.layer.0.SelfAttention.v.weight', 'encoder.block.7.layer.0.SelfAttention.o.weight', 'encoder.block.7.layer.0.layer_norm.weight', 'encoder.block.7.layer.1.DenseReluDense.wi.weight', 'encoder.block.7.layer.1.DenseReluDense.wo.weight', 'encoder.block.7.layer.1.layer_norm.weight', 'encoder.block.8.layer.0.SelfAttention.q.weight', 'encoder.block.8.layer.0.SelfAttention.k.weight', 'encoder.block.8.layer.0.SelfAttention.v.weight', 'encoder.block.8.layer.0.SelfAttention.o.weight', 'encoder.block.8.layer.0.layer_norm.weight', 'encoder.block.8.layer.1.DenseReluDense.wi.weight', 'encoder.block.8.layer.1.DenseReluDense.wo.weight', 'encoder.block.8.layer.1.layer_norm.weight', 'encoder.block.9.layer.0.SelfAttention.q.weight', 'encoder.block.9.layer.0.SelfAttention.k.weight', 'encoder.block.9.layer.0.SelfAttention.v.weight', 'encoder.block.9.layer.0.SelfAttention.o.weight', 'encoder.block.9.layer.0.layer_norm.weight', 'encoder.block.9.layer.1.DenseReluDense.wi.weight', 'encoder.block.9.layer.1.DenseReluDense.wo.weight', 'encoder.block.9.layer.1.layer_norm.weight', 'encoder.block.10.layer.0.SelfAttention.q.weight', 'encoder.block.10.layer.0.SelfAttention.k.weight', 'encoder.block.10.layer.0.SelfAttention.v.weight', 'encoder.block.10.layer.0.SelfAttention.o.weight', 'encoder.block.10.layer.0.layer_norm.weight', 'encoder.block.10.layer.1.DenseReluDense.wi.weight', 'encoder.block.10.layer.1.DenseReluDense.wo.weight', 'encoder.block.10.layer.1.layer_norm.weight', 'encoder.block.11.layer.0.SelfAttention.q.weight', 'encoder.block.11.layer.0.SelfAttention.k.weight', 'encoder.block.11.layer.0.SelfAttention.v.weight', 'encoder.block.11.layer.0.SelfAttention.o.weight', 'encoder.block.11.layer.0.layer_norm.weight', 'encoder.block.11.layer.1.DenseReluDense.wi.weight', 'encoder.block.11.layer.1.DenseReluDense.wo.weight', 'encoder.block.11.layer.1.layer_norm.weight', 'encoder.final_layer_norm.weight', 'decoder.embed_tokens.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.final_layer_norm.weight', 'lm_head.weight']\n",
      "- This IS expected if you are initializing JointEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing JointEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of JointEncoder were not initialized from the model checkpoint at data_model/t5_pretrained and are newly initialized: ['embed_tokens.weight', 'block.0.layer.0.SelfAttention.q.weight', 'block.0.layer.0.SelfAttention.k.weight', 'block.0.layer.0.SelfAttention.v.weight', 'block.0.layer.0.SelfAttention.o.weight', 'block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'block.0.layer.0.layer_norm.weight', 'block.0.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.0.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.0.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.0.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.0.layer.1.DenseReluDense.wi.weight', 'block.0.layer.1.DenseReluDense.wo.weight', 'block.0.layer.1.layer_norm.weight', 'block.0.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.0.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.0.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.0.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.1.layer.0.SelfAttention.q.weight', 'block.1.layer.0.SelfAttention.k.weight', 'block.1.layer.0.SelfAttention.v.weight', 'block.1.layer.0.SelfAttention.o.weight', 'block.1.layer.0.layer_norm.weight', 'block.1.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.1.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.1.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.1.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.1.layer.1.DenseReluDense.wi.weight', 'block.1.layer.1.DenseReluDense.wo.weight', 'block.1.layer.1.layer_norm.weight', 'block.1.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.1.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.1.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.1.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.2.layer.0.SelfAttention.q.weight', 'block.2.layer.0.SelfAttention.k.weight', 'block.2.layer.0.SelfAttention.v.weight', 'block.2.layer.0.SelfAttention.o.weight', 'block.2.layer.0.layer_norm.weight', 'block.2.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.2.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.2.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.2.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.2.layer.1.DenseReluDense.wi.weight', 'block.2.layer.1.DenseReluDense.wo.weight', 'block.2.layer.1.layer_norm.weight', 'block.2.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.2.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.2.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.2.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.3.layer.0.SelfAttention.q.weight', 'block.3.layer.0.SelfAttention.k.weight', 'block.3.layer.0.SelfAttention.v.weight', 'block.3.layer.0.SelfAttention.o.weight', 'block.3.layer.0.layer_norm.weight', 'block.3.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.3.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.3.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.3.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.3.layer.1.DenseReluDense.wi.weight', 'block.3.layer.1.DenseReluDense.wo.weight', 'block.3.layer.1.layer_norm.weight', 'block.3.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.3.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.3.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.3.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.4.layer.0.SelfAttention.q.weight', 'block.4.layer.0.SelfAttention.k.weight', 'block.4.layer.0.SelfAttention.v.weight', 'block.4.layer.0.SelfAttention.o.weight', 'block.4.layer.0.layer_norm.weight', 'block.4.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.4.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.4.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.4.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.4.layer.1.DenseReluDense.wi.weight', 'block.4.layer.1.DenseReluDense.wo.weight', 'block.4.layer.1.layer_norm.weight', 'block.4.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.4.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.4.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.4.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.5.layer.0.SelfAttention.q.weight', 'block.5.layer.0.SelfAttention.k.weight', 'block.5.layer.0.SelfAttention.v.weight', 'block.5.layer.0.SelfAttention.o.weight', 'block.5.layer.0.layer_norm.weight', 'block.5.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.5.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.5.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.5.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.5.layer.1.DenseReluDense.wi.weight', 'block.5.layer.1.DenseReluDense.wo.weight', 'block.5.layer.1.layer_norm.weight', 'block.5.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.5.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.5.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.5.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.6.layer.0.SelfAttention.q.weight', 'block.6.layer.0.SelfAttention.k.weight', 'block.6.layer.0.SelfAttention.v.weight', 'block.6.layer.0.SelfAttention.o.weight', 'block.6.layer.0.layer_norm.weight', 'block.6.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.6.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.6.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.6.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.6.layer.1.DenseReluDense.wi.weight', 'block.6.layer.1.DenseReluDense.wo.weight', 'block.6.layer.1.layer_norm.weight', 'block.6.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.6.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.6.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.6.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.7.layer.0.SelfAttention.q.weight', 'block.7.layer.0.SelfAttention.k.weight', 'block.7.layer.0.SelfAttention.v.weight', 'block.7.layer.0.SelfAttention.o.weight', 'block.7.layer.0.layer_norm.weight', 'block.7.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.7.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.7.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.7.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.7.layer.1.DenseReluDense.wi.weight', 'block.7.layer.1.DenseReluDense.wo.weight', 'block.7.layer.1.layer_norm.weight', 'block.7.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.7.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.7.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.7.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.8.layer.0.SelfAttention.q.weight', 'block.8.layer.0.SelfAttention.k.weight', 'block.8.layer.0.SelfAttention.v.weight', 'block.8.layer.0.SelfAttention.o.weight', 'block.8.layer.0.layer_norm.weight', 'block.8.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.8.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.8.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.8.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.8.layer.1.DenseReluDense.wi.weight', 'block.8.layer.1.DenseReluDense.wo.weight', 'block.8.layer.1.layer_norm.weight', 'block.8.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.8.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.8.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.8.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.9.layer.0.SelfAttention.q.weight', 'block.9.layer.0.SelfAttention.k.weight', 'block.9.layer.0.SelfAttention.v.weight', 'block.9.layer.0.SelfAttention.o.weight', 'block.9.layer.0.layer_norm.weight', 'block.9.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.9.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.9.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.9.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.9.layer.1.DenseReluDense.wi.weight', 'block.9.layer.1.DenseReluDense.wo.weight', 'block.9.layer.1.layer_norm.weight', 'block.9.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.9.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.9.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.9.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.10.layer.0.SelfAttention.q.weight', 'block.10.layer.0.SelfAttention.k.weight', 'block.10.layer.0.SelfAttention.v.weight', 'block.10.layer.0.SelfAttention.o.weight', 'block.10.layer.0.layer_norm.weight', 'block.10.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.10.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.10.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.10.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.10.layer.1.DenseReluDense.wi.weight', 'block.10.layer.1.DenseReluDense.wo.weight', 'block.10.layer.1.layer_norm.weight', 'block.10.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.10.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.10.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.10.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.11.layer.0.SelfAttention.q.weight', 'block.11.layer.0.SelfAttention.k.weight', 'block.11.layer.0.SelfAttention.v.weight', 'block.11.layer.0.SelfAttention.o.weight', 'block.11.layer.0.layer_norm.weight', 'block.11.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.11.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.11.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.11.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.11.layer.1.DenseReluDense.wi.weight', 'block.11.layer.1.DenseReluDense.wo.weight', 'block.11.layer.1.layer_norm.weight', 'block.11.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.11.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.11.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.11.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'final_layer_norm.weight', 'visual_embedding.feat_embedding.0.weight', 'visual_embedding.feat_embedding.0.bias', 'visual_embedding.feat_embedding.1.weight', 'visual_embedding.absolute_vis_pos_embedding.0.weight', 'visual_embedding.absolute_vis_pos_embedding.0.bias', 'visual_embedding.absolute_vis_pos_embedding.1.weight', 'visual_embedding.obj_order_embedding.weight', 'visual_embedding.img_order_embedding.weight', 'projection.projection.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from  data_model/jointEncoder/state_dict/VLT5epoch30.pth\n",
      "_IncompatibleKeys(missing_keys=['block.0.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.0.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.0.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.0.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.0.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.0.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.0.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.0.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.1.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.1.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.1.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.1.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.1.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.1.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.1.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.1.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.2.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.2.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.2.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.2.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.2.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.2.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.2.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.2.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.3.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.3.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.3.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.3.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.3.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.3.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.3.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.3.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.4.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.4.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.4.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.4.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.4.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.4.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.4.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.4.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.5.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.5.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.5.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.5.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.5.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.5.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.5.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.5.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.6.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.6.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.6.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.6.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.6.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.6.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.6.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.6.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.7.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.7.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.7.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.7.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.7.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.7.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.7.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.7.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.8.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.8.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.8.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.8.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.8.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.8.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.8.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.8.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.9.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.9.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.9.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.9.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.9.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.9.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.9.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.9.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.10.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.10.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.10.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.10.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.10.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.10.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.10.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.10.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.11.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.11.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.11.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.11.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.11.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.11.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.11.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.11.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'projection.projection.weight'], unexpected_keys=[])\n",
      "Model loaded from  data_model/jointEncoder/state_dict/VLT5epoch30.pth\n",
      "_IncompatibleKeys(missing_keys=['block.0.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.0.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.0.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.0.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.0.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.0.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.0.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.0.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.1.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.1.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.1.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.1.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.1.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.1.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.1.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.1.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.2.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.2.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.2.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.2.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.2.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.2.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.2.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.2.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.3.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.3.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.3.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.3.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.3.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.3.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.3.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.3.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.4.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.4.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.4.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.4.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.4.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.4.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.4.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.4.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.5.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.5.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.5.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.5.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.5.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.5.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.5.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.5.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.6.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.6.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.6.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.6.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.6.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.6.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.6.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.6.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.7.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.7.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.7.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.7.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.7.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.7.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.7.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.7.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.8.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.8.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.8.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.8.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.8.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.8.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.8.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.8.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.9.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.9.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.9.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.9.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.9.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.9.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.9.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.9.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.10.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.10.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.10.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.10.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.10.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.10.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.10.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.10.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'block.11.layer.0.attn_adapter.adapters.IR.down_sampler.weight', 'block.11.layer.0.attn_adapter.adapters.IR.down_sampler.bias', 'block.11.layer.0.attn_adapter.adapters.IR.up_sampler.weight', 'block.11.layer.0.attn_adapter.adapters.IR.up_sampler.bias', 'block.11.layer.1.ff_adapter.adapters.IR.down_sampler.weight', 'block.11.layer.1.ff_adapter.adapters.IR.down_sampler.bias', 'block.11.layer.1.ff_adapter.adapters.IR.up_sampler.weight', 'block.11.layer.1.ff_adapter.adapters.IR.up_sampler.bias', 'projection.projection.weight'], unexpected_keys=[])\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "import os\n",
    "from cvlep.VLT5.param import Config\n",
    "\n",
    "if \"RANK\" in os.environ and \"WORLD_SIZE\" in os.environ:  # torchrun launch\n",
    "    rank = int(os.environ[\"RANK\"])\n",
    "    local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "    world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "elif int(os.environ.get('SLURM_NPROCS', 1)) > 1:  # slurm launch\n",
    "    rank = int(os.environ[\"SLURM_PROCID\"])\n",
    "    local_rank = int(os.environ[\"SLURM_LOCALID\"])\n",
    "    world_size = int(os.environ[\"SLURM_NPROCS\"])\n",
    "else:  # single gpu & process launch\n",
    "    rank = 0\n",
    "    local_rank = 0\n",
    "    world_size = 0\n",
    "\n",
    "config_encoder_question = \"experiments/config_vladapter/local/encoder_simple.json\"\n",
    "config_encoder_passage = \"experiments/config_vladapter/local/encoder_simple.json\"\n",
    "config_model = \"experiments/config_vladapter/local/config_model.json\"\n",
    "config_training = \"experiments/config_vladapter/local/training_simple_adapter.json\"\n",
    "\n",
    "# Training config\n",
    "config_training = Config.load_json(config_training)\n",
    "config_training.world_size = world_size\n",
    "config_training.rank = rank\n",
    "config_training.local_rank = local_rank\n",
    "if world_size > 1:\n",
    "    config_training.distributed = True\n",
    "    config_training.multiGPU = True\n",
    "else:\n",
    "    config_training.distributed = False\n",
    "    config_training.multiGPU = False\n",
    "\n",
    "config_encoder_question = Config.load_json(config_encoder_question)\n",
    "config_encoder_passage = Config.load_json(config_encoder_passage)\n",
    "config_model = Config.load_json(config_model)\n",
    "\n",
    "from cvlep.trainer_base_vladapter import Trainer\n",
    "\n",
    "trainer = Trainer(config_encoder_question,config_encoder_passage, config_model, config_training, train=False, local=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from datasets import load_from_disk, disable_caching\n",
    "disable_caching()\n",
    "path_dataset = \"data_model/dataset/small_dataset\"\n",
    "dataset = load_from_disk(path_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 11])\n",
      "torch.Size([1, 36, 2048])\n"
     ]
    }
   ],
   "source": [
    "# prepare inputs\n",
    "import torch\n",
    "from cvlep.utils import device\n",
    "index = 1\n",
    "item = dataset[1]\n",
    "key_vision_features = \"vlt5_features\"\n",
    "key_boxes = \"vlt5_normalized_boxes\"\n",
    "key_text=\"input\"\n",
    "tokenizer = trainer.tokenizer_question\n",
    "\n",
    "vision_features = torch.Tensor(item[key_vision_features]).to(device)\n",
    "boxes = torch.Tensor(item[key_boxes]).to(device)\n",
    "vision_features = torch.squeeze(vision_features, dim=1)\n",
    "vis_inputs = (vision_features,boxes)\n",
    "boxes = torch.squeeze(boxes, dim=2)\n",
    "input_ids = tokenizer(\n",
    "    item[key_text], return_tensors='pt', padding=True, truncation=True)\n",
    "return_pooled_output=True,\n",
    "pool_strategy=\"avg\"\n",
    "print(input_ids.input_ids.size())\n",
    "print(vision_features.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = {}\n",
    "batch.update(input_ids = input_ids.input_ids.to(device),\n",
    "    attention_mask = input_ids.attention_mask.to(device),\n",
    "    vis_inputs = vis_inputs,\n",
    "    pool_strategy = pool_strategy,\n",
    "    return_pooled_output = return_pooled_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.encoder_question.eval()\n",
    "output = trainer.encoder_question(\n",
    "        input_ids=batch['input_ids'],\n",
    "        attention_mask=batch['attention_mask'],\n",
    "\n",
    "        vis_inputs=None,\n",
    "        vis_attention_mask=None,\n",
    "        task ='IR',\n",
    "        return_pooled_output=True,\n",
    "        pool_strategy=\"avg\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.1674, -0.1520, -0.0951,  ...,  0.0763, -0.0916,  0.4618],\n",
       "         [-0.3441, -0.1762, -0.2591,  ..., -0.1362, -0.0674,  0.1027],\n",
       "         [-0.4702, -0.1144, -0.3313,  ..., -0.1278, -0.0036,  0.0587],\n",
       "         ...,\n",
       "         [-0.0994, -0.0885, -0.0502,  ..., -0.2866,  0.2314,  0.3491],\n",
       "         [ 0.3561, -0.0430, -0.0020,  ..., -0.1969, -0.1524,  0.0317],\n",
       "         [ 0.0424,  0.0197,  0.0048,  ..., -0.0328, -0.0645,  0.0272]]],\n",
       "       grad_fn=<MulBackward0>), pooler_output=tensor([[ 1.3825e-02,  1.6060e-02,  1.0948e-01,  1.4112e-02, -4.7519e-02,\n",
       "          3.2587e-02,  2.9294e-03, -5.1448e-03,  6.1910e-03, -1.0511e-02,\n",
       "         -2.8557e-02, -2.5254e-02, -2.6999e-02,  2.1565e-02, -4.2359e-02,\n",
       "         -9.5461e-03,  7.9472e-03, -6.6379e-03,  2.9980e-02,  7.4853e-02,\n",
       "         -1.2898e-02, -7.8000e-02, -2.8604e-02,  4.5789e-02,  4.7480e-03,\n",
       "          1.4054e-02, -1.9011e-02, -2.4584e-02,  5.3269e-02, -9.4996e-03,\n",
       "          3.3811e-02, -3.3232e-02,  1.6302e-02,  1.4407e-02, -7.8989e-03,\n",
       "         -2.9916e-02,  2.3427e-03,  8.1735e-04, -3.3261e-02,  1.8592e-02,\n",
       "          2.5504e-02, -4.3749e-02, -7.3690e-02, -1.4730e-02, -2.2745e-02,\n",
       "         -1.4255e-02, -2.2711e-02,  1.0448e-02,  3.8984e-02, -5.4083e-02,\n",
       "         -4.1501e-02,  2.3711e-02, -2.8216e-03, -6.1461e-02,  6.0400e-02,\n",
       "         -4.9822e-02,  2.3609e-02,  4.6734e-02, -2.8204e-02, -4.3744e-02,\n",
       "          1.1402e-02,  3.4327e-02,  3.3013e-02,  8.2030e-02,  2.7538e-02,\n",
       "         -3.8761e-02,  4.0449e-02,  1.9467e-02, -2.9454e-03,  1.6281e-02,\n",
       "          1.1701e-03, -2.8879e-02, -4.4263e-02, -6.1799e-02,  2.7403e-02,\n",
       "          1.9877e-02,  1.1887e-02,  1.6633e-02,  4.2815e-02, -2.9425e-02,\n",
       "         -2.5312e-03,  3.7038e-02,  3.3879e-02,  1.5167e-03, -4.5482e-02,\n",
       "         -3.1353e-02,  3.2878e-02, -3.5561e-02,  7.8371e-03, -2.9615e-02,\n",
       "          5.4643e-03,  1.1028e-02,  1.8421e-02, -1.6476e-02,  1.0203e-03,\n",
       "          4.3170e-03,  1.1456e-04, -3.2262e-02,  1.1924e-02,  3.6530e-03,\n",
       "          3.7615e-02,  2.1133e-02, -4.0418e-02, -1.1865e-02, -2.9250e-03,\n",
       "          2.4320e-02,  2.6096e-02, -2.1072e-02,  3.3873e-02, -4.4379e-02,\n",
       "          3.5225e-03,  4.5090e-02,  7.8939e-03,  3.8398e-02, -5.2656e-02,\n",
       "          6.3189e-03, -2.1494e-02,  3.8455e-02, -2.7225e-02,  3.3801e-02,\n",
       "          5.0836e-02, -7.7045e-03,  5.2430e-02,  2.3820e-02, -5.4614e-02,\n",
       "          2.9350e-02,  1.6746e-03,  5.1343e-02,  2.7329e-03,  5.7651e-02,\n",
       "         -5.6972e-02,  4.1885e-02,  7.4586e-02, -3.7915e-02, -1.8122e-02,\n",
       "          2.8980e-02, -1.0791e-02, -1.7115e-03,  5.4023e-02, -2.7140e-02,\n",
       "         -1.3973e-02, -2.5612e-02, -4.7713e-02,  1.1002e-01, -2.3268e-02,\n",
       "         -7.0879e-02, -9.7969e-03, -7.5492e-03, -5.3356e-02,  5.8203e-02,\n",
       "          2.8065e-02, -2.7700e-02, -4.4697e-03,  3.8999e-02,  1.5246e-02,\n",
       "         -1.7979e-02, -4.4919e-02, -1.4579e-02,  3.5846e-02, -1.5021e-02,\n",
       "         -1.0205e-02,  2.8319e-02, -3.9984e-02, -7.6640e-02,  5.9133e-02,\n",
       "         -1.2916e-03, -1.8601e-02, -4.2650e-03, -4.8496e-02, -1.3339e-02,\n",
       "          5.9268e-02, -3.6634e-02,  3.0665e-02,  8.7296e-03, -1.5803e-02,\n",
       "         -2.0736e-02,  2.3813e-03, -3.5014e-02, -2.5396e-02, -1.4119e-02,\n",
       "          7.8341e-02, -1.2480e-02, -3.0809e-02, -1.3571e-03, -1.5996e-02,\n",
       "          5.8924e-02,  4.0736e-02,  1.5763e-02, -2.2969e-02, -1.4438e-02,\n",
       "          1.3933e-02, -2.2412e-02,  4.8588e-02, -4.6762e-02,  4.5482e-02,\n",
       "          6.3656e-02,  7.8189e-02, -2.5829e-02, -1.1359e-04,  2.9791e-02,\n",
       "         -3.7357e-03,  6.0881e-02, -8.9607e-03,  1.2326e-03, -2.4528e-02,\n",
       "         -5.3899e-02,  3.1907e-02,  5.0231e-02, -1.2292e-02, -9.9316e-03,\n",
       "         -8.0555e-03, -5.4912e-02,  2.4384e-02, -1.8613e-02,  1.0512e-02,\n",
       "         -2.1273e-02, -6.7394e-03,  9.9238e-03, -2.5785e-03, -4.5477e-03,\n",
       "         -6.4276e-02, -2.8059e-02,  4.7882e-02, -6.6890e-02,  1.9184e-02,\n",
       "         -5.1266e-02,  5.1509e-02, -1.1868e-02, -5.9483e-02, -3.3233e-02,\n",
       "         -1.5657e-02,  2.9001e-02, -4.9688e-02,  4.5132e-02, -8.7432e-03,\n",
       "          2.7539e-03,  3.1415e-02, -2.2215e-03, -2.0950e-02,  1.7307e-02,\n",
       "          2.7594e-02, -4.3409e-02,  4.5511e-02, -6.7728e-02,  4.3818e-02,\n",
       "         -4.2348e-02,  4.4712e-02, -9.9772e-02,  5.7732e-03,  1.5149e-02,\n",
       "         -1.7827e-02,  9.4587e-02,  3.1151e-02, -2.1502e-02,  3.4521e-02,\n",
       "          2.2358e-02, -1.4665e-02, -3.1773e-02,  9.2491e-03,  8.2485e-02,\n",
       "         -6.0706e-02, -7.2325e-02, -5.6317e-03,  3.9011e-02, -1.2828e-02,\n",
       "          8.0642e-03,  4.1454e-02,  5.9927e-03, -3.7033e-02,  2.0551e-02,\n",
       "          1.6361e-02, -3.2697e-02,  3.2948e-02,  1.9258e-02,  5.1536e-02,\n",
       "          5.4966e-03, -2.0188e-02, -7.8812e-02,  1.7762e-02, -4.3991e-02,\n",
       "         -2.0707e-02,  4.4804e-02, -4.8859e-03, -2.8708e-03,  8.0328e-03,\n",
       "         -1.0603e-02,  6.2902e-02,  2.5306e-02, -1.6397e-02, -4.5611e-03,\n",
       "          3.5330e-03, -1.0928e-02,  4.2420e-02, -7.9832e-02, -8.2451e-02,\n",
       "          3.2626e-02,  6.2756e-02, -5.6973e-02,  3.9236e-02,  2.1242e-02,\n",
       "          6.0306e-02, -5.2123e-02, -4.9360e-02, -2.1521e-02,  5.6854e-02,\n",
       "          9.0981e-02,  2.9522e-02,  1.9613e-02,  1.6741e-02, -3.5130e-02,\n",
       "         -2.9602e-02,  6.3220e-03,  8.8006e-03, -6.4878e-02, -4.3683e-03,\n",
       "          3.4266e-02, -2.9015e-02,  9.6329e-03,  2.8982e-02, -1.6152e-02,\n",
       "          1.9742e-02,  5.9847e-02,  4.9740e-02,  5.9528e-02,  2.6410e-02,\n",
       "          2.8766e-02,  1.8017e-02, -4.8365e-02,  2.1586e-02,  5.6551e-02,\n",
       "         -1.4831e-02, -4.0522e-02, -2.3863e-02, -4.5389e-02, -3.7142e-02,\n",
       "         -1.4817e-02,  4.2542e-02,  5.0169e-02, -1.9745e-02, -3.5238e-02,\n",
       "         -2.5137e-03,  1.9403e-02, -2.0980e-02,  1.2917e-02, -2.1321e-02,\n",
       "         -3.5085e-02,  1.5850e-02,  4.3951e-02, -4.4455e-02, -1.8184e-02,\n",
       "         -1.0514e-02,  3.2208e-02,  3.5699e-02, -9.2077e-02,  7.6140e-03,\n",
       "          4.0976e-02,  2.8255e-02, -3.5810e-02,  2.0296e-02, -3.1918e-02,\n",
       "          6.5680e-02,  1.1333e-02, -3.5639e-02, -8.0437e-02, -4.3676e-03,\n",
       "          9.3535e-02, -6.1188e-03,  7.3094e-03,  4.2842e-02, -2.5258e-02,\n",
       "          4.1819e-02, -1.3072e-02,  5.7066e-02, -4.6042e-03,  1.4505e-02,\n",
       "         -1.7269e-02,  3.2397e-02, -4.8038e-03, -7.4666e-04,  1.6248e-02,\n",
       "         -1.2702e-02, -3.3087e-04, -2.3795e-02, -3.1606e-02,  4.3190e-02,\n",
       "         -2.0106e-02, -3.0463e-02, -2.0704e-02,  3.0021e-02, -2.4876e-02,\n",
       "          2.9567e-02, -5.9429e-02, -3.4978e-02,  9.2874e-03, -1.3584e-02,\n",
       "         -4.9686e-02, -1.7612e-02,  3.8412e-02, -1.6689e-03,  5.7020e-02,\n",
       "         -2.7444e-02,  1.2359e-02, -8.9702e-02, -2.9444e-03, -3.5671e-03,\n",
       "         -3.2446e-03, -8.6923e-02,  9.2609e-03, -4.3810e-02,  4.7987e-02,\n",
       "         -2.9164e-02,  1.0419e-02, -4.6678e-03, -2.5897e-02, -2.1592e-02,\n",
       "         -2.9626e-02, -3.8812e-02,  1.3073e-03,  7.7058e-03,  5.0537e-02,\n",
       "          1.0757e-02, -2.9007e-03, -2.6436e-02,  2.1380e-02, -1.0530e-02,\n",
       "          3.0469e-02,  4.5421e-02,  1.1910e-02,  2.1542e-02, -3.0441e-03,\n",
       "          2.1778e-02,  1.5995e-02,  8.2721e-02, -1.9819e-02, -1.0206e-03,\n",
       "          4.3990e-03, -1.4454e-02, -3.9667e-02,  7.7948e-03,  7.6844e-02,\n",
       "         -4.8110e-02, -2.2599e-02,  2.1195e-02, -4.9723e-03, -3.9405e-02,\n",
       "         -5.9649e-02, -3.8701e-02,  4.1839e-02,  2.6744e-02,  3.8551e-02,\n",
       "         -1.1521e-02,  2.2596e-02,  2.1964e-02, -1.2686e-02,  6.0765e-02,\n",
       "         -8.9082e-03,  1.3528e-04, -4.5834e-02,  2.4957e-02,  3.2763e-02,\n",
       "         -3.7852e-02,  1.3692e-02,  6.4073e-03,  1.9353e-02, -3.6991e-03,\n",
       "         -3.3062e-02,  9.2450e-03,  1.0604e-02, -2.3840e-02,  1.3940e-02,\n",
       "         -1.1228e-03,  2.1913e-02, -4.5284e-02, -3.1699e-02,  1.4642e-02,\n",
       "          1.3601e-02, -2.7899e-02,  6.5754e-03,  8.1375e-03, -3.3566e-02,\n",
       "         -1.3045e-02,  3.4389e-03,  7.7235e-02, -1.1544e-02, -3.3154e-03,\n",
       "         -3.1015e-02,  4.2437e-02, -7.9510e-02,  3.0418e-02, -4.8533e-02,\n",
       "         -4.5815e-02,  3.8188e-02,  1.1242e-02, -3.0726e-02, -1.1645e-02,\n",
       "         -1.2814e-02, -2.6978e-02, -4.2418e-02, -2.7765e-02,  8.6254e-03,\n",
       "          4.1982e-02,  2.6903e-02,  1.9227e-02, -2.5651e-02, -1.1637e-02,\n",
       "          4.3830e-02, -1.8142e-02,  1.8626e-02, -1.7855e-02,  1.9320e-02,\n",
       "          3.9909e-02, -1.7457e-02, -4.7959e-02, -3.3782e-03,  5.4836e-02,\n",
       "          4.2449e-02,  5.4714e-03, -2.7140e-03, -4.4211e-02, -4.2034e-02,\n",
       "          1.9854e-02, -3.8455e-02, -6.6167e-02, -1.2112e-02, -1.5382e-02,\n",
       "          6.8876e-02,  3.2631e-02, -1.5812e-02, -1.3992e-02,  3.6660e-02,\n",
       "          2.0501e-02,  2.4372e-02,  2.3821e-03, -4.9558e-02,  5.4359e-02,\n",
       "         -1.9093e-02, -1.3415e-02,  3.7111e-02, -7.4902e-03,  1.8717e-03,\n",
       "          3.8110e-02, -2.9097e-02, -3.8533e-02, -1.2119e-02,  2.0579e-02,\n",
       "          1.7928e-02, -4.2189e-02,  6.7793e-02,  1.8408e-03,  3.1936e-02,\n",
       "          4.0380e-02, -8.5705e-03, -3.1987e-02, -2.3348e-02, -2.4913e-02,\n",
       "          3.6175e-02,  3.6519e-02, -1.5436e-02, -9.2700e-03,  1.3044e-02,\n",
       "         -2.9741e-02,  2.6418e-02,  2.3577e-02,  3.9725e-02,  4.2468e-02,\n",
       "          2.1347e-02,  5.9761e-03, -4.2805e-04,  3.4947e-02,  6.9515e-02,\n",
       "         -3.3010e-02, -4.8981e-02, -2.0168e-02, -4.1994e-02,  1.8551e-02,\n",
       "          2.6387e-02,  5.3666e-02,  8.6620e-03,  2.8503e-02, -3.0452e-02,\n",
       "          4.9398e-02, -3.2781e-02,  1.7171e-02, -1.2371e-02, -8.3590e-04,\n",
       "          4.2562e-02, -2.7059e-02, -8.3310e-03,  3.2384e-02,  7.8069e-02,\n",
       "          3.6310e-03, -2.6009e-02,  1.4700e-02,  4.3719e-02,  6.9543e-03,\n",
       "         -2.2135e-04,  2.2799e-02,  7.0898e-03, -6.4063e-02, -7.1115e-04,\n",
       "          9.3692e-03,  3.4853e-02,  5.1047e-02, -1.7533e-02,  2.4680e-02,\n",
       "         -3.2750e-02, -2.5483e-02, -4.5491e-03,  7.3467e-02,  1.3632e-03,\n",
       "         -3.3039e-02,  7.8467e-03,  3.5487e-02, -1.3200e-02, -1.2162e-02,\n",
       "         -9.3825e-02,  1.3861e-02,  6.5426e-02,  4.7518e-02, -4.7590e-02,\n",
       "          4.6078e-02, -1.1129e-02, -3.4585e-02,  5.6461e-03,  8.3957e-02,\n",
       "          2.4387e-02, -2.9418e-02, -2.8157e-02,  4.7388e-02,  4.5314e-03,\n",
       "         -2.0163e-02, -2.8309e-02, -2.7305e-02, -4.7805e-02, -2.4872e-02,\n",
       "          2.5169e-02, -4.8500e-02, -1.8346e-02, -3.8143e-02,  9.1235e-04,\n",
       "          3.6070e-02,  8.6353e-02,  9.2900e-02,  1.0195e-02,  3.5120e-02,\n",
       "          4.0760e-02, -1.0779e-02,  1.5556e-02, -3.4705e-02, -3.1744e-02,\n",
       "          2.7759e-02, -4.6400e-02,  1.0628e-02,  1.0468e-02, -1.4834e-02,\n",
       "          1.3920e-02,  4.7098e-02,  1.9446e-02,  9.0392e-03, -3.5109e-02,\n",
       "         -9.6622e-02, -8.0010e-03, -5.0859e-02,  2.0131e-02,  1.8651e-02,\n",
       "         -2.4953e-02, -6.4527e-03,  2.3103e-02, -6.7219e-02, -4.9870e-02,\n",
       "          7.4904e-03,  3.7271e-02, -3.9044e-02,  8.8170e-03,  1.7510e-02,\n",
       "         -5.4731e-02, -1.7680e-02,  1.9199e-02,  6.6751e-02,  4.3682e-02,\n",
       "          5.9188e-03, -4.2146e-02,  1.5491e-02,  5.9887e-02,  4.8076e-02,\n",
       "         -2.7278e-03,  3.6380e-02, -1.6499e-02,  5.8240e-02,  3.1383e-02,\n",
       "         -3.1827e-02, -3.0511e-02,  4.4156e-02, -1.7718e-03,  3.3056e-02,\n",
       "         -2.0609e-03,  8.5078e-03,  1.7759e-02,  2.6816e-02,  4.7652e-02,\n",
       "         -5.1022e-03,  8.5800e-03,  7.5641e-02, -1.9526e-02, -4.1754e-02,\n",
       "         -4.8265e-02, -4.4625e-03,  1.9352e-02, -2.6768e-02, -2.2431e-02,\n",
       "          6.6498e-02, -3.9898e-02,  3.2259e-02,  3.9802e-02,  8.7618e-04,\n",
       "          4.4234e-02,  9.0659e-03, -4.0719e-02, -1.4703e-02,  3.6604e-03,\n",
       "         -3.7741e-02,  1.8583e-02,  5.3746e-02,  4.8096e-02, -3.1667e-02,\n",
       "          1.5214e-02,  1.8558e-02, -5.1637e-02,  9.2612e-03, -6.5700e-02,\n",
       "          7.6068e-02,  3.8854e-03,  1.5812e-02,  3.2624e-02,  5.3062e-02,\n",
       "         -2.9225e-02,  8.8018e-03, -7.9513e-02, -4.5826e-02,  2.7062e-02,\n",
       "          4.6417e-02, -3.7972e-02, -2.3612e-02,  2.5999e-02,  2.4805e-02,\n",
       "         -9.1664e-03, -9.3878e-02, -7.0323e-02, -4.5816e-02, -9.3224e-03,\n",
       "          1.1496e-02,  3.6604e-02,  2.8666e-02, -5.3975e-03, -2.8957e-02,\n",
       "         -5.3128e-02,  1.4171e-02, -1.7366e-02,  4.6113e-04,  1.9226e-02,\n",
       "          1.5667e-02,  6.9123e-02, -2.4632e-02,  3.1424e-03,  5.1386e-02,\n",
       "          9.2801e-03,  3.1335e-02, -3.4979e-05]], grad_fn=<DivBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.1930, -0.0386, -0.1303,  ...,  0.2305, -0.0779,  0.4695],\n",
       "         [-0.3207, -0.2826, -0.3696,  ..., -0.1144,  0.0158,  0.1040],\n",
       "         [-0.3690, -0.0773, -0.3705,  ..., -0.0626,  0.1236,  0.0322],\n",
       "         ...,\n",
       "         [-0.1749, -0.0239, -0.0803,  ..., -0.2275,  0.2995,  0.2830],\n",
       "         [ 0.1887,  0.1462,  0.0492,  ..., -0.2820, -0.1005, -0.0899],\n",
       "         [ 0.0303,  0.0038, -0.0122,  ...,  0.0185, -0.0210, -0.0029]]],\n",
       "       grad_fn=<MulBackward0>), pooler_output=tensor([[-1.6906e-02, -5.6155e-02,  7.1999e-03,  2.1250e-02,  6.2286e-02,\n",
       "         -5.0613e-03,  9.4231e-03,  3.2725e-02, -3.1976e-02, -3.7512e-02,\n",
       "         -1.6157e-02,  4.7103e-02, -3.9021e-02,  1.7731e-03,  2.4045e-02,\n",
       "          8.6636e-03,  3.4755e-02,  5.5665e-02, -1.6583e-04,  1.0474e-02,\n",
       "         -7.8621e-02, -6.4565e-03, -2.3094e-02, -3.0080e-02, -3.7827e-02,\n",
       "         -3.6126e-02, -5.3715e-02,  6.9204e-02, -3.0558e-02,  8.3113e-04,\n",
       "         -5.5085e-03,  2.4779e-02,  2.2223e-02, -1.6586e-02, -4.8503e-02,\n",
       "          2.1637e-02,  7.6077e-03,  3.8111e-02,  1.3499e-02,  8.1523e-03,\n",
       "         -9.4177e-03, -1.4076e-02, -1.3632e-02, -3.7516e-02,  6.3269e-03,\n",
       "          1.9696e-02,  2.6932e-03,  2.2693e-02,  2.7168e-03,  1.2363e-02,\n",
       "         -2.4756e-02,  9.3466e-03,  1.9727e-02,  2.2192e-02,  1.9899e-02,\n",
       "          1.8579e-02,  4.5663e-02,  1.1865e-02,  2.1281e-02,  4.0493e-04,\n",
       "          1.0813e-03, -5.4231e-03,  3.2032e-02,  5.3749e-02, -4.1112e-02,\n",
       "          6.7805e-02, -2.3483e-03,  1.2943e-02,  1.1104e-02,  4.4759e-02,\n",
       "         -1.3106e-02, -4.7403e-02, -1.6662e-02, -5.4469e-02,  7.8648e-02,\n",
       "         -4.6227e-03, -2.2683e-02, -5.0063e-03, -2.2221e-02,  7.2448e-02,\n",
       "         -5.1563e-02,  4.0055e-02, -1.4874e-02, -3.6367e-02, -5.7766e-03,\n",
       "          3.3556e-02,  5.9174e-02, -1.6515e-02, -7.7167e-02, -5.8735e-03,\n",
       "         -2.8442e-02,  3.6463e-02,  2.4682e-03, -1.2105e-02, -5.7210e-02,\n",
       "          3.7351e-02,  2.0371e-02, -3.0206e-02,  3.3250e-02, -2.8137e-02,\n",
       "         -2.0056e-02,  2.1362e-02, -3.7122e-02,  1.5553e-02, -1.1191e-02,\n",
       "         -4.4482e-02,  2.4194e-03,  1.9994e-02, -3.3796e-02, -5.0059e-02,\n",
       "         -1.6704e-03,  1.2928e-02,  5.7980e-02,  4.3475e-02, -2.3939e-02,\n",
       "         -5.5912e-02,  4.3278e-02, -1.0483e-02,  2.6392e-02,  4.3157e-03,\n",
       "         -5.0274e-02, -5.0582e-02,  2.4022e-02, -7.3916e-03,  2.1921e-02,\n",
       "          6.1808e-02,  2.7852e-02, -2.3588e-02,  1.6313e-02, -2.3983e-02,\n",
       "         -4.9960e-02,  3.9196e-03,  4.5492e-03,  3.7790e-02, -7.4113e-03,\n",
       "         -1.3910e-02, -6.1922e-02, -4.9997e-03,  1.0840e-02, -3.0099e-02,\n",
       "         -4.4917e-02,  1.5276e-02,  1.9827e-02,  1.6318e-02,  1.0461e-02,\n",
       "         -8.5871e-04,  6.6870e-02, -2.7363e-02,  5.4896e-02,  9.1411e-03,\n",
       "         -3.3719e-02, -7.3906e-02, -7.3861e-02,  9.3573e-03,  2.5938e-02,\n",
       "         -3.3382e-02,  3.7959e-04, -5.2805e-02,  3.7138e-03,  2.6266e-02,\n",
       "          8.9077e-02,  2.0069e-03, -1.3579e-02,  1.0105e-02,  5.0739e-02,\n",
       "          3.1259e-02,  5.8742e-02, -1.9529e-02,  6.8748e-02, -6.5429e-02,\n",
       "         -3.3484e-03,  8.5751e-03,  2.2422e-02,  1.6860e-02, -4.9799e-02,\n",
       "          2.3384e-03, -4.2650e-02,  2.2405e-02, -1.8401e-02, -7.9258e-04,\n",
       "          1.0113e-02, -5.8365e-02, -2.4272e-02, -6.9798e-03, -3.8771e-02,\n",
       "          4.0633e-03,  1.1751e-02,  3.7265e-02,  2.3701e-02, -3.7739e-02,\n",
       "          2.0611e-02, -2.0699e-02,  4.4023e-02,  3.9030e-02,  4.4696e-02,\n",
       "          3.9362e-02, -2.4538e-03, -4.6666e-02,  4.9508e-02, -2.6520e-02,\n",
       "         -1.0778e-02, -2.6199e-02,  1.8596e-03, -1.4086e-02, -1.6315e-02,\n",
       "         -1.8175e-02, -2.9116e-03, -2.9585e-02, -4.4475e-02, -3.7716e-02,\n",
       "          1.1126e-02,  6.3030e-02, -2.2266e-02,  5.8848e-02, -2.6536e-03,\n",
       "         -5.1691e-02, -2.6029e-02, -5.2213e-02,  3.1580e-02, -5.4156e-03,\n",
       "          2.4617e-02, -2.0322e-02, -2.2136e-02, -2.0156e-02, -3.3600e-02,\n",
       "          5.0843e-02, -1.0805e-02, -4.5353e-02,  4.4723e-02,  1.7275e-02,\n",
       "         -1.7141e-02,  3.9134e-02,  9.3016e-03, -1.4485e-02, -3.8432e-02,\n",
       "         -3.7504e-02, -5.8936e-02,  5.0659e-02, -3.2729e-02,  5.7634e-02,\n",
       "         -1.0418e-02, -3.3776e-02,  6.5242e-03, -1.3668e-03, -2.8497e-02,\n",
       "         -1.3401e-02,  2.5278e-02, -1.5457e-03, -6.4340e-02,  2.5606e-02,\n",
       "         -3.2331e-02,  3.3718e-02, -1.6342e-02,  1.0290e-02,  7.9938e-03,\n",
       "          3.5738e-02, -4.0267e-02, -1.5024e-02,  7.6659e-02,  2.3803e-02,\n",
       "         -4.7032e-02, -8.0881e-03, -3.1291e-02, -2.6749e-02,  1.6161e-02,\n",
       "         -2.0337e-02,  1.3376e-02,  4.2773e-02,  3.6475e-02, -3.8309e-02,\n",
       "          5.6031e-02, -2.4430e-02,  4.6977e-02, -5.1405e-02, -1.4983e-02,\n",
       "          2.0433e-02,  4.5654e-02,  3.1643e-02,  2.8382e-02, -4.2736e-02,\n",
       "          3.6190e-02, -2.1676e-02,  2.3580e-02, -7.7002e-03, -4.0445e-02,\n",
       "         -1.0387e-02,  3.4136e-02,  3.0193e-02,  1.8939e-03, -3.8487e-02,\n",
       "         -6.0349e-02, -2.9306e-02,  4.7986e-02,  1.1288e-02, -1.2105e-02,\n",
       "          2.3229e-02,  2.0923e-02, -8.9078e-02,  5.8675e-03, -1.2638e-02,\n",
       "         -1.7164e-02, -2.3238e-02,  1.7122e-03,  7.0654e-03,  2.0264e-02,\n",
       "          4.5766e-02, -1.9192e-02,  2.3417e-02,  2.2187e-02, -5.7198e-02,\n",
       "         -4.6172e-02,  1.9091e-02,  1.1547e-02, -9.1721e-02,  2.5363e-02,\n",
       "         -9.5077e-04,  1.0218e-02, -7.3768e-03,  5.0501e-03, -3.6117e-02,\n",
       "          7.2906e-02, -4.6687e-02, -1.3299e-02,  9.1834e-02,  1.3161e-03,\n",
       "          2.9547e-02, -2.8728e-02, -9.0424e-04,  4.2706e-03, -5.8752e-02,\n",
       "         -1.1018e-01, -2.3024e-02,  2.0632e-02, -1.5464e-02, -5.9251e-02,\n",
       "         -1.3202e-02,  5.8758e-02, -5.9162e-02, -1.9229e-02, -5.7586e-02,\n",
       "         -2.6075e-02,  6.2953e-02, -4.3314e-02, -1.7985e-02, -1.4976e-02,\n",
       "         -1.4025e-02,  7.3341e-02,  1.9034e-02,  4.2351e-02, -7.6921e-03,\n",
       "          1.3738e-02, -1.5108e-02, -1.7294e-02, -2.8216e-02,  2.9553e-02,\n",
       "          2.1912e-03,  7.2809e-02,  4.6660e-03, -5.7470e-02, -3.2297e-02,\n",
       "         -4.1773e-02, -3.3086e-02,  5.1831e-02,  9.0802e-03, -1.3748e-02,\n",
       "         -2.0197e-02,  5.7554e-02, -3.2614e-02, -5.3320e-02, -8.3379e-02,\n",
       "          1.9830e-02, -2.2501e-02, -2.5894e-02,  1.7042e-02,  1.8530e-02,\n",
       "          3.2769e-02,  4.2445e-02, -6.1992e-02, -1.7103e-02,  3.6947e-02,\n",
       "         -6.1804e-02, -1.9941e-02,  1.8312e-02, -4.0979e-02,  4.5607e-02,\n",
       "         -1.6482e-02,  4.4054e-02,  2.1847e-02,  1.4196e-02, -7.1670e-03,\n",
       "          1.6114e-02,  3.6979e-02,  2.2576e-02, -2.0161e-02, -2.5447e-02,\n",
       "         -5.5902e-02,  2.8347e-02,  1.0614e-02,  2.0104e-02, -5.6447e-02,\n",
       "         -3.4640e-02,  1.8844e-03,  3.6520e-02, -4.0070e-02, -6.6700e-02,\n",
       "          3.9431e-02,  2.4797e-02, -1.8836e-02, -1.7829e-03,  1.1602e-01,\n",
       "          4.2846e-03,  4.7246e-02,  3.2501e-02,  4.1168e-02, -3.0979e-02,\n",
       "          3.6814e-02, -4.2070e-02,  2.8270e-02,  1.4425e-02,  2.5672e-02,\n",
       "         -1.8869e-02,  2.2396e-05,  2.2515e-02,  5.9175e-03, -2.8095e-02,\n",
       "         -1.0376e-03,  3.4802e-02, -4.0563e-02,  5.6131e-02,  6.7199e-02,\n",
       "         -2.5507e-02, -5.7172e-02, -7.3876e-02,  1.2171e-02,  2.0775e-02,\n",
       "         -7.3622e-03, -6.2363e-02,  2.9572e-02, -4.0618e-02, -3.3161e-02,\n",
       "          2.2283e-02,  2.5776e-02,  1.5616e-02, -4.8792e-03,  6.1884e-03,\n",
       "         -1.0213e-02,  6.8236e-03, -5.9136e-02, -5.1796e-02, -6.7593e-02,\n",
       "          1.2110e-02,  9.6615e-02,  3.8791e-02,  6.2373e-03, -1.9153e-02,\n",
       "          4.0040e-02,  8.1989e-03,  3.3071e-03, -5.1490e-02, -3.4799e-02,\n",
       "          7.4989e-03,  6.4252e-02,  4.1249e-02, -2.1228e-02,  9.2874e-03,\n",
       "          3.8694e-03,  2.4499e-02, -4.6852e-03, -3.3792e-02, -1.7901e-02,\n",
       "          4.0594e-03, -2.2022e-02,  2.6576e-02, -2.9005e-02, -6.1908e-02,\n",
       "          1.7083e-02, -2.2552e-02,  4.2985e-02,  7.7706e-02,  7.2026e-02,\n",
       "          2.9577e-02, -2.5641e-02,  1.1245e-02, -1.3038e-02, -4.8427e-02,\n",
       "         -1.2906e-02, -4.1111e-04, -1.3234e-02,  1.1701e-02,  1.6036e-02,\n",
       "         -1.4888e-02, -3.1899e-02, -2.0451e-02, -1.9215e-02,  1.6712e-02,\n",
       "          3.1397e-02,  9.7115e-03, -1.6734e-02,  1.2676e-02,  5.1367e-02,\n",
       "         -9.6888e-03,  9.3357e-03, -3.7395e-03,  2.4164e-03,  4.6302e-03,\n",
       "          2.7048e-02, -1.6847e-03, -1.5265e-02,  2.5141e-02, -4.7684e-02,\n",
       "          1.5993e-03, -1.6651e-02, -8.5968e-02,  6.1902e-03, -6.5809e-02,\n",
       "          2.2624e-02,  2.6648e-02,  7.7237e-02,  1.8489e-02,  3.9057e-02,\n",
       "          6.1467e-02,  3.6740e-02, -9.1805e-03,  2.9823e-02,  3.5042e-02,\n",
       "         -1.1109e-02, -8.1577e-02, -2.1868e-02, -4.3775e-02, -7.9689e-03,\n",
       "         -1.4027e-02, -1.1392e-02,  3.5429e-02, -1.8042e-02,  1.5467e-02,\n",
       "         -2.0623e-02,  4.6586e-02,  4.5017e-02,  1.8534e-02,  7.5368e-02,\n",
       "          5.2940e-02, -3.3331e-02, -2.5514e-02,  6.9978e-02, -9.7126e-03,\n",
       "          5.6452e-03, -3.1800e-02,  2.3731e-02, -4.3791e-02, -1.9828e-02,\n",
       "         -5.1498e-03, -1.7646e-02,  1.9079e-03, -7.7357e-03, -8.5301e-03,\n",
       "          3.1115e-02, -6.6128e-02, -2.5854e-02,  2.1786e-02, -3.5408e-02,\n",
       "         -7.8651e-03, -8.6778e-03, -3.7951e-02, -2.0959e-02, -2.5734e-02,\n",
       "          1.9701e-02,  2.9981e-02, -5.6902e-02,  3.3676e-02,  1.7801e-02,\n",
       "          2.6459e-02, -2.8707e-02, -6.4070e-03, -3.0103e-02,  5.8144e-02,\n",
       "          4.0399e-02,  3.3215e-02, -3.9847e-03, -6.1900e-02,  2.9255e-02,\n",
       "         -2.0988e-02,  2.9884e-02,  2.2035e-02, -3.2751e-02,  1.0416e-01,\n",
       "          4.5235e-02, -2.7773e-02, -2.6698e-02,  1.3160e-02,  4.9331e-02,\n",
       "         -7.6819e-02, -7.2728e-03, -2.3200e-02, -2.5927e-02, -2.6726e-02,\n",
       "          1.1307e-02, -1.2650e-02, -3.1845e-02,  2.9019e-02, -3.6564e-03,\n",
       "         -3.9835e-02, -3.7218e-02, -3.4120e-02, -4.6867e-03,  1.7177e-03,\n",
       "         -9.6883e-02,  8.9739e-03,  1.1011e-02, -3.7167e-03,  2.0298e-02,\n",
       "         -1.1670e-02,  4.0518e-02,  3.7929e-02, -1.5952e-02, -2.8929e-02,\n",
       "         -4.8628e-03,  3.6032e-02,  4.1786e-02,  7.0508e-03,  2.6533e-02,\n",
       "         -3.5579e-02,  4.8363e-02,  6.2720e-03,  1.4329e-02,  5.7157e-02,\n",
       "         -7.0131e-03,  1.2960e-02,  2.2677e-02,  4.9290e-02,  4.5404e-02,\n",
       "          1.2376e-02, -3.2091e-02,  4.7240e-02,  7.3930e-02,  2.3988e-02,\n",
       "         -1.0955e-02, -3.3906e-02, -4.7480e-02,  2.0386e-02, -2.4681e-02,\n",
       "          2.0582e-02,  1.9598e-02, -5.6824e-02, -9.0759e-03, -5.2488e-02,\n",
       "         -3.2673e-02,  2.8318e-02,  4.0575e-02, -2.6431e-02,  1.9112e-02,\n",
       "         -4.3109e-03, -3.3702e-02, -4.3992e-02, -5.1051e-02, -2.3392e-02,\n",
       "         -5.4390e-02, -2.1975e-02,  6.7999e-02,  2.1847e-02, -7.1121e-02,\n",
       "         -3.3976e-03, -4.1843e-02,  7.5447e-03,  6.7708e-02, -2.7631e-02,\n",
       "         -5.0824e-02, -3.4338e-02,  1.1081e-02,  2.7148e-02, -2.2358e-02,\n",
       "         -1.5813e-02,  1.7539e-02, -2.3978e-02,  1.6037e-02, -8.0544e-02,\n",
       "         -1.9945e-02, -4.2223e-03,  6.1717e-02,  3.4380e-02, -4.6937e-03,\n",
       "         -5.3846e-03,  1.0676e-02,  1.2323e-02, -4.1267e-02, -2.1462e-02,\n",
       "         -1.7536e-02,  8.7161e-02, -4.6615e-02, -3.9944e-02, -3.0961e-02,\n",
       "         -3.9846e-02,  3.6302e-02, -2.7864e-03, -7.7861e-02, -4.2846e-03,\n",
       "         -2.7058e-02,  3.6560e-02, -2.4295e-02, -1.1807e-02, -7.7311e-02,\n",
       "         -1.7581e-03, -1.3724e-02,  2.8583e-02, -5.1848e-02, -3.6584e-02,\n",
       "         -1.5042e-02,  3.1555e-03, -1.4723e-03, -1.5844e-02,  1.9027e-02,\n",
       "          8.9748e-04,  8.6684e-03, -3.0762e-02, -3.0019e-02, -3.7089e-02,\n",
       "          5.4863e-02, -2.9272e-02, -4.0300e-02, -1.8686e-04, -1.6295e-02,\n",
       "          8.0593e-02, -3.9161e-02,  5.4288e-03, -1.9839e-02,  4.8655e-02,\n",
       "         -3.2888e-02, -3.7698e-02,  2.4396e-02,  1.4106e-02,  1.4639e-02,\n",
       "          7.8905e-02, -3.2799e-02,  4.4128e-02, -1.7738e-02, -1.8075e-03,\n",
       "         -1.0309e-02,  1.7904e-02, -5.5121e-02,  3.6029e-02,  6.7429e-02,\n",
       "         -3.2175e-02,  4.9229e-02,  4.6396e-02, -4.4543e-02, -2.5372e-02,\n",
       "          9.9330e-03, -5.5385e-03,  4.1427e-02,  2.9406e-02,  2.2530e-04,\n",
       "         -5.6573e-02, -1.9793e-02, -2.5397e-02, -3.0285e-02, -4.0114e-02,\n",
       "          1.8404e-02,  2.2669e-02, -2.1097e-02,  4.1282e-02,  5.0041e-02,\n",
       "          3.8025e-02, -4.0571e-02, -2.5178e-02,  7.9337e-02, -2.7883e-02,\n",
       "          1.0701e-01, -7.9259e-02,  7.8504e-03]], grad_fn=<DivBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = trainer.encoder_question(\n",
    "        vis_inputs=batch['vis_inputs'],\n",
    "        vis_attention_mask=None,\n",
    "        task ='IR',\n",
    "        return_pooled_output=True,\n",
    "        pool_strategy=\"avg\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 3.2728e-02, -2.0178e-01, -7.6980e-02,  ..., -1.1240e-01,\n",
       "          -3.0784e-01,  3.8074e-01],\n",
       "         [ 2.2909e-01, -9.1326e-02, -3.5157e-02,  ..., -1.7308e-01,\n",
       "          -1.4064e-01,  2.4560e-01],\n",
       "         [ 2.2376e-01,  4.4100e-02, -3.8268e-02,  ..., -1.5783e-01,\n",
       "          -9.5935e-02,  1.8547e-01],\n",
       "         ...,\n",
       "         [ 8.8216e-02, -2.5647e-02, -1.6850e-01,  ..., -6.4090e-02,\n",
       "          -1.8485e-01,  3.2030e-01],\n",
       "         [ 1.0231e-01,  1.7612e-01, -8.0537e-02,  ..., -1.6791e-01,\n",
       "          -8.2071e-02,  3.1913e-01],\n",
       "         [ 1.7962e-02,  3.3173e-02,  1.1373e-02,  ...,  3.3538e-04,\n",
       "          -2.3700e-02, -1.3789e-02]]], grad_fn=<MulBackward0>), pooler_output=tensor([[ 0.0365,  0.0102,  0.0062, -0.0148,  0.0259,  0.0775,  0.0252, -0.0388,\n",
       "          0.0039,  0.0144,  0.0306,  0.0593, -0.0337, -0.0132, -0.0053,  0.0260,\n",
       "          0.0065, -0.0687, -0.0466, -0.0049, -0.0344,  0.0467,  0.0474, -0.0725,\n",
       "          0.0040, -0.0118, -0.0419,  0.0244, -0.0424, -0.0161, -0.0168,  0.0123,\n",
       "         -0.0548, -0.0360,  0.0005, -0.0284, -0.0131, -0.0109,  0.0006,  0.0462,\n",
       "          0.0004, -0.0504, -0.0616,  0.0365,  0.0442, -0.0082,  0.0047, -0.0228,\n",
       "          0.0294, -0.0085, -0.0200, -0.0516, -0.0177,  0.0387, -0.0456, -0.0091,\n",
       "         -0.0047,  0.0316, -0.0707,  0.0501,  0.0362,  0.0131, -0.0493, -0.0321,\n",
       "         -0.0099, -0.0382,  0.0749, -0.0522, -0.0347, -0.0024, -0.0146,  0.0276,\n",
       "          0.0058,  0.0304, -0.0022,  0.0807,  0.0253, -0.0191,  0.0255,  0.0179,\n",
       "          0.0133,  0.0202,  0.0463,  0.0294,  0.0505,  0.0198, -0.0047,  0.0108,\n",
       "         -0.0272, -0.0143, -0.0349,  0.0361, -0.0483, -0.0476,  0.0158, -0.0177,\n",
       "         -0.0506, -0.0226,  0.0206,  0.0006, -0.0516, -0.0531,  0.0136, -0.0377,\n",
       "         -0.0484,  0.0069,  0.0240,  0.0081,  0.0084, -0.0520,  0.0073, -0.0078,\n",
       "         -0.0026,  0.0338, -0.0363,  0.0004,  0.0220, -0.0103, -0.0147,  0.0707,\n",
       "         -0.0917,  0.0403, -0.0093, -0.0008,  0.0043, -0.0300, -0.0085, -0.0420,\n",
       "          0.0551, -0.0106, -0.0016,  0.0265, -0.0127, -0.0533,  0.0118, -0.0369,\n",
       "         -0.0525, -0.0120, -0.0753,  0.0252,  0.0197,  0.0361,  0.0010, -0.0127,\n",
       "         -0.0047,  0.0188, -0.0476, -0.0742,  0.0014,  0.0288, -0.0542, -0.0759,\n",
       "          0.0035,  0.0256,  0.0363, -0.0340, -0.0363,  0.0291,  0.0088, -0.0390,\n",
       "         -0.0225, -0.0086,  0.0073, -0.0047, -0.0092,  0.0281,  0.0035, -0.0593,\n",
       "          0.0261, -0.0088, -0.0821,  0.0064, -0.0560,  0.0584, -0.0438, -0.0282,\n",
       "          0.0337, -0.0038,  0.0234,  0.1053,  0.0030, -0.0278, -0.0144, -0.0415,\n",
       "          0.0529,  0.0184, -0.0527, -0.0340, -0.0491,  0.0177, -0.0739,  0.0328,\n",
       "          0.0680,  0.0445, -0.0254,  0.0156, -0.0154,  0.0001,  0.0180,  0.0125,\n",
       "         -0.0062, -0.0363, -0.0318,  0.0411,  0.0572, -0.0089, -0.0173,  0.0215,\n",
       "         -0.0042,  0.0468,  0.0351, -0.0051,  0.0348, -0.0218,  0.0146, -0.0275,\n",
       "         -0.0377,  0.0172, -0.0238, -0.0229,  0.0354, -0.0165, -0.0899,  0.0254,\n",
       "          0.0813, -0.0543,  0.0192,  0.0307,  0.0023, -0.0060, -0.0072,  0.0448,\n",
       "          0.0046, -0.0639, -0.0135, -0.0133,  0.0060,  0.0015, -0.0489,  0.0027,\n",
       "          0.0346, -0.0362, -0.0606,  0.0566,  0.0112,  0.0036,  0.0340, -0.0279,\n",
       "         -0.0834,  0.0301,  0.0237, -0.0712,  0.0255,  0.0337, -0.0014,  0.0230,\n",
       "         -0.0022, -0.0533,  0.0017,  0.0208, -0.0133,  0.0015, -0.0214,  0.0401,\n",
       "         -0.0094,  0.0021, -0.0179, -0.0120,  0.0026, -0.0016, -0.0440, -0.0202,\n",
       "         -0.0632,  0.0111,  0.0155, -0.0092,  0.0149, -0.0550,  0.0184,  0.0124,\n",
       "          0.0722, -0.0164,  0.0396, -0.0307, -0.0214,  0.0083,  0.0548, -0.0210,\n",
       "         -0.0397, -0.0309, -0.0255,  0.0033,  0.0541,  0.0021, -0.0140, -0.0096,\n",
       "          0.0157, -0.0077,  0.0506,  0.0418,  0.0247,  0.0312, -0.0388, -0.0271,\n",
       "         -0.0202,  0.0324,  0.0132, -0.0618,  0.0101, -0.0604,  0.0330, -0.0417,\n",
       "          0.0106, -0.0567, -0.0338, -0.0952, -0.0073,  0.0135,  0.0355,  0.0475,\n",
       "          0.0404, -0.0343, -0.0190,  0.0070,  0.0486,  0.0952,  0.0064,  0.0374,\n",
       "         -0.1052,  0.0010,  0.0304, -0.0202, -0.0388, -0.0097,  0.0265, -0.0298,\n",
       "         -0.0015, -0.0393, -0.0220, -0.0725, -0.0463, -0.0377,  0.0268,  0.0346,\n",
       "         -0.0561, -0.0319,  0.0496,  0.0111,  0.0222,  0.0239,  0.0511,  0.0420,\n",
       "          0.0210, -0.0019,  0.0637,  0.0127, -0.0106, -0.0047,  0.0615,  0.0245,\n",
       "         -0.0104, -0.0496, -0.0440,  0.0352,  0.0268,  0.0187, -0.0425,  0.0547,\n",
       "          0.0783, -0.0261, -0.0559,  0.0006, -0.0641, -0.0035, -0.0154, -0.0224,\n",
       "          0.0171,  0.0505, -0.0266,  0.0061, -0.0535, -0.0240,  0.0689,  0.0103,\n",
       "          0.0097, -0.0142, -0.0109,  0.0300,  0.0114,  0.0114,  0.0125,  0.0338,\n",
       "         -0.0277, -0.0044, -0.0443,  0.0056, -0.0041,  0.0103, -0.0634,  0.0016,\n",
       "          0.0780,  0.0299, -0.0092, -0.0421, -0.0357, -0.0462, -0.0500, -0.0018,\n",
       "          0.0114, -0.0273,  0.0449, -0.0046,  0.0421, -0.0357,  0.0166, -0.0046,\n",
       "         -0.0396,  0.0364,  0.0206, -0.0245, -0.0597, -0.0081,  0.0072, -0.0044,\n",
       "         -0.0781, -0.0304,  0.0308, -0.0102,  0.0007, -0.0037,  0.0220,  0.0346,\n",
       "         -0.0207, -0.0746, -0.0777,  0.0155, -0.0210, -0.0517, -0.0637,  0.0224,\n",
       "         -0.0454, -0.0033, -0.0370,  0.0472,  0.0146,  0.0011, -0.0052,  0.0311,\n",
       "         -0.0350,  0.0296,  0.0512,  0.0238,  0.0424,  0.0284,  0.0348,  0.0272,\n",
       "          0.0260,  0.0236,  0.0504,  0.0478, -0.0049,  0.0111,  0.0656,  0.0557,\n",
       "         -0.0278, -0.0171,  0.0181,  0.0084, -0.0093, -0.0790,  0.0548, -0.0136,\n",
       "          0.0124,  0.1072,  0.0288,  0.0149,  0.0279, -0.0287,  0.0197,  0.0294,\n",
       "          0.0472,  0.0078,  0.0265,  0.0052, -0.0064, -0.0163,  0.0592, -0.0246,\n",
       "          0.0227, -0.0686, -0.0193, -0.0516, -0.0167, -0.0063, -0.0106,  0.0010,\n",
       "          0.0160,  0.0578, -0.0599,  0.0442,  0.0031,  0.0242, -0.0024,  0.0311,\n",
       "         -0.0335,  0.0364,  0.0060, -0.0683,  0.0353, -0.0185,  0.0093,  0.0071,\n",
       "         -0.0312, -0.0552,  0.0096, -0.0234, -0.0190, -0.0005,  0.0242, -0.0441,\n",
       "         -0.0035, -0.0373, -0.0254,  0.0069, -0.0214, -0.0424,  0.0431, -0.0652,\n",
       "          0.0106,  0.0290, -0.0259, -0.0245, -0.0120, -0.0091, -0.0317, -0.0061,\n",
       "          0.0083, -0.0386, -0.0084, -0.0178, -0.0245,  0.0168,  0.0261, -0.0209,\n",
       "         -0.0389, -0.0296, -0.0013, -0.0678, -0.0472,  0.0309,  0.0084,  0.0017,\n",
       "          0.0445, -0.0050, -0.0341,  0.0054,  0.0114,  0.0893,  0.0662, -0.0069,\n",
       "          0.0638,  0.0067,  0.0452,  0.0289, -0.0346, -0.0581,  0.0050, -0.0304,\n",
       "         -0.0063,  0.0043,  0.0051, -0.0095, -0.0740,  0.0322,  0.0119, -0.0211,\n",
       "         -0.0214,  0.0312, -0.0299,  0.0436, -0.0144, -0.0045,  0.0155, -0.0257,\n",
       "          0.0575,  0.0006,  0.0078, -0.0223, -0.0255, -0.0051, -0.0312,  0.0346,\n",
       "         -0.0560,  0.0083, -0.0280, -0.0085, -0.0437, -0.0363, -0.0088,  0.0008,\n",
       "          0.0354,  0.0180,  0.0583, -0.0225, -0.0049, -0.0247, -0.0845, -0.0272,\n",
       "          0.0661,  0.0098, -0.0822, -0.0025, -0.0165, -0.0117,  0.0379, -0.0719,\n",
       "          0.0460,  0.0464,  0.0298,  0.0291,  0.0459,  0.0122,  0.0248,  0.0465,\n",
       "          0.0474, -0.0932,  0.0031, -0.0312, -0.0681, -0.0159,  0.0423, -0.0160,\n",
       "          0.0224,  0.0316,  0.0311, -0.0174, -0.0350, -0.0184,  0.0292, -0.0035,\n",
       "          0.0125,  0.0776, -0.0160,  0.0156,  0.0487, -0.0369,  0.0627,  0.0467,\n",
       "          0.0170,  0.0295,  0.0646,  0.0425,  0.0056, -0.0171,  0.0307, -0.0429,\n",
       "         -0.0164, -0.0418,  0.0486,  0.0328,  0.0243,  0.0102, -0.0132, -0.0112,\n",
       "         -0.0136,  0.0005, -0.0025,  0.0003, -0.0453, -0.0409, -0.0008, -0.0401,\n",
       "         -0.0067, -0.0507, -0.0286,  0.0381,  0.0425, -0.0081,  0.0467, -0.0219,\n",
       "          0.0300,  0.0298,  0.0009, -0.0301, -0.0470,  0.0122,  0.0208,  0.0238,\n",
       "          0.0271,  0.0665, -0.0862, -0.0064, -0.0133,  0.0581, -0.0047, -0.0515,\n",
       "          0.0058,  0.0520, -0.0157,  0.0164, -0.0990,  0.0220, -0.0127,  0.0316,\n",
       "         -0.0199,  0.0087,  0.0270,  0.0249, -0.0362, -0.0009, -0.0421,  0.0055,\n",
       "          0.0178, -0.0413,  0.0004, -0.0350,  0.0168, -0.0077,  0.0126,  0.0236,\n",
       "         -0.0041, -0.0125, -0.0065, -0.0292, -0.0135, -0.0293, -0.0205, -0.0519,\n",
       "         -0.0603,  0.0685,  0.0521, -0.0474,  0.0252,  0.0303, -0.0447,  0.0482,\n",
       "          0.0197, -0.0309, -0.0092, -0.0299,  0.0701, -0.0155,  0.0280, -0.0171,\n",
       "         -0.0422, -0.0387,  0.0470, -0.0051, -0.0573, -0.0221, -0.0177,  0.0451,\n",
       "         -0.0268,  0.0411, -0.0142,  0.0303, -0.0071,  0.0257, -0.0142,  0.0652,\n",
       "         -0.0400,  0.0338, -0.0021, -0.0714,  0.0043, -0.0199,  0.0228,  0.1067]],\n",
       "       grad_fn=<DivBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "025084c40b588eb05019761c48ebfbdc57758708fc0433510479ba20b4a83a56"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('cvlp2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
